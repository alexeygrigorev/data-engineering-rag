{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deea5eb6-4af4-415b-8acd-9e02066523e8",
   "metadata": {
    "_sphinx_cell_id": "89577dfe-ecc1-4b36-a665-58185dcaa4cf"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf69474d-d31f-4945-b440-95c68f5c30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_extensions = {'md', 'mdx'}\n",
    "code_extensions = {'py', 'sql', 'java', 'ipynb'}\n",
    "\n",
    "extensions = doc_extensions | code_extensions\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filepath = file_info.filename\n",
    "        filepath_lower = filepath.lower()\n",
    "\n",
    "        if filepath_lower.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        filename = filepath_lower.split('/')[-1]\n",
    "\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        ext = filename.split('.')[-1]\n",
    "\n",
    "        if ext not in extensions:\n",
    "            continue\n",
    "\n",
    "        filepath_edited = filepath.split('/', maxsplit=1)[1]\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                if ext in doc_extensions:\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data['filename'] = filepath_edited\n",
    "                elif ext in code_extensions:\n",
    "                    data = {\n",
    "                        'code': True,\n",
    "                        'content': content,\n",
    "                        'filename': filepath_edited\n",
    "                    }\n",
    "\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef88a6ec-8511-4dd5-86ad-2ad9f967626a",
   "metadata": {
    "_sphinx_cell_id": "125c4d5b-7113-484d-aca0-a83fde964a48"
   },
   "outputs": [],
   "source": [
    "de_zoomcamp_data = read_repo_data('DataTalksClub', 'data-engineering-zoomcamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3031e517-b52b-4779-b463-16e2f6b04599",
   "metadata": {
    "_sphinx_cell_id": "f956d5ae-868c-4ded-a0fa-a3bad88c6b2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_zoomcamp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77a382b-edc7-49c9-95a3-a40b05160159",
   "metadata": {
    "_sphinx_cell_id": "e55121aa-c222-4ac3-9911-37161a43f2ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-docker-terraform/1_terraform_gcp/1_terraform_overview.md\n",
      "01-docker-terraform/1_terraform_gcp/2_gcp_overview.md\n",
      "01-docker-terraform/1_terraform_gcp/README.md\n",
      "01-docker-terraform/1_terraform_gcp/terraform/README.md\n",
      "01-docker-terraform/1_terraform_gcp/windows.md\n",
      "01-docker-terraform/2_docker_sql/README.md\n",
      "01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb\n",
      "01-docker-terraform/2_docker_sql/data-loading-parquet.py\n",
      "01-docker-terraform/2_docker_sql/ingest_data.py\n",
      "01-docker-terraform/2_docker_sql/pg-test-connection.ipynb\n",
      "01-docker-terraform/2_docker_sql/pipeline.py\n",
      "01-docker-terraform/2_docker_sql/upload-data.ipynb\n",
      "01-docker-terraform/README.md\n",
      "02-workflow-orchestration/README.md\n",
      "03-data-warehouse/README.md\n",
      "03-data-warehouse/big_query.sql\n",
      "03-data-warehouse/big_query_hw.sql\n",
      "03-data-warehouse/big_query_ml.sql\n",
      "03-data-warehouse/extract_model.md\n",
      "03-data-warehouse/extras/README.md\n",
      "03-data-warehouse/extras/web_to_gcs.py\n",
      "04-analytics-engineering/README.md\n",
      "04-analytics-engineering/SQL_refresher.md\n",
      "04-analytics-engineering/dbt_cloud_setup.md\n",
      "04-analytics-engineering/docker_setup/README.md\n",
      "04-analytics-engineering/taxi_rides_ny/README.md\n",
      "04-analytics-engineering/taxi_rides_ny/analyses/hack-load-data.sql\n",
      "04-analytics-engineering/taxi_rides_ny/macros/get_payment_type_description.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/core/dim_zones.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/core/dm_monthly_zone_revenue.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/staging/stg_green_tripdata.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\n",
      "05-batch/README.md\n",
      "05-batch/code/03_test.ipynb\n",
      "05-batch/code/04_pyspark.ipynb\n",
      "05-batch/code/05_taxi_schema.ipynb\n",
      "05-batch/code/06_spark_sql.ipynb\n",
      "05-batch/code/06_spark_sql.py\n",
      "05-batch/code/06_spark_sql_big_query.py\n",
      "05-batch/code/07_groupby_join.ipynb\n",
      "05-batch/code/08_rdds.ipynb\n",
      "05-batch/code/09_spark_gcs.ipynb\n",
      "05-batch/code/cloud.md\n",
      "05-batch/code/homework.ipynb\n",
      "05-batch/setup/hadoop-yarn.md\n",
      "05-batch/setup/linux.md\n",
      "05-batch/setup/macos.md\n",
      "05-batch/setup/pyspark.md\n",
      "05-batch/setup/windows.md\n",
      "06-streaming/README.md\n",
      "06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java\n",
      "06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java\n",
      "06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonConsumer.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/Secrets.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/Topics.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/customserdes/CustomSerdes.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/data/PickupLocation.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/data/Ride.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/data/VendorInfo.java\n",
      "06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java\n",
      "06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java\n",
      "06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java\n",
      "06-streaming/ksqldb/commands.md\n",
      "06-streaming/pyflink/README.md\n",
      "06-streaming/pyflink/homework.md\n",
      "06-streaming/pyflink/src/job/aggregation_job.py\n",
      "06-streaming/pyflink/src/job/start_job.py\n",
      "06-streaming/pyflink/src/job/taxi_job.py\n",
      "06-streaming/pyflink/src/producers/load_taxi_data.py\n",
      "06-streaming/pyflink/src/producers/producer.py\n",
      "06-streaming/python/README.md\n",
      "06-streaming/python/avro_example/consumer.py\n",
      "06-streaming/python/avro_example/producer.py\n",
      "06-streaming/python/avro_example/ride_record.py\n",
      "06-streaming/python/avro_example/ride_record_key.py\n",
      "06-streaming/python/avro_example/settings.py\n",
      "06-streaming/python/docker/README.md\n",
      "06-streaming/python/json_example/consumer.py\n",
      "06-streaming/python/json_example/producer.py\n",
      "06-streaming/python/json_example/ride.py\n",
      "06-streaming/python/json_example/settings.py\n",
      "06-streaming/python/redpanda_example/README.md\n",
      "06-streaming/python/redpanda_example/consumer.py\n",
      "06-streaming/python/redpanda_example/producer.py\n",
      "06-streaming/python/redpanda_example/ride.py\n",
      "06-streaming/python/redpanda_example/settings.py\n",
      "06-streaming/python/streams-example/faust/branch_price.py\n",
      "06-streaming/python/streams-example/faust/producer_taxi_json.py\n",
      "06-streaming/python/streams-example/faust/stream.py\n",
      "06-streaming/python/streams-example/faust/stream_count_vendor_trips.py\n",
      "06-streaming/python/streams-example/faust/taxi_rides.py\n",
      "06-streaming/python/streams-example/faust/windowing.py\n",
      "06-streaming/python/streams-example/pyspark/README.md\n",
      "06-streaming/python/streams-example/pyspark/consumer.py\n",
      "06-streaming/python/streams-example/pyspark/producer.py\n",
      "06-streaming/python/streams-example/pyspark/settings.py\n",
      "06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb\n",
      "06-streaming/python/streams-example/pyspark/streaming.py\n",
      "06-streaming/python/streams-example/redpanda/README.md\n",
      "06-streaming/python/streams-example/redpanda/consumer.py\n",
      "06-streaming/python/streams-example/redpanda/producer.py\n",
      "06-streaming/python/streams-example/redpanda/settings.py\n",
      "06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb\n",
      "06-streaming/python/streams-example/redpanda/streaming.py\n",
      "README.md\n",
      "after-sign-up.md\n",
      "asking-questions.md\n",
      "awesome-data-engineering.md\n",
      "certificates.md\n",
      "cohorts/2022/README.md\n",
      "cohorts/2022/project.md\n",
      "cohorts/2022/week_1_basics_n_setup/homework.md\n",
      "cohorts/2022/week_2_data_ingestion/README.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/1_setup_official.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/2_setup_nofrills.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/README.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py\n",
      "cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py\n",
      "cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py\n",
      "cohorts/2022/week_2_data_ingestion/airflow/docs/1_concepts.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py\n",
      "cohorts/2022/week_2_data_ingestion/homework/homework.md\n",
      "cohorts/2022/week_2_data_ingestion/homework/solution.py\n",
      "cohorts/2022/week_2_data_ingestion/transfer_service/README.md\n",
      "cohorts/2022/week_3_data_warehouse/airflow/1_setup_official.md\n",
      "cohorts/2022/week_3_data_warehouse/airflow/2_setup_nofrills.md\n",
      "cohorts/2022/week_3_data_warehouse/airflow/README.md\n",
      "cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py\n",
      "cohorts/2022/week_5_batch_processing/homework.md\n",
      "cohorts/2022/week_6_stream_processing/homework.md\n",
      "cohorts/2023/README.md\n",
      "cohorts/2023/leaderboard.md\n",
      "cohorts/2023/project.md\n",
      "cohorts/2023/week_1_docker_sql/homework.md\n",
      "cohorts/2023/week_1_terraform/homework.md\n",
      "cohorts/2023/week_2_workflow_orchestration/README.md\n",
      "cohorts/2023/week_2_workflow_orchestration/homework.md\n",
      "cohorts/2023/week_3_data_warehouse/homework.md\n",
      "cohorts/2023/week_4_analytics_engineering/homework.md\n",
      "cohorts/2023/week_5_batch_processing/homework.md\n",
      "cohorts/2023/week_6_stream_processing/homework.md\n",
      "cohorts/2023/week_6_stream_processing/producer_confluent.py\n",
      "cohorts/2023/week_6_stream_processing/settings.py\n",
      "cohorts/2023/week_6_stream_processing/streaming_confluent.py\n",
      "cohorts/2023/workshops/piperider.md\n",
      "cohorts/2024/01-docker-terraform/homework.md\n",
      "cohorts/2024/01-docker-terraform/solutions.md\n",
      "cohorts/2024/02-workflow-orchestration/README.md\n",
      "cohorts/2024/02-workflow-orchestration/homework.md\n",
      "cohorts/2024/03-data-warehouse/homework.md\n",
      "cohorts/2024/04-analytics-engineering/homework.md\n",
      "cohorts/2024/05-batch/homework.md\n",
      "cohorts/2024/06-streaming/homework.md\n",
      "cohorts/2024/README.md\n",
      "cohorts/2024/leaderboard.md\n",
      "cohorts/2024/project.md\n",
      "cohorts/2024/workshops/dlt.md\n",
      "cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md\n",
      "cohorts/2024/workshops/dlt_resources/homework_solution.ipynb\n",
      "cohorts/2024/workshops/dlt_resources/homework_starter.ipynb\n",
      "cohorts/2024/workshops/dlt_resources/workshop.ipynb\n",
      "cohorts/2024/workshops/rising-wave.md\n",
      "cohorts/2025/01-docker-terraform/homework.md\n",
      "cohorts/2025/01-docker-terraform/solution.md\n",
      "cohorts/2025/02-workflow-orchestration/homework.md\n",
      "cohorts/2025/02-workflow-orchestration/solution.md\n",
      "cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb\n",
      "cohorts/2025/03-data-warehouse/homework.md\n",
      "cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py\n",
      "cohorts/2025/04-analytics-engineering/homework.md\n",
      "cohorts/2025/05-batch/homework.md\n",
      "cohorts/2025/05-batch/homework/solution.ipynb\n",
      "cohorts/2025/06-streaming/homework.md\n",
      "cohorts/2025/06-streaming/homework/homework.ipynb\n",
      "cohorts/2025/README.md\n",
      "cohorts/2025/project.md\n",
      "cohorts/2025/workshops/dlt/README.md\n",
      "cohorts/2025/workshops/dlt/data_ingestion_workshop.md\n",
      "cohorts/2025/workshops/dlt/dlt_homework.md\n",
      "cohorts/2025/workshops/dynamic_load_dlt.py\n",
      "dataset.md\n",
      "learning-in-public.md\n",
      "projects/README.md\n",
      "projects/datasets.md\n"
     ]
    }
   ],
   "source": [
    "index = {}\n",
    "\n",
    "for record in de_zoomcamp_data:\n",
    "    index[record['filename']] = record\n",
    "    print(record['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae9c710c-55a2-469c-9ada-403670efddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = index['04-analytics-engineering/taxi_rides_ny/models/core/dim_zones.sql']\n",
    "code = cnt['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f42422-e0e9-4dba-a75e-b1465e7b2cd4",
   "metadata": {
    "_sphinx_cell_id": "c933285c-97e4-43f6-9f7d-caba91beaeea"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c320a93c-1d82-4a15-aa53-a6961ea89b68",
   "metadata": {
    "_sphinx_cell_id": "8021b298-87b3-4f8f-8ee7-b675e66bf691"
   },
   "outputs": [],
   "source": [
    "def llm(instructions, content, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages,\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da62b0a0-62b4-4f38-8e85-d2b5ca3e4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_doc_instructions = \"\"\"\n",
    "You are given a piece of source code.  \n",
    "\n",
    "Your task:  \n",
    "- Analyze the code and produce a clear, high-level description of what it does.  \n",
    "- If the code defines functions, methods, or classes, describe their purpose and role.  \n",
    "- If it’s just a script without explicit functions/classes, summarize what the script does step by step at a high level.  \n",
    "- Add logical sections or headings (##) if needed. Sections must be relatively large (8-10 paragraphs and code blocks)\n",
    "- Keep explanations concise and clear — avoid unnecessary verbosity.  \n",
    "- Output the result in Markdown, structured like documentation.  \n",
    "- Do not rewrite or modify the code itself, only provide descriptive documentation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c829f4e5-663b-45ee-becd-caf6b05f8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm(code_doc_instructions, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11cf9b45-e781-452d-b6df-d1afff8d6686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# High-Level Description of the Code\n",
      "\n",
      "This code snippet is written in SQL and is intended for use within a data transformation and modeling tool, likely a system like dbt (data build tool). It transforms data from a reference table, typically in a data warehouse, into another table for further analysis or reporting.\n",
      "\n",
      "## Materialization Configuration\n",
      "\n",
      "```sql\n",
      "{{ config(materialized='table') }}\n",
      "```\n",
      "\n",
      "This line indicates that the SQL query should be materialized as a table in the database. Materialization refers to the way the result of a query is stored; in this case, the results will be saved in a physical table rather than just a view or temporary output. This is beneficial for performance reasons if the query will be run frequently, as it reduces the need to recompute the data every time it is accessed.\n",
      "\n",
      "## Data Selection\n",
      "\n",
      "```sql\n",
      "select \n",
      "    locationid, \n",
      "    borough, \n",
      "    zone, \n",
      "    replace(service_zone,'Boro','Green') as service_zone \n",
      "from {{ ref('taxi_zone_lookup') }}\n",
      "```\n",
      "\n",
      "This `SELECT` statement specifies the columns to be retrieved from the source table referenced as `taxi_zone_lookup`. Each column serves a specific purpose:\n",
      "\n",
      "### Selected Columns\n",
      "\n",
      "1. **locationid**: \n",
      "   - This column likely represents a unique identifier for various locations within the dataset, possibly indicating where taxi services operate or are available.\n",
      "\n",
      "2. **borough**: \n",
      "   - This column probably denotes the borough in which the location resides. In urban environments, boroughs are significant administrative regions that help in geographical and statistical analyses.\n",
      "\n",
      "3. **zone**: \n",
      "   - The zone column may refer to specific operational areas for taxi services, which can be relevant for determining fares, service availability, or traffic patterns.\n",
      "\n",
      "4. **service_zone**: \n",
      "   - This column is modified using the `replace` function, transforming occurrences of ‘Boro’ in the `service_zone` field to ‘Green’. This transformation suggests a naming convention or categorization change for service zones. The new naming could imply a particular type of service or area designation (e.g., \"Green\" might represent eco-friendly taxi services).\n",
      "\n",
      "### Data Source\n",
      "\n",
      "```sql\n",
      "from {{ ref('taxi_zone_lookup') }}\n",
      "```\n",
      "\n",
      "The `from` clause indicates that data is being pulled from a reference table named `taxi_zone_lookup`. Using `{{ ref('table_name') }}` is a common practice in dbt to establish dependencies and ensure the correct management and execution order of transformations. It helps in maintaining referential integrity across transformations and ensures that the table is built first before this SQL execution.\n",
      "\n",
      "## Summary of the Script's Purpose\n",
      "\n",
      "Overall, this script serves to create a new table that contains refined data about taxi service areas based on boroughs and zones. The use of the `replace` function helps standardize naming conventions in the service zones for clarity or consistency in downstream analyses.\n",
      "\n",
      "By materializing the results as a table, this design aims to optimize queries and improve performance for later retrieval and analysis of taxi service areas. This transformation lays the groundwork for subsequent analyses, reports, or even for other data transformations that might depend on this refined dataset.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "972a262d-febb-49d1-a975-6b50f30fbf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ config(materialized='table') }}\n",
      "\n",
      "select \n",
      "    locationid, \n",
      "    borough, \n",
      "    zone, \n",
      "    replace(service_zone,'Boro','Green') as service_zone \n",
      "from {{ ref('taxi_zone_lookup') }}\n"
     ]
    }
   ],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5dc37-ff31-4a98-8ed7-60556c9a2d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
