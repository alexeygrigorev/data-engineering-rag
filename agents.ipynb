{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86f9619-fc8c-4879-85f8-b5929fc1e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "695f1a3b-48b1-4074-8dec-586781bc26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/de-zoomcamp-processed.json', 'r') as f_in:\n",
    "    de_zoomcamp_data = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773c0ce6-f609-44ac-9aa6-d985a95dfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ff0cd3-50fc-44df-a108-99171953e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zoomcamp_chunks = []\n",
    "\n",
    "for doc in de_zoomcamp_data:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    de_zoomcamp_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34fc1bba-717d-4be2-ae73-3498de0506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f253e686-c801-4d2e-9b71-cecc9e878dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x242a2cbc2f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"filename\"],\n",
    ")\n",
    "\n",
    "index.fit(de_zoomcamp_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae1e05d7-ceb6-4ee6-8a91-298b69453b5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f50d33-3365-447a-bca8-5d1d515cf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.tools import Tools\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a656ea4-a159-47c4-af0c-eba94eb5858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = Tools()\n",
    "tools.add_tool(text_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de97be07-9150-4a20-a5f9-290924158e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You are a helpful assistant for a  course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a83095-3bff-46a0-ab87-4b4e84163b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "openai_client = OpenAIClient(model=\"gpt-4o-mini\")\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=openai_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bc0c757-3de9-46bf-b422-918f54cea6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: How do I run kafka in Python?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>text_search({\"query\":\"run kafka in Python\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"run kafka in Python\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_ehTzogZdTNHFwlaMAQNmjLPd', 'output': '[\\n  {\\n    \"start\": 0,\\n    \"chunk\": \"# Learning in public\\\\n\\\\nMost people learn in private: they consume content but don\\'t tell\\\\nanyone about it. There\\'s nothing wrong with it.\\\\n\\\\nBut we want to encourage you to document your progress and\\\\nshare it publicly on social media.\\\\n\\\\nIt helps you get noticed and will lead to:\\\\n\\\\n* Expanding your network: meeting new people and making new friends\\\\n* Being invited to meetups, conferences and podcasts\\\\n* Landing a job or getting clients\\\\n* Many other good things\\\\n\\\\nHere\\'s a more comprehensive reading on why you want to do it: https://github.com/readme/guides/publishing-your-work\\\\n\\\\n\\\\n## Learning in Public for Zoomcamps\\\\n\\\\nWhen you submit your homework or project, you can also submit\\\\nlearning in public posts:\\\\n\\\\n<img src=\\\\\"https://github.com/DataTalksClub/mlops-zoomcamp/raw/main/images/learning-in-public-links.png\\\\\" />\\\\n\\\\nYou can watch this video to see how your learning in public posts may look like:\\\\n\\\\n<a href=\\\\\"https://www.loom.com/share/710e3297487b409d94df0e8da1c984ce\\\\\" target=\\\\\"_blank\\\\\">\\\\n    <img src=\\\\\"https://github.com/DataTalksClub/mlops-zoomcamp/raw/main/images/learning-in-public.png\\\\\" height=\\\\\"240\\\\\" />\\\\n</a>\\\\n\\\\n## Daily Documentation\\\\n\\\\n- **Post Daily Diaries**: Document what you learn each day, including the challenges faced and the methods used to overcome them.\\\\n- **Create Quick Videos**: Make short videos showcasing your work and upload them to GitHub.\\\\n\\\\nSend a PR if you want to suggest improvements for this document\",\\n    \"filename\": \"learning-in-public.md\"\\n  },\\n  {\\n    \"start\": 0,\\n    \"chunk\": \"### Stream-Processing with Python\\\\n\\\\nIn this document, you will be finding information about stream processing \\\\nusing different Python libraries (`kafka-python`,`confluent-kafka`,`pyspark`, `faust`).\\\\n\\\\nThis Python module can be separated in following modules.\\\\n\\\\n####  1. Docker\\\\nDocker module includes, Dockerfiles and docker-compose definitions \\\\nto run Kafka and Spark in a docker container. Setting up required services is\\\\nthe prerequsite step for running following modules.\\\\n\\\\n#### 2. Kafka Producer - Consumer Examples\\\\n- [Json Producer-Consumer Example](json_example) using `kafka-python` library\\\\n- [Avro Producer-Consumer Example](avro_example) using `confluent-kafka` library\\\\n\\\\nBoth of these examples require, up-and running Kafka services, therefore please ensure\\\\nfollowing steps under [docker-README](docker/README.md)\\\\n\\\\nTo run the producer-consumer examples in the respective example folder, run following commands\\\\n```bash\\\\n# Start producer script\\\\npython3 producer.py\\\\n# Start consumer script\\\\npython3 consumer.py\\\\n```\",\\n    \"filename\": \"06-streaming/python/README.md\"\\n  },\\n  {\\n    \"start\": 2000,\\n    \"chunk\": \" service is running on the same machine.\\\\n\\\\n\\\\n### Bootstrap Servers\\\\n\\\\n```python\\\\nBOOTSTRAP_SERVERS = \\'localhost:9092\\'\\\\n```\\\\n\\\\nThis variable specifies the address of the Kafka broker(s) that the application will connect to for producing and consuming messages. In this case, it points to a local Kafka server running on port 9092. This is the entry point for communication with the Kafka ecosystem.\\\\n\\\\n\\\\n### Kafka Topic\\\\n\\\\n```python\\\\nKAFKA_TOPIC = \\'rides_avro\\'\\\\n```\\\\n\\\\nThis variable defines the Kafka topic named `rides_avro`, where the ride-sharing data will be published and consumed. Topics in Kafka serve as categories or feeds to which records are sent, and they play a critical role in how data is organized and processed.\\\\n\\\\n\\\\n## Summary\\\\n\\\\nIn summary, this code snippet serves as a fundamental configuration block for an application that aims to process taxi ride data using Kafka and Avro for serialization. The specified file paths, schema registry URL, bootstrap servers, and Kafka topic provide the necessary parameters for the program to effectively read ride data from a CSV file, validate it against defined schemas, and interact with Kafka\\'s messaging system for data flow.\",\\n    \"code\": false,\\n    \"filename\": \"06-streaming/python/avro_example/settings.py\"\\n  },\\n  {\\n    \"start\": 0,\\n    \"chunk\": \"# Kafka JSON Consumer Documentation\\\\n\\\\nThis document provides a high-level overview of a Kafka JSON consumer implemented using Python. The code is designed to consume messages from a Kafka topic, deserialize them into Python objects, and print key-value pairs to the console. Below is a structured breakdown of the code.\\\\n\\\\n## Imports and Dependencies\\\\n\\\\nThe script begins by importing essential libraries:\\\\n\\\\n```python\\\\nimport os\\\\nfrom typing import Dict, List\\\\nfrom json import loads\\\\nfrom kafka import KafkaConsumer\\\\n\\\\nfrom ride import Ride\\\\nfrom settings import BOOTSTRAP_SERVERS, KAFKA_TOPIC\\\\n```\\\\n\\\\n1. **Standard Libraries**: \\\\n    - `os`: Standard library for interacting with the operating system, though not explicitly used in the code.\\\\n    - `typing`: Provides type hints for better readability and type checking. `Dict` and `List` are imported for type annotations.\\\\n    - `json`: The `loads` function is used to deserialize JSON strings into Python objects.\\\\n  \\\\n2. **Kafka Library**: `KafkaConsumer` is imported from the `kafka` library for consuming messages from Kafka topics.\\\\n\\\\n3. **Local Modules**: \\\\n    - `Ride`: A custom class presumably defined in a separate module, used to deserialize message values.\\\\n    - `settings`: Contains application configuration such as bootstrap servers and Kafka topic names.\\\\n\\\\n## Class Definition: `JsonConsumer`\\\\n\\\\n### Purpose\\\\n\\\\nThe `JsonConsumer` class is designed to encapsulate the functionality required to create a Kafka consumer, subscribe to specified topics, and consume messages.\\\\n\\\\n### Constructor\\\\n\\\\n```python\\\\ndef __init__(self, props: Dict):\\\\n    self.consumer = KafkaConsumer(**props)\\\\n```\\\\n\\\\n- The constructor initializes the Kafka consumer with properties passed as a `Dict`. This allows for flexible configuration.\\\\n\\\\n### Method: `consume_from_kafka`\\\\n\\\\n```python\\\\ndef consume_from_kafka(self, topics: List[str]):\\\\n```\\\\n\\\\n- This method subscribes to one or more Kafka topics and enters an infinite loop to consume messages continuously.\\\\n\\\\n#### Subscription and Initialization\\\\n\\\\n\",\\n    \"code\": false,\\n    \"filename\": \"06-streaming/python/redpanda_example/consumer.py\"\\n  },\\n  {\\n    \"start\": 0,\\n    \"chunk\": \"# Kafka CSV Producer\\\\n\\\\nThis script is designed to read ride data from a CSV file and send it to a Kafka topic using a Kafka producer. It leverages the `csv`, `json`, and `kafka` libraries in Python, executing a straightforward process to facilitate the transfer of ride information.\\\\n\\\\n## Libraries and Dependencies\\\\n\\\\nThe script begins with the following imports:\\\\n\\\\n```python\\\\nimport csv\\\\nfrom json import dumps\\\\nfrom kafka import KafkaProducer\\\\nfrom time import sleep\\\\n```\\\\n\\\\n- **csv**: This module is used for reading and parsing CSV files. It allows for ease of access to data structured in rows and columns.\\\\n  \\\\n- **json.dumps**: This function converts Python objects into JSON strings, making it suitable for serializing the data before sending it over Kafka.\\\\n\\\\n- **KafkaProducer**: This class from the `kafka` library is used to send messages to a Kafka broker. The producer will be configured later with serializers for the keys and values.\\\\n\\\\n- **time.sleep**: This function pauses the execution of the script for a specified amount of time, allowing for the control of message flow to the Kafka topic.\\\\n\\\\n## Kafka Producer Initialization\\\\n\\\\nA Kafka producer is initialized with specific configurations:\\\\n\\\\n```python\\\\nproducer = KafkaProducer(bootstrap_servers=[\\'localhost:9092\\'],\\\\n                         key_serializer=lambda x: dumps(x).encode(\\'utf-8\\'),\\\\n                         value_serializer=lambda x: dumps(x).encode(\\'utf-8\\'))\\\\n```\\\\n\\\\n- **bootstrap_servers**: This parameter specifies the address of the Kafka broker. In this case, it points to a local server running on port 9092.\\\\n\\\\n- **key_serializer**: This lambda function serializes the key into a JSON string format and encodes it in UTF-8. The serialization allows the key to be sent as a uniform message over Kafka.\\\\n\\\\n- **value_serializer**: Similar to the key serializer, this function serializes the value to JSON format and encodes it to ensure compatibility with Kafka message formats.\\\\n\\\\n## CSV File Handling\\\\n\\\\nNext, the script opens the CSV file that \",\\n    \"code\": false,\\n    \"filename\": \"06-streaming/python/streams-example/faust/producer_taxi_json.py\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To run Kafka in Python, you can use libraries such as <code>kafka-python</code> or <code>confluent-kafka</code>. Below are the steps to set up Kafka and run producer-consumer examples using these libraries.</p>\n",
       "<h3>Prerequisites</h3>\n",
       "<ol>\n",
       "<li><p><strong>Kafka Setup</strong>: Ensure you have a running Kafka instance. This can be done locally or using Docker.</p>\n",
       "<ul>\n",
       "<li>You can find guidance on how to set up Kafka in a Docker container in dedicated Docker documentation.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Install Required Libraries</strong>:\n",
       "You may need to install the necessary Python libraries:</p>\n",
       "<pre><code class=\"language-bash\">pip install kafka-python\n",
       "pip install confluent-kafka  # if you're using Confluent's client\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Basic Example of Producer and Consumer</h3>\n",
       "<ol>\n",
       "<li><p><strong>Kafka Producer Example Using <code>kafka-python</code></strong>:</p>\n",
       "<pre><code class=\"language-python\">from kafka import KafkaProducer\n",
       "import json\n",
       "\n",
       "producer = KafkaProducer(\n",
       "    bootstrap_servers=['localhost:9092'],\n",
       "    value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
       ")\n",
       "\n",
       "producer.send('your_topic_name', value={'key': 'value'})\n",
       "producer.close()\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Kafka Consumer Example Using <code>kafka-python</code></strong>:</p>\n",
       "<pre><code class=\"language-python\">from kafka import KafkaConsumer\n",
       "import json\n",
       "\n",
       "consumer = KafkaConsumer(\n",
       "    'your_topic_name',\n",
       "    bootstrap_servers=['localhost:9092'],\n",
       "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
       ")\n",
       "\n",
       "for message in consumer:\n",
       "    print(message.value)\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Running the Scripts</h3>\n",
       "<ol>\n",
       "<li>Start your Kafka broker.</li>\n",
       "<li>Run the producer script:<pre><code class=\"language-bash\">python3 producer.py\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>In another terminal, run the consumer script:<pre><code class=\"language-bash\">python3 consumer.py\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Configuration</h3>\n",
       "<p>Make sure you define the <code>BOOTSTRAP_SERVERS</code> (usually <code>localhost:9092</code> if running locally) and the <code>KAFKA_TOPIC</code> you want to use.</p>\n",
       "<h3>Example Project Structure</h3>\n",
       "<p>Your Python files could be structured as follows:</p>\n",
       "<pre><code>/kafka_project\n",
       "    |-- producer.py\n",
       "    |-- consumer.py\n",
       "</code></pre>\n",
       "<p>This should give you a solid foundation to work with Kafka in your Python applications! If you need more specific information, feel free to ask.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a253047a-68a1-402a-800f-8b9f968224b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9112efb3-9e4d-44ad-a745-e48e368a27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=developer_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07b8caa9-1dd9-49c6-95b3-c00e1622116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install and use Kafka with Python, you can use the `kafka-python` library or the `confluent-kafka` library. Here’s a general guide to get you started:\n",
      "\n",
      "### 1. Install Required Libraries\n",
      "\n",
      "You can install the `kafka-python` library using pip:\n",
      "\n",
      "```bash\n",
      "pip install kafka-python\n",
      "```\n",
      "\n",
      "For the `confluent-kafka` library, you can install it using:\n",
      "\n",
      "```bash\n",
      "pip install confluent-kafka\n",
      "```\n",
      "\n",
      "### 2. Set Up Kafka\n",
      "\n",
      "Before you run your Python scripts, ensure that you have a Kafka server running. You can use Docker to quickly set up Kafka if you prefer that approach. Check the following steps:\n",
      "\n",
      "#### Using Docker\n",
      "\n",
      "You can pull the Kafka image and run it as follows:\n",
      "\n",
      "```bash\n",
      "docker run -d --name zookeeper -p 2181:2181 zookeeper:3.7\n",
      "docker run -d --name kafka --link zookeeper -p 9092:9092 wurstmeister/kafka\n",
      "```\n",
      "\n",
      "### 3. Create a Kafka Producer and Consumer\n",
      "\n",
      "Here's a simple example using `kafka-python` to create a producer and consumer:\n",
      "\n",
      "#### Kafka Producer Example\n",
      "\n",
      "```python\n",
      "from kafka import KafkaProducer\n",
      "from json import dumps\n",
      "\n",
      "# Initialize producer\n",
      "producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
      "                         value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
      "\n",
      "# Send a message\n",
      "producer.send('my_topic', value={'key': 'value'})\n",
      "producer.flush()\n",
      "```\n",
      "\n",
      "#### Kafka Consumer Example\n",
      "\n",
      "```python\n",
      "from kafka import KafkaConsumer\n",
      "from json import loads\n",
      "\n",
      "# Initialize consumer\n",
      "consumer = KafkaConsumer('my_topic',\n",
      "                         bootstrap_servers=['localhost:9092'],\n",
      "                         auto_offset_reset='earliest',\n",
      "                         enable_auto_commit=True,\n",
      "                         group_id='my-group',\n",
      "                         value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
      "\n",
      "for message in consumer:\n",
      "    print(f\"Received message: {message.value}\")\n",
      "```\n",
      "\n",
      "### Summary\n",
      "\n",
      "1. Install the necessary Kafka Python libraries.\n",
      "2. Start a Kafka instance on your machine (using Docker or any other method).\n",
      "3. Create producer and consumer scripts as shown above to start sending and receiving messages.\n",
      "\n",
      "If you follow these steps, you should be able to successfully install and use Kafka with Python. If you have any specific questions or issues, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e5f81-8f91-471d-94cd-29fbbfe7ac26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069b465-62a2-4d4b-90d5-ada3863997ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc8eb6-ae8f-4365-8a6b-5392777a1d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b471615-31e1-4542-89ac-7fdd31811392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb9b6a-05ab-48e6-8b97-8e99ce2acae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
