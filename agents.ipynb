{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86f9619-fc8c-4879-85f8-b5929fc1e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "695f1a3b-48b1-4074-8dec-586781bc26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/de-zoomcamp-processed.json', 'r') as f_in:\n",
    "    de_zoomcamp_data = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773c0ce6-f609-44ac-9aa6-d985a95dfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ff0cd3-50fc-44df-a108-99171953e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zoomcamp_chunks = []\n",
    "\n",
    "for doc in de_zoomcamp_data:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    de_zoomcamp_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9154fe51-fcfc-4a59-8aad-0f3e6e3c7c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 1000,\n",
       " 'chunk': 'taxi\" \\\\\\n  -v e:/zoomcamp/data_engineer/week_1_fundamentals/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data  \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\\n\\ndocker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\\nSee \\'docker run --help\\'.\\n```\\n\\nChange the mounting path. Replace it with the following:\\n\\n```\\n-v /e/zoomcamp/...:/var/lib/postgresql/data\\n```\\n\\n#### Linux and MacOS\\n\\n\\n```bash\\ndocker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"root\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\\n```\\n\\nIf you see that `ny_taxi_postgres_data` is empty after running\\nthe container, try these:\\n\\n* Deleting the folder and running Docker again (Docker will re-create the folder)\\n* Adjust the permissions of the folder by running `sudo chmod a+rwx ny_taxi_postgres_data`\\n\\n\\n### CLI for Postgres\\n\\nInstalling `pgcli`\\n\\n```bash\\npip install pgcli\\n```\\n\\nIf you have problems installing `pgcli` with the command above, try this:\\n\\n```bash\\nconda install -c conda-forge pgcli\\npip install -U mycli\\n```\\n\\nUsing `pgcli` to connect to Postgres\\n\\n```bash\\npgcli -h localhost -p 5432 -u root -d ny_taxi\\n```\\n\\n\\n### NY Trips Dataset\\n\\nDataset:\\n\\n* https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\\n* https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\\n\\n> According to the [TLC data website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page),\\n> from 05/13/2022, the data will be in ```.parquet``` format instead of ```.csv```\\n> The website has provided a useful [link](https://www1.nyc.gov/assets/tlc/downloads/pdf/working_parquet_format.pdf) with sample steps to read ```.parquet``` file and convert it to Pandas data frame.\\n>\\n> You can use the csv backup located here, https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz, to follow along with the video.\\n```\\n$ aws s3 ls s3://nyc-tlc\\n                  ',\n",
       " 'filename': '01-docker-terraform/2_docker_sql/README.md'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_zoomcamp_chunks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34fc1bba-717d-4be2-ae73-3498de0506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f253e686-c801-4d2e-9b71-cecc9e878dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x278521fc2f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(de_zoomcamp_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae1e05d7-ceb6-4ee6-8a91-298b69453b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': '### Stream-Processing with Python\\n\\nIn this document, you will be finding information about stream processing \\nusing different Python libraries (`kafka-python`,`confluent-kafka`,`pyspark`, `faust`).\\n\\nThis Python module can be separated in following modules.\\n\\n####  1. Docker\\nDocker module includes, Dockerfiles and docker-compose definitions \\nto run Kafka and Spark in a docker container. Setting up required services is\\nthe prerequsite step for running following modules.\\n\\n#### 2. Kafka Producer - Consumer Examples\\n- [Json Producer-Consumer Example](json_example) using `kafka-python` library\\n- [Avro Producer-Consumer Example](avro_example) using `confluent-kafka` library\\n\\nBoth of these examples require, up-and running Kafka services, therefore please ensure\\nfollowing steps under [docker-README](docker/README.md)\\n\\nTo run the producer-consumer examples in the respective example folder, run following commands\\n```bash\\n# Start producer script\\npython3 producer.py\\n# Start consumer script\\npython3 consumer.py\\n```',\n",
       "  'filename': '06-streaming/python/README.md'},\n",
       " {'start': 2000,\n",
       "  'chunk': \" service is running on the same machine.\\n\\n\\n### Bootstrap Servers\\n\\n```python\\nBOOTSTRAP_SERVERS = 'localhost:9092'\\n```\\n\\nThis variable specifies the address of the Kafka broker(s) that the application will connect to for producing and consuming messages. In this case, it points to a local Kafka server running on port 9092. This is the entry point for communication with the Kafka ecosystem.\\n\\n\\n### Kafka Topic\\n\\n```python\\nKAFKA_TOPIC = 'rides_avro'\\n```\\n\\nThis variable defines the Kafka topic named `rides_avro`, where the ride-sharing data will be published and consumed. Topics in Kafka serve as categories or feeds to which records are sent, and they play a critical role in how data is organized and processed.\\n\\n\\n## Summary\\n\\nIn summary, this code snippet serves as a fundamental configuration block for an application that aims to process taxi ride data using Kafka and Avro for serialization. The specified file paths, schema registry URL, bootstrap servers, and Kafka topic provide the necessary parameters for the program to effectively read ride data from a CSV file, validate it against defined schemas, and interact with Kafka's messaging system for data flow.\",\n",
       "  'code': False,\n",
       "  'filename': '06-streaming/python/avro_example/settings.py'},\n",
       " {'start': 0,\n",
       "  'chunk': '# Kafka JSON Consumer Documentation\\n\\nThis document provides a high-level overview of a Kafka JSON consumer implemented using Python. The code is designed to consume messages from a Kafka topic, deserialize them into Python objects, and print key-value pairs to the console. Below is a structured breakdown of the code.\\n\\n## Imports and Dependencies\\n\\nThe script begins by importing essential libraries:\\n\\n```python\\nimport os\\nfrom typing import Dict, List\\nfrom json import loads\\nfrom kafka import KafkaConsumer\\n\\nfrom ride import Ride\\nfrom settings import BOOTSTRAP_SERVERS, KAFKA_TOPIC\\n```\\n\\n1. **Standard Libraries**: \\n    - `os`: Standard library for interacting with the operating system, though not explicitly used in the code.\\n    - `typing`: Provides type hints for better readability and type checking. `Dict` and `List` are imported for type annotations.\\n    - `json`: The `loads` function is used to deserialize JSON strings into Python objects.\\n  \\n2. **Kafka Library**: `KafkaConsumer` is imported from the `kafka` library for consuming messages from Kafka topics.\\n\\n3. **Local Modules**: \\n    - `Ride`: A custom class presumably defined in a separate module, used to deserialize message values.\\n    - `settings`: Contains application configuration such as bootstrap servers and Kafka topic names.\\n\\n## Class Definition: `JsonConsumer`\\n\\n### Purpose\\n\\nThe `JsonConsumer` class is designed to encapsulate the functionality required to create a Kafka consumer, subscribe to specified topics, and consume messages.\\n\\n### Constructor\\n\\n```python\\ndef __init__(self, props: Dict):\\n    self.consumer = KafkaConsumer(**props)\\n```\\n\\n- The constructor initializes the Kafka consumer with properties passed as a `Dict`. This allows for flexible configuration.\\n\\n### Method: `consume_from_kafka`\\n\\n```python\\ndef consume_from_kafka(self, topics: List[str]):\\n```\\n\\n- This method subscribes to one or more Kafka topics and enters an infinite loop to consume messages continuously.\\n\\n#### Subscription and Initialization\\n\\n',\n",
       "  'code': False,\n",
       "  'filename': '06-streaming/python/redpanda_example/consumer.py'},\n",
       " {'start': 0,\n",
       "  'chunk': \"# Kafka CSV Producer\\n\\nThis script is designed to read ride data from a CSV file and send it to a Kafka topic using a Kafka producer. It leverages the `csv`, `json`, and `kafka` libraries in Python, executing a straightforward process to facilitate the transfer of ride information.\\n\\n## Libraries and Dependencies\\n\\nThe script begins with the following imports:\\n\\n```python\\nimport csv\\nfrom json import dumps\\nfrom kafka import KafkaProducer\\nfrom time import sleep\\n```\\n\\n- **csv**: This module is used for reading and parsing CSV files. It allows for ease of access to data structured in rows and columns.\\n  \\n- **json.dumps**: This function converts Python objects into JSON strings, making it suitable for serializing the data before sending it over Kafka.\\n\\n- **KafkaProducer**: This class from the `kafka` library is used to send messages to a Kafka broker. The producer will be configured later with serializers for the keys and values.\\n\\n- **time.sleep**: This function pauses the execution of the script for a specified amount of time, allowing for the control of message flow to the Kafka topic.\\n\\n## Kafka Producer Initialization\\n\\nA Kafka producer is initialized with specific configurations:\\n\\n```python\\nproducer = KafkaProducer(bootstrap_servers=['localhost:9092'],\\n                         key_serializer=lambda x: dumps(x).encode('utf-8'),\\n                         value_serializer=lambda x: dumps(x).encode('utf-8'))\\n```\\n\\n- **bootstrap_servers**: This parameter specifies the address of the Kafka broker. In this case, it points to a local server running on port 9092.\\n\\n- **key_serializer**: This lambda function serializes the key into a JSON string format and encodes it in UTF-8. The serialization allows the key to be sent as a uniform message over Kafka.\\n\\n- **value_serializer**: Similar to the key serializer, this function serializes the value to JSON format and encodes it to ensure compatibility with Kafka message formats.\\n\\n## CSV File Handling\\n\\nNext, the script opens the CSV file that \",\n",
       "  'code': False,\n",
       "  'filename': '06-streaming/python/streams-example/faust/producer_taxi_json.py'},\n",
       " {'start': 0,\n",
       "  'chunk': '# Kafka JSON Consumer Documentation\\n\\nThis document provides a high-level overview of a Python script that implements a Kafka consumer for JSON messages. The script utilizes the `kafka-python` library to connect to a Kafka broker, consume messages from a specified topic, and deserialize the JSON messages into Python objects. \\n\\n## Overview\\n\\nThe script consists of a **JsonConsumer** class responsible for connecting to Kafka and consuming messages, and a stand-alone execution block that initializes and runs the consumer. The primary focus of this code is to read messages from a Kafka topic, print the keys and values of the messages, and deserialize the JSON data into `Ride` objects for further processing.\\n\\n## Key Components\\n\\n### Imports\\n\\nThe necessary modules and classes are imported at the beginning of the script:\\n- `Dict` and `List` from the `typing` module for type hinting.\\n- `loads` from the `json` module for JSON deserialization.\\n- `KafkaConsumer` from the `kafka` package to facilitate interaction with the Kafka messaging system.\\n- The `Ride` class from a local module that likely defines how to structure the ride data.\\n- `BOOTSTRAP_SERVERS` and `KAFKA_TOPIC` constants, presumably defined in a `settings` module, which configure the Kafka connection and topic.\\n\\n### JsonConsumer Class\\n\\nThe **JsonConsumer** class encapsulates the logic for consuming messages from Kafka:\\n\\n- **`__init__` method**: \\n  - The constructor initializes a Kafka consumer instance using the provided properties (props) dictionary, which includes configuration settings like server details and deserialization functions. \\n\\n- **`consume_from_kafka` method**: \\n  - This method manages the workflow of subscribing to specified Kafka topics and consuming messages in a loop. \\n  - The consumer subscribes to the given topics and starts a loop to poll for messages every second.\\n  - If a keyboard interrupt (SIGINT) occurs, the loop exits gracefully, and the consumer closes the connection. \\n  - The messages are ',\n",
       "  'code': False,\n",
       "  'filename': '06-streaming/python/json_example/consumer.py'},\n",
       " {'start': 0,\n",
       "  'chunk': \"# Kafka Consumer Documentation\\n\\n## Overview\\nThis code implements a Kafka consumer that reads messages from a specified Kafka topic, particularly aimed at processing CSV data related to rides. It utilizes the `kafka-python` library to manage the communication with Kafka. The consumer will connect to a Kafka instance, subscribe to a topic, and continuously poll for messages, printing their keys and values until interrupted.\\n\\n## Dependencies\\nThe script imports several modules:\\n- `argparse`: Used for command-line argument parsing.\\n- `Dict` and `List` from `typing`: Used for type hinting, enhancing code readability.\\n- `KafkaConsumer` from `kafka`: Main class to interact with Kafka for consuming messages.\\n- `settings`: Custom settings module that provides configuration variables for Kafka connection.\\n\\n## RideCSVConsumer Class\\n### Initialization\\n```python\\nclass RideCSVConsumer:\\n    def __init__(self, props: Dict):\\n        self.consumer = KafkaConsumer(**props)\\n```\\nThis class is initialized with a dictionary of properties (`props`) necessary for the Kafka consumer. The `KafkaConsumer` instance is created using these properties, establishing the connection to the Kafka broker(s).\\n\\n### Consuming Messages\\n```python\\ndef consume_from_kafka(self, topics: List[str]):\\n```\\nThe `consume_from_kafka` method subscribes to the specified Kafka topics and enters a loop that polls for messages. \\n\\n- **Subscription**: It subscribes to the provided topics and confirms the subscription.\\n- **Message Polling**: The method repeatedly polls for new messages using a timeout of 1 second.\\n  - If no messages are returned, it continues polling.\\n  - Upon receiving messages, it iterates through them, printing out each message's key and value along with their data types.\\n  \\n### Handling Interruptions\\nThe method contains a try-except block to gracefully handle keyboard interrupts (SIGINT). When such an interrupt is detected, the loop is exited, and the consumer connection is closed.\\n\\n```python\\nself.consumer\",\n",
       "  'code': False,\n",
       "  'filename': '06-streaming/python/streams-example/pyspark/consumer.py'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"c'\\n```\\n\\nThese variables define paths to Avro schema files used to enforce data structure and validation for the ride-sharing data. \\n\\n- **Key Schema** (`taxi_ride_key.avsc`): This schema is likely responsible for defining the structure of data keys used in Kafka, typically to identify unique records.\\n  \\n- **Value Schema** (`taxi_ride_value.avsc`): This schema describes the structure of the data values (payload) carried by the records relating to taxi rides, dictating what fields should be included and their types (e.g., strings, integers, dates).\\n\\n\\n## Kafka Configuration\\n\\n### Schema Registry URL\\n\\n```python\\nSCHEMA_REGISTRY_URL = 'http://localhost:8081'\\n```\\n\\nThis variable indicates the URL of the Schema Registry, which is essential for managing and retrieving Avro schemas dynamically. The Schema Registry exists as a service that allows producers and consumers of Kafka messages to store and retrieve schema definitions. The URL points to a local instance, suggesting that the schema registry service is running on the same machine.\\n\\n\\n### Bootstrap Servers\\n\\n```python\\nBOOTSTRAP_SERVERS = 'localhost:9092'\\n```\\n\\nThis variable specifies the address of the Kafka broker(s) that the application will connect to for producing and consuming messages. In this case, it points to a local Kafka server running on port 9092. This is the entry point for communication with the Kafka ecosystem.\\n\\n\\n### Kafka Topic\\n\\n```python\\nKAFKA_TOPIC = 'rides_avro'\\n```\\n\\nThis variable defines the Kafka topic named `rides_avro`, where the ride-sharing data will be published and consumed. Topics in Kafka serve as categories or feeds to which records are sent, and they play a critical role in how data is organized and processed.\\n\\n\\n## Summary\\n\\nIn summary, this code snippet serves as a fundamental configuration block for an application that aims to process taxi ride data using Kafka and Avro for serialization. The specified file paths, schema registry URL, bootstrap servers, and Kafka topic provide the necessary pa\",\n",
       "  'code': False,\n",
       "  'filename': '06-streaming/python/avro_example/settings.py'},\n",
       " {'start': 2000,\n",
       "  'chunk': \"terates over a list of `Ride` objects, converting each one into a message format suitable for Kafka.\\n  - Calls `self.producer.send()` to publish each ride.\\n  - Handles `KafkaTimeoutError` to catch errors during sending, logging the issue to the console alongside the affected ride's location ID.\\n\\n## Main Execution Block\\n```python\\nif __name__ == '__main__':\\n```\\n- This block ensures that the following code only runs when the script is executed as a standalone program and not when imported as a module.\\n\\n### Configuration Initialization\\n```python\\nconfig = {\\n    'bootstrap_servers': BOOTSTRAP_SERVERS,\\n    'key_serializer': lambda key: str(key).encode(),\\n    'value_serializer': lambda x: json.dumps(x.__dict__, default=str).encode('utf-8')\\n}\\n```\\n- Constructs a configuration dictionary containing:\\n  - `bootstrap_servers`: The address(es) of Kafka brokers, loaded from the settings.\\n  - `key_serializer`: A lambda function that encodes keys as byte strings for Kafka compatibility.\\n  - `value_serializer`: A lambda function that converts objects to JSON format before sending to Kafka.\\n\\n### Producer Instantiation and Record Processing\\n```python\\nproducer = JsonProducer(props=config)\\n```\\n- Creates an instance of `JsonProducer` using the defined configuration.\\n\\n### Reading Records\\n```python\\nrides = producer.read_records(resource_path=INPUT_DATA_PATH)\\n```\\n- Calls the `read_records` method to load ride data from the specified CSV file path (retrieved from settings).\\n\\n### Publishing Rides to Kafka\\n```python\\nproducer.publish_rides(topic=KAFKA_TOPIC, messages=rides)\\n```\\n- Finally, the script calls the `publish_rides` method to send all the ride records to the predefined Kafka topic.\\n\\n## Summary\\nIn summary, this script establishes a Kafka producer, reads ride data from a specified CSV file converting each row into `Ride` objects, and publishes these rides as JSON-serialized messages to a specified Kafka topic. It handles potential errors during the publishing process, making it robust for \",\n",
       "  'code': False,\n",
       "  'filename': '06-streaming/python/redpanda_example/producer.py'},\n",
       " {'start': 0,\n",
       "  'chunk': '# Running Spark and Kafka Clusters on Docker\\n\\n### 1. Build Required Images for running Spark\\n\\nThe details of how to spark-images are build in different layers can be created can be read through \\nthe blog post written by André Perez on [Medium blog -Towards Data Science](https://towardsdatascience.com/apache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445)\\n\\n```bash\\n# Build Spark Images\\n./build.sh \\n```\\n\\n### 2. Create Docker Network & Volume\\n\\n```bash\\n# Create Network\\ndocker network  create kafka-spark-network\\n\\n# Create Volume\\ndocker volume create --name=hadoop-distributed-file-system\\n```\\n\\n### 3. Run Services on Docker\\n```bash\\n# Start Docker-Compose (within for kafka and spark folders)\\ndocker compose up -d\\n```\\nIn depth explanation of [Kafka Listeners](https://www.confluent.io/blog/kafka-listeners-explained/)\\n\\nExplanation of [Kafka Listeners](https://www.confluent.io/blog/kafka-listeners-explained/)\\n\\n### 4. Stop Services on Docker\\n```bash\\n# Stop Docker-Compose (within for kafka and spark folders)\\ndocker compose down\\n```\\n\\n### 5. Helpful Comands\\n```bash\\n# Delete all Containers\\ndocker rm -f $(docker ps -a -q)\\n\\n# Delete all volumes\\ndocker volume rm $(docker volume ls -q)\\n```',\n",
       "  'filename': '06-streaming/python/docker/README.md'},\n",
       " {'start': 0,\n",
       "  'chunk': '# High-Level Description of the Code\\n\\nThis script sets up a Kafka producer that reads ride data from a CSV file and publishes it to a specified Kafka topic after converting the data into JSON format. It utilizes the `kafka-python` library to interact with Kafka, and the `Ride` class is presumably defined in another module, modeling the ride data structure. This script emphasizes modularity through the use of a dedicated class for producing JSON data and encapsulating the related logic.\\n\\n## Imports and Dependencies\\n\\nThe script imports several modules:\\n- **csv**: To handle CSV file operations, allowing the reading of ride data from a CSV file.\\n- **json**: For converting objects into JSON format to be sent to Kafka.\\n- **List** and **Dict**: Type hinting provided by the `typing` library for better code clarity.\\n- **KafkaProducer** and **KafkaTimeoutError**: Classes from the `kafka` module for producing messages to Kafka topics and handling potential timeout errors.\\n- **Ride**: A custom class presumably defined in another module, which represents the structure of a ride.\\n- **settings**: A module that includes configuration settings like Kafka server addresses, CSV input path, and Kafka topic.\\n\\n## JsonProducer Class\\n\\n### Overview\\n\\nThe `JsonProducer` class inherits from `KafkaProducer`. It serves as a specialized producer that can handle JSON serialization of ride data.\\n\\n### Initialization\\n\\n```python\\ndef __init__(self, props: Dict):\\n    self.producer = KafkaProducer(**props)\\n```\\n\\nThe constructor initializes the `JsonProducer` class by accepting a dictionary of properties (`props`) required for the `KafkaProducer`. This allows it to configure the Kafka connection according to the provided settings.\\n\\n### Read Records Method\\n\\n```python\\n@staticmethod\\ndef read_records(resource_path: str):\\n    ...\\n```\\n\\n- **Purpose**: This static method reads ride records from a specified CSV file.\\n- **Functionality**:\\n  - Opens the CSV file and uses a CSV reader to read its contents.\\n  - Skips t',\n",
       "  'code': False,\n",
       "  'filename': '06-streaming/python/json_example/producer.py'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search('kafka python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389a6d1-e2b3-4e38-ae2a-b44726d79f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f50d33-3365-447a-bca8-5d1d515cf948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
