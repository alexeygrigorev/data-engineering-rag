[
  {
    "content": "## Terraform Overview\n\n[Video](https://www.youtube.com/watch?v=18jIzE41fJ4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=2)\n\n### Concepts\n\n#### Introduction\n\n1. What is [Terraform](https://www.terraform.io)?\n   * open-source tool by [HashiCorp](https://www.hashicorp.com), used for provisioning infrastructure resources\n   * supports DevOps best practices for change management\n   * Managing configuration files in source control to maintain an ideal provisioning state \n     for testing and production environments\n2. What is IaC?\n   * Infrastructure-as-Code\n   * build, change, and manage your infrastructure in a safe, consistent, and repeatable way \n     by defining resource configurations that you can version, reuse, and share.\n3. Some advantages\n   * Infrastructure lifecycle management\n   * Version control commits\n   * Very useful for stack-based deployments, and with cloud providers such as AWS, GCP, Azure, K8S\u2026\n   * State-based approach to track resource changes throughout deployments\n\n\n#### Files\n\n* `main.tf`\n* `variables.tf`\n* Optional: `resources.tf`, `output.tf`\n* `.tfstate`\n\n#### Declarations\n* `terraform`: configure basic Terraform settings to provision your infrastructure\n   * `required_version`: minimum Terraform version to apply to your configuration\n   * `backend`: stores Terraform's \"state\" snapshots, to map real-world resources to your configuration.\n      * `local`: stores state file locally as `terraform.tfstate`\n   * `required_providers`: specifies the providers required by the current module\n* `provider`:\n   * adds a set of resource types and/or data sources that Terraform can manage\n   * The Terraform Registry is the main directory of publicly available providers from most major infrastructure platforms.\n* `resource`\n  * blocks to define components of your infrastructure\n  * Project modules/resources: google_storage_bucket, google_bigquery_dataset, google_bigquery_table\n* `variable` & `locals`\n  * runtime arguments and constants\n\n\n#### Execution steps\n1. `terraform init`: \n    * Initializes & configures the backend, installs plugins/providers, & checks out an existing configuration from a version control \n2. `terraform plan`:\n    * Matches/previews local changes against a remote state, and proposes an Execution Plan.\n3. `terraform apply`: \n    * Asks for approval to the proposed plan, and applies changes to cloud\n4. `terraform destroy`\n    * Removes your stack from the Cloud\n\n\n### Terraform Workshop to create GCP Infra\nContinue [here](./terraform): `week_1_basics_n_setup/1_terraform_gcp/terraform`\n\n\n### References\nhttps://learn.hashicorp.com/collections/terraform/gcp-get-started",
    "filename": "01-docker-terraform/1_terraform_gcp/1_terraform_overview.md"
  },
  {
    "content": "## GCP Overview\n\n[Video](https://www.youtube.com/watch?v=18jIzE41fJ4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=2)\n\n\n### Project infrastructure modules in GCP:\n* Google Cloud Storage (GCS): Data Lake\n* BigQuery: Data Warehouse\n\n(Concepts explained in Week 2 - Data Ingestion)\n\n### Initial Setup\n\nFor this course, we'll use a free version (upto EUR 300 credits). \n\n1. Create an account with your Google email ID \n2. Setup your first [project](https://console.cloud.google.com/) if you haven't already\n    * eg. \"DTC DE Course\", and note down the \"Project ID\" (we'll use this later when deploying infra with TF)\n3. Setup [service account & authentication](https://cloud.google.com/docs/authentication/getting-started) for this project\n    * Grant `Viewer` role to begin with.\n    * Download service-account-keys (.json) for auth.\n4. Download [SDK](https://cloud.google.com/sdk/docs/quickstart) for local setup\n5. Set environment variable to point to your downloaded GCP keys:\n   ```shell\n   export GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json\"\n   \n   # Refresh token/session, and verify authentication\n   gcloud auth application-default login\n   ```\n   \n### Setup for Access\n \n1. [IAM Roles](https://cloud.google.com/storage/docs/access-control/iam-roles) for Service account:\n   * Go to the *IAM* section of *IAM & Admin* https://console.cloud.google.com/iam-admin/iam\n   * Click the *Edit principal* icon for your service account.\n   * Add these roles in addition to *Viewer* : **Storage Admin** + **Storage Object Admin** + **BigQuery Admin**\n   \n2. Enable these APIs for your project:\n   * https://console.cloud.google.com/apis/library/iam.googleapis.com\n   * https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com\n   \n3. Please ensure `GOOGLE_APPLICATION_CREDENTIALS` env-var is set.\n   ```shell\n   export GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json\"\n   ```\n \n### Terraform Workshop to create GCP Infra\nContinue [here](./terraform): `week_1_basics_n_setup/1_terraform_gcp/terraform`",
    "filename": "01-docker-terraform/1_terraform_gcp/2_gcp_overview.md"
  },
  {
    "content": "## Local Setup for Terraform and GCP\n\n### Pre-Requisites\n1. Terraform client installation: https://www.terraform.io/downloads\n2. Cloud Provider account: https://console.cloud.google.com/ \n\n### Terraform Concepts\n[Terraform Overview](1_terraform_overview.md)\n\n### GCP setup\n\n1. [Setup for First-time](2_gcp_overview.md#initial-setup)\n    * [Only for Windows](windows.md) - Steps 4 & 5\n2. [IAM / Access specific to this course](2_gcp_overview.md#setup-for-access)\n\n### Terraform Workshop for GCP Infra\nYour setup is ready!\nNow head to the [terraform](terraform) directory, and perform the execution steps to create your infrastructure.",
    "filename": "01-docker-terraform/1_terraform_gcp/README.md"
  },
  {
    "content": "### Concepts\n* [Terraform_overview](../1_terraform_overview.md)\n\n### Execution\n\n```shell\n# Refresh service-account's auth-token for this session\ngcloud auth application-default login\n\n# Initialize state file (.tfstate)\nterraform init\n\n# Check changes to new infra plan\nterraform plan -var=\"project=<your-gcp-project-id>\"\n```\n\n```shell\n# Create new infra\nterraform apply -var=\"project=<your-gcp-project-id>\"\n```\n\n```shell\n# Delete infra after your work, to avoid costs on any running services\nterraform destroy\n```",
    "filename": "01-docker-terraform/1_terraform_gcp/terraform/README.md"
  },
  {
    "content": "## GCP and Terraform on Windows\n\nYou don't need these instructions if you use WSL. It's only for \"plain Windows\" \n\n### Google Cloud SDK\n\n* For this tutorial, you'll need a Linux-like environment, e.g. [GitBash](https://gitforwindows.org/), [MinGW](https://www.mingw-w64.org/) or [cygwin](https://www.cygwin.com/)\n  * Power Shell should also work, but will require adjustments \n* Download SDK in zip: https://dl.google.com/dl/cloudsdk/channels/rapid/google-cloud-sdk.zip\n  * source: https://cloud.google.com/sdk/docs/downloads-interactive\n* Unzip it and run the `install.sh` script\n\nWhen installing it, you might see something like that:\n\n```\nThe installer is unable to automatically update your system PATH. Please add\n  C:\\tools\\google-cloud-sdk\\bin\n```\n\n* To fix that, adjust your `.bashrc` to include this in `PATH` ([instructions](https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path))\n* You can also do it system-wide ([instructions](https://gist.github.com/nex3/c395b2f8fd4b02068be37c961301caa7))\n\nNow we need to point it to correct Python installation. Assuming you use [Anaconda](https://www.anaconda.com/products/individual):\n\n```bash\nexport CLOUDSDK_PYTHON=~/Anaconda3/python\n```\n\nNow let's check that it works:\n\n```bash\n$ gcloud version\nGoogle Cloud SDK 367.0.0\nbq 2.0.72\ncore 2021.12.10\ngsutil 5.5\n```\n\n### Google Cloud SDK Authentication \n\n* Now create a service account and generate keys like shown in the videos\n* Download the key and put it to some location, e.g. `.gc/ny-rides.json`\n* Set `GOOGLE_APPLICATION_CREDENTIALS` to point to the file\n\n```bash\nexport GOOGLE_APPLICATION_CREDENTIALS=~/.gc/ny-rides.json\n```\n\nNow authenticate: \n\n```bash\ngcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS\n```\n\nAlternatively, you can authenticate using OAuth like shown in the video\n\n```bash\ngcloud auth application-default login\n```\n\nIf you get a message like `quota exceeded`\n\n> WARNING:\n> Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. \n> Run `$ gcloud auth application-default set-quota-project` to add a quota project.\n\nThen run this:\n\n```bash\nPROJECT_NAME=\"ny-rides-alexey\"\ngcloud auth application-default set-quota-project ${PROJECT_NAME}\n```\n\n\n### Terraform \n\n* [Download Terraform](https://www.terraform.io/downloads)\n* Put it to a folder in [PATH](https://gist.github.com/nex3/c395b2f8fd4b02068be37c961301caa7)\n* Go to the location with Terraform files and initialize it\n\n```bash\nterraform init\n```\n\nOptionally you can configure your terraform files (`variables.tf`) to include your project id:\n\n```bash\nvariable \"project\" {\n  description = \"Your GCP Project ID\"\n  default = \"ny-rides-alexey\"\n  type = string\n}\n```\n\n* Now [follow the instructions](1_terraform_overview.md#execution-steps)\n  * Run `terraform plan`\n  * Next, run `terraform apply`\n\nIf you get an error like that:\n\n> Error: googleapi: Error 403: terraform@ny-rides-alexey.iam.gserviceaccount.com does not have\n> storage.buckets.create access to the Google Cloud project., forbidden\n\n\nThen you need to give your service account all the permissions. Make sure you follow the instructions in the videos \n\n* You can also use [this file](https://docs.google.com/document/d/e/2PACX-1vSZapy7gIj0TP-EFzub2OpAlAkuifGEVJ4XpkA1RvxZ45NjiQi29b6OhLuetdXXHWAn2lbbKxnbzMdd/pub), but it doesn't list all the required permissions",
    "filename": "01-docker-terraform/1_terraform_gcp/windows.md"
  },
  {
    "content": "## Docker and SQL\n\nNotes I used for preparing the videos: [link](https://docs.google.com/document/d/e/2PACX-1vRJUuGfzgIdbkalPgg2nQ884CnZkCg314T_OBq-_hfcowPxNIA0-z5OtMTDzuzute9VBHMjNYZFTCc1/pub)\n\n\n## Commands \n\nAll the commands from the video\n\nDownloading the data\n\n```bash\nwget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz \n```\n\n> Note: now the CSV data is stored in the `csv_backup` folder, not `trip+date` like previously\n\n### Running Postgres with Docker\n\n#### Windows\n\nRunning Postgres on Windows (note the full path)\n\n```bash\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v c:/Users/alexe/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  postgres:13\n```\n\nIf you have the following error:\n\n```\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v e:/zoomcamp/data_engineer/week_1_fundamentals/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data  \\\n  -p 5432:5432 \\\n  postgres:13\n\ndocker: Error response from daemon: invalid mode: \\Program Files\\Git\\var\\lib\\postgresql\\data.\nSee 'docker run --help'.\n```\n\nChange the mounting path. Replace it with the following:\n\n```\n-v /e/zoomcamp/...:/var/lib/postgresql/data\n```\n\n#### Linux and MacOS\n\n\n```bash\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  postgres:13\n```\n\nIf you see that `ny_taxi_postgres_data` is empty after running\nthe container, try these:\n\n* Deleting the folder and running Docker again (Docker will re-create the folder)\n* Adjust the permissions of the folder by running `sudo chmod a+rwx ny_taxi_postgres_data`\n\n\n### CLI for Postgres\n\nInstalling `pgcli`\n\n```bash\npip install pgcli\n```\n\nIf you have problems installing `pgcli` with the command above, try this:\n\n```bash\nconda install -c conda-forge pgcli\npip install -U mycli\n```\n\nUsing `pgcli` to connect to Postgres\n\n```bash\npgcli -h localhost -p 5432 -u root -d ny_taxi\n```\n\n\n### NY Trips Dataset\n\nDataset:\n\n* https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n* https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\n\n> According to the [TLC data website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page),\n> from 05/13/2022, the data will be in ```.parquet``` format instead of ```.csv```\n> The website has provided a useful [link](https://www1.nyc.gov/assets/tlc/downloads/pdf/working_parquet_format.pdf) with sample steps to read ```.parquet``` file and convert it to Pandas data frame.\n>\n> You can use the csv backup located here, https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz, to follow along with the video.\n```\n$ aws s3 ls s3://nyc-tlc\n                           PRE csv_backup/\n                           PRE misc/\n                           PRE trip data/\n```\n\nYou can refer the `data-loading-parquet.ipynb` and `data-loading-parquet.py` for code to handle both csv and parquet files. (The lookup zones table which is needed later in this course is a csv file)\n> Note: You will need to install the `pyarrow` library. (add it to your Dockerfile)\n\n\n### pgAdmin\n\nRunning pgAdmin\n\n```bash\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\n```\n\n### Running Postgres and pgAdmin together\n\nCreate a network\n\n```bash\ndocker network create pg-network\n```\n\nRun Postgres (change the path)\n\n```bash\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v c:/Users/alexe/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  --network=pg-network \\\n  --name pg-database \\\n  postgres:13\n```\n\nRun pgAdmin\n\n```bash\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  --network=pg-network \\\n  --name pgadmin-2 \\\n  dpage/pgadmin4\n```\n\n\n### Data ingestion\n\nRunning locally\n\n```bash\nURL=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n\npython ingest_data.py \\\n  --user=root \\\n  --password=root \\\n  --host=localhost \\\n  --port=5432 \\\n  --db=ny_taxi \\\n  --table_name=yellow_taxi_trips \\\n  --url=${URL}\n```\n\nBuild the image\n\n```bash\ndocker build -t taxi_ingest:v001 .\n```\n\nOn Linux you may have a problem building it:\n\n```\nerror checking context: 'can't stat '/home/name/data_engineering/ny_taxi_postgres_data''.\n```\n\nYou can solve it with `.dockerignore`:\n\n* Create a folder `data`\n* Move `ny_taxi_postgres_data` to `data` (you might need to use `sudo` for that)\n* Map `-v $(pwd)/data/ny_taxi_postgres_data:/var/lib/postgresql/data`\n* Create a file `.dockerignore` and add `data` there\n* Check [this video](https://www.youtube.com/watch?v=tOr4hTsHOzU&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb) (the middle) for more details \n\n\n\nRun the script with Docker\n\n```bash\nURL=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n\ndocker run -it \\\n  --network=pg-network \\\n  taxi_ingest:v001 \\\n    --user=root \\\n    --password=root \\\n    --host=pg-database \\\n    --port=5432 \\\n    --db=ny_taxi \\\n    --table_name=yellow_taxi_trips \\\n    --url=${URL}\n```\n\n### Docker-Compose \n\nRun it:\n\n```bash\ndocker-compose up\n```\n\nRun in detached mode:\n\n```bash\ndocker-compose up -d\n```\n\nShutting it down:\n\n```bash\ndocker-compose down\n```\n\nNote: to make pgAdmin configuration persistent, create a folder `data_pgadmin`. Change its permission via\n\n```bash\nsudo chown 5050:5050 data_pgadmin\n```\n\nand mount it to the `/var/lib/pgadmin` folder:\n\n```yaml\nservices:\n  pgadmin:\n    image: dpage/pgadmin4\n    volumes:\n      - ./data_pgadmin:/var/lib/pgadmin\n    ...\n```\n\n\n### SQL Refresher\n\nPre-Requisites: If you followed the course in the given order,\nDocker Compose should already be running with pgdatabase and pgAdmin.\n\nYou can run the following code using Jupyter Notebook to ingest the data for Taxi Zones:\n\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nengine.connect()\n\n!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n\ndf_zones = pd.read_csv(\"taxi_zone_lookup.csv\")\ndf_zones.to_sql(name='zones', con=engine, if_exists='replace')\n```\n\nOnce done, you can go to http://localhost:8080/browser/ to access pgAdmin.\nDon't forget to Right Click on the server or database to refresh it in case you don't see the new table.\n\nNow start querying!\n\nJoining Yellow Taxi table with Zones Lookup table (implicit INNER JOIN)\n \n```sql\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    CONCAT(zpu.\"Borough\", ' | ', zpu.\"Zone\") AS \"pickup_loc\",\n    CONCAT(zdo.\"Borough\", ' | ', zdo.\"Zone\") AS \"dropoff_loc\"\nFROM \n    yellow_taxi_trips t,\n    zones zpu,\n    zones zdo\nWHERE\n    t.\"PULocationID\" = zpu.\"LocationID\"\n    AND t.\"DOLocationID\" = zdo.\"LocationID\"\nLIMIT 100;\n```\n\nJoining Yellow Taxi table with Zones Lookup table (Explicit INNER JOIN)\n\n```sql\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    CONCAT(zpu.\"Borough\", ' | ', zpu.\"Zone\") AS \"pickup_loc\",\n    CONCAT(zdo.\"Borough\", ' | ', zdo.\"Zone\") AS \"dropoff_loc\"\nFROM \n    yellow_taxi_trips t\nJOIN \n-- or INNER JOIN but it's less used, when writing JOIN, postgreSQL understands implicitly that we want to use an INNER JOIN\n    zones zpu ON t.\"PULocationID\" = zpu.\"LocationID\"\nJOIN\n    zones zdo ON t.\"DOLocationID\" = zdo.\"LocationID\"\nLIMIT 100;\n```\n\nChecking for records with NULL Location IDs in the Yellow Taxi table\n\n```sql\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    \"PULocationID\",\n    \"DOLocationID\"\nFROM \n    yellow_taxi_trips\nWHERE\n    \"PULocationID\" IS NULL\n    OR \"DOLocationID\" IS NULL\nLIMIT 100;\n```\n\nChecking for Location IDs in the Zones table NOT IN the Yellow Taxi table\n\n```sql\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    \"PULocationID\",\n    \"DOLocationID\"\nFROM \n    yellow_taxi_trips\nWHERE\n    \"DOLocationID\" NOT IN (SELECT \"LocationID\" from zones)\n    OR \"PULocationID\" NOT IN (SELECT \"LocationID\" from zones)\nLIMIT 100;\n```\n\nUsing LEFT, RIGHT, and OUTER JOINS when some Location IDs are not in either Tables\n\n```sql\nDELETE FROM zones WHERE \"LocationID\" = 142;\n\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    CONCAT(zpu.\"Borough\", ' | ', zpu.\"Zone\") AS \"pickup_loc\",\n    CONCAT(zdo.\"Borough\", ' | ', zdo.\"Zone\") AS \"dropoff_loc\"\nFROM \n    yellow_taxi_trips t\nLEFT JOIN \n    zones zpu ON t.\"PULocationID\" = zpu.\"LocationID\"\nJOIN\n    zones zdo ON t.\"DOLocationID\" = zdo.\"LocationID\"\nLIMIT 100;\n```\n\n```sql\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    CONCAT(zpu.\"Borough\", ' | ', zpu.\"Zone\") AS \"pickup_loc\",\n    CONCAT(zdo.\"Borough\", ' | ', zdo.\"Zone\") AS \"dropoff_loc\"\nFROM \n    yellow_taxi_trips t\nRIGHT JOIN \n    zones zpu ON t.\"PULocationID\" = zpu.\"LocationID\"\nJOIN\n    zones zdo ON t.\"DOLocationID\" = zdo.\"LocationID\"\nLIMIT 100;\n```\n\n```sql\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    CONCAT(zpu.\"Borough\", ' | ', zpu.\"Zone\") AS \"pickup_loc\",\n    CONCAT(zdo.\"Borough\", ' | ', zdo.\"Zone\") AS \"dropoff_loc\"\nFROM \n    yellow_taxi_trips t\nOUTER JOIN \n    zones zpu ON t.\"PULocationID\" = zpu.\"LocationID\"\nJOIN\n    zones zdo ON t.\"DOLocationID\" = zdo.\"LocationID\"\nLIMIT 100;\n```\n\nUsing GROUP BY to calculate number of trips per day\n\n```sql\nSELECT\n    CAST(tpep_dropoff_datetime AS DATE) AS \"day\",\n    COUNT(1)\nFROM \n    yellow_taxi_trips\nGROUP BY\n    CAST(tpep_dropoff_datetime AS DATE)\nLIMIT 100;\n```\n\nUsing ORDER BY to order the results of your query\n\n```sql\n-- Ordering by day\n\nSELECT\n    CAST(tpep_dropoff_datetime AS DATE) AS \"day\",\n    COUNT(1)\nFROM \n    yellow_taxi_trips\nGROUP BY\n    CAST(tpep_dropoff_datetime AS DATE)\nORDER BY\n    \"day\" ASC\nLIMIT 100;\n\n-- Ordering by count\n\nSELECT\n    CAST(tpep_dropoff_datetime AS DATE) AS \"day\",\n    COUNT(1) AS \"count\"\nFROM \n    yellow_taxi_trips\nGROUP BY\n    CAST(tpep_dropoff_datetime AS DATE)\nORDER BY\n    \"count\" DESC\nLIMIT 100;\n```\n\nOther kinds of aggregations\n\n```sql\nSELECT\n    CAST(tpep_dropoff_datetime AS DATE) AS \"day\",\n    COUNT(1) AS \"count\",\n    MAX(total_amount) AS \"total_amount\",\n    MAX(passenger_count) AS \"passenger_count\"\nFROM \n    yellow_taxi_trips\nGROUP BY\n    CAST(tpep_dropoff_datetime AS DATE)\nORDER BY\n    \"count\" DESC\nLIMIT 100;\n```\n\nGrouping by multiple fields\n\n\n```sql\nSELECT\n    CAST(tpep_dropoff_datetime AS DATE) AS \"day\",\n    \"DOLocationID\",\n    COUNT(1) AS \"count\",\n    MAX(total_amount) AS \"total_amount\",\n    MAX(passenger_count) AS \"passenger_count\"\nFROM \n    yellow_taxi_trips\nGROUP BY\n    1, 2\nORDER BY\n    \"day\" ASC, \n    \"DOLocationID\" ASC\nLIMIT 100;\n```",
    "filename": "01-docker-terraform/2_docker_sql/README.md"
  },
  {
    "code": false,
    "content": "# Data Loading\n\nIn this document, we will outline the process of loading a Parquet file into a PostgreSQL database. This includes checking metadata, converting the file to a Pandas DataFrame, generating SQL statements, establishing a database connection, and finally inserting the data into the database. We'll also cover handling both CSV and Parquet files for flexibility.\n\n## Initial Setup \n\nTo begin, we first import the necessary libraries: `pandas`, `pyarrow`, and `SQLAlchemy`. These libraries will facilitate reading the Parquet file, manipulating data, and connecting to the SQL database.\n\n```python\nimport pandas as pd \nimport pyarrow.parquet as pq\nfrom time import time\n```\n\n## Reading Metadata from the Parquet File\n\nNext, we will read the metadata from the specified Parquet file. This step helps us understand the structure and schema of the data without loading it entirely into memory.\n\n```python\n# Read metadata \npq.read_metadata('yellow_tripdata_2023-09.parquet')\n```\n\n## Reading Data and Schema Check\n\nWe will now read the full Parquet file and check its schema. Understanding the schema ensures that we know the data types and structure before converting the data to a Pandas DataFrame.\n\n```python\n# Read file, read the table from file and check schema\nfile = pq.ParquetFile('yellow_tripdata_2023-09.parquet')\ntable = file.read()\ntable.schema\n```\n\n## Converting to Pandas DataFrame\n\nAfter reading the data, we will convert the table into a Pandas DataFrame. This allows us to utilize Pandas' powerful data manipulation capabilities, including checking the DataFrame's information and data types.\n\n```python\n# Convert to pandas and check data \ndf = table.to_pandas()\ndf.info()\n```\n\n## Establishing Database Connection\n\nBefore proceeding with inserting data, we need to establish a connection to our PostgreSQL database. This connection will allow us to execute SQL commands directly from Python.\n\n```python\n# Create an open SQL database connection object or a SQLAlchemy connectable\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nengine.connect()\n```\n\n## Generating SQL CREATE Statement\n\nTo ensure that the Pandas DataFrame translates correctly to our database table, we generate a SQL CREATE statement. This allows for validation of the schema before actually inserting the data.\n\n```python\n# Generate CREATE SQL statement from schema for validation\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\n```\n\n## Inserting Data into Database\n\nWith the connection established and schema validated, we can now focus on inserting the data into the PostgreSQL database. The dataset contains 2,846,722 rows, so we'll process the file in batches of 100,000 rows using the `iter_batches` function. This method is efficient and prevents memory overload.\n\n```python\n# Creating batches of 100,000 for the parquet file\nbatches_iter = file.iter_batches(batch_size=100000)\nbatches_iter\n\n# Take the first batch for testing\ndf = next(batches_iter).to_pandas()\ndf\n\n# Creating just the table in postgres\n#df.head(0).to_sql(name='ny_taxi_data',con=engine, if_exists='replace')\n```\n\n### Batch Insertion Process\n\nIn this section, we will iterate through each batch and insert it into the database. This process includes timing each insertion for performance evaluation.\n\n```python\n# Insert values into the table \nt_start = time()\ncount = 0\nfor batch in file.iter_batches(batch_size=100000):\n    count += 1\n    batch_df = batch.to_pandas()\n    print(f'inserting batch {count}...')\n    b_start = time()\n    \n    batch_df.to_sql(name='ny_taxi_data', con=engine, if_exists='append')\n    b_end = time()\n    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n    \nt_end = time()\nprint(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    \n```\n\n## Handling Multiple File Formats\n\nFor future flexibility, we may also need to handle both CSV and Parquet file formats. Here, we check the file extension and process it accordingly, allowing for versatile data loading from different sources.\n\n```python\nfrom time import time\nimport pandas as pd \nimport pyarrow.parquet as pq\nfrom sqlalchemy import create_engine\n\nurl = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv'\nurl = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet'\n\nfile_name = url.rsplit('/', 1)[-1].strip()\nfile_name\n```\n\n### Conditional Loading Based on File Type\n\nHere we implement the logic to load CSV and Parquet files. This ensures that we can adapt to different data sources while maintaining consistency in our data processing pipeline.\n\n```python\nif '.csv' in file_name:\n    print('yay') \n    df = pd.read_csv(file_name, nrows=10)\n    df_iter = pd.read_csv(file_name, iterator=True, chunksize=100000)\nelif '.parquet' in file_name:\n    print('oh yea')\n    file = pq.ParquetFile(file_name)\n    df = next(file.iter_batches(batch_size=10)).to_pandas()\n    df_iter = file.iter_batches(batch_size=100000)\nelse: \n    print('Error. Only .csv or .parquet files allowed.')\n    sys.exit() \n```\n\nThis concludes the documentation for loading data from Parquet files into a PostgreSQL database. The established workflows can be further refined and customized as per project needs.",
    "filename": "01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb"
  },
  {
    "code": false,
    "content": "# Data Loading Script Documentation\n\nThis script is designed to facilitate the downloading of data files, specifically in CSV or Parquet formats, and subsequently load these files into a PostgreSQL database. It employs command-line arguments for user configuration and utilizes libraries like Pandas and SQLAlchemy to manage data operations.\n\n## Overview\n\nThe script primarily performs the following steps:\n\n1. **Argument Parsing**: It captures connection details to a PostgreSQL database and the URL of the data file through command-line arguments.\n2. **File Downloading**: It downloads a specified data file from a provided URL using `curl`.\n3. **Data Frame Creation**: It reads the downloaded file into a Pandas DataFrame, supporting both CSV and Parquet formats.\n4. **Database Connection Setup**: It establishes a connection to a PostgreSQL database using SQLAlchemy.\n5. **Table Creation and Data Insertion**: It creates a new table in the database and inserts the data in chunks, while logging the duration of each insertion batch.\n\n## Argument Parsing\n\n```python\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Loading data from .parquet file link to a Postgres database.')\n\n    parser.add_argument('--user', help='Username for Postgres.')\n    parser.add_argument('--password', help='Password to the username for Postgres.')\n    parser.add_argument('--host', help='Hostname for Postgres.')\n    parser.add_argument('--port', help='Port for Postgres connection.')\n    parser.add_argument('--db', help='Database name for Postgres')\n    parser.add_argument('--tb', help='Destination table name for Postgres.')\n    parser.add_argument('--url', help='URL for .parquet file.')\n\n    args = parser.parse_args()\n    main(args)\n```\n\nThe script begins by defining a main guard to ensure it runs as a standalone program. It uses the `argparse` module to capture various command-line arguments. These include:\n\n- `user`: Username for authenticating with the PostgreSQL database.\n- `password`: Password associated with the user.\n- `host`: The database server's hostname.\n- `port`: The port through which to connect to the database.\n- `db`: The name of the database in which to store the data.\n- `tb`: The target table name for the dataset.\n- `url`: The URL from which to download the data file.\n\n## File Downloading\n\n```python\nfile_name = url.rsplit('/', 1)[-1].strip()\nprint(f'Downloading {file_name} ...')\nos.system(f'curl {url.strip()} -o {file_name}')\nprint('\\n')\n```\n\nOnce the arguments have been parsed, the script extracts the filename from the provided URL. It then uses `curl` to download the file, printing informative messages to the console about the download process. If the URL is malformed or the server is unreachable, the script would terminate after an unsuccessful download.\n\n## Connecting to the Database\n\n```python\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n```\n\nAfter downloading the file, the script creates a SQLAlchemy engine that establishes a connection to the specified PostgreSQL database. This engine will be used for all subsequent database operations, allowing data manipulation within the defined constraints of the database.\n\n## Reading the Data File\n\n```python\nif '.csv' in file_name:\n    df = pd.read_csv(file_name, nrows=10)\n    df_iter = pd.read_csv(file_name, iterator=True, chunksize=100000)\nelif '.parquet' in file_name:\n    file = pq.ParquetFile(file_name)\n    df = next(file.iter_batches(batch_size=10)).to_pandas()\n    df_iter = file.iter_batches(batch_size=100000)\nelse: \n    print('Error. Only .csv or .parquet files allowed.')\n    sys.exit()\n```\n\nThe script checks the file extension to determine the format of the downloaded file. If the file is a `.csv`, it reads the first 10 rows to infer the structure and creates an iterator to read the remainder in chunks of 100,000 rows. For a `.parquet` file, it initializes a `ParquetFile` object and performs the same operations. If the file is neither format, it prints an error message and exits.\n\n## Creating the Database Table\n\n```python\ndf.head(0).to_sql(name=tb, con=engine, if_exists='replace')\n```\n\nBefore inserting data, the script creates a new table in the PostgreSQL database using the inferred schema from the DataFrame (with no actual data). The table is created with the name specified by the `tb` parameter, and if it already exists, it will be replaced.\n\n## Data Insertion\n\n```python\nt_start = time()\ncount = 0\nfor batch in df_iter:\n    count+=1\n\n    if '.parquet' in file_name:\n        batch_df = batch.to_pandas()\n    else:\n        batch_df = batch\n\n    print(f'inserting batch {count}...')\n\n    b_start = time()\n    batch_df.to_sql(name=tb, con=engine, if_exists='append')\n    b_end = time()\n\n    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\nt_end = time()   \nprint(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')\n```\n\nIn the data insertion phase, the script measures the total time required and tracks the number of batches inserted. For each batch, the DataFrame is converted to a pandas DataFrame (if necessary) and then appended to the PostgreSQL table using the `.to_sql()` function. During each insertion, the script logs the time taken to complete that specific batch, providing a clear picture of performance.\n\n## Final Output\n\nUpon completion, the script prints a summary of the total time taken for all batches to be inserted, giving the user feedback on the overall efficiency of the data-loading process.\n\nIn summary, this script automates the entire workflow of downloading data from web sources and loading it into a PostgreSQL database, making it a valuable tool for data engineers and analysts.",
    "filename": "01-docker-terraform/2_docker_sql/data-loading-parquet.py"
  },
  {
    "code": false,
    "content": "# CSV to PostgreSQL Ingestion Script Documentation\n\nThis Python script is designed to download CSV data from a specified URL, process the data, and ingest it into a PostgreSQL database. It handles large datasets by reading them in chunks, enabling efficient memory usage and minimizing the risk of overwhelming the system.\n\n## Overview\n\nThe script uses several libraries including `os` for operating system interface, `argparse` for command-line argument parsing, `pandas` for data manipulation, and `sqlalchemy` for database interaction. The overall workflow includes downloading the CSV file, connecting to the PostgreSQL database, processing the data, and performing batch inserts into a specified table.\n\n### Key Libraries\n\n- **os**: Used for system-level operations such as downloading files.\n- **argparse**: Facilitates the handling of command-line arguments for user configuration.\n- **pandas**: Provides powerful data manipulation capabilities, especially for working with CSV files.\n- **sqlalchemy**: Offers a high-level interface to manage database connections and operations.\n\n## Command-Line Arguments\n\nThe script accepts the following command-line arguments, which are used to establish the database connection and specify the CSV file:\n\n- `--user`: PostgreSQL username (required).\n- `--password`: PostgreSQL password (required).\n- `--host`: Database host (required).\n- `--port`: Port number for database connection (required).\n- `--db`: Database name (required).\n- `--table_name`: Name of the table where data will be ingested (required).\n- `--url`: URL from which to download the CSV file (required).\n\n### Main Function\n\nThe core functionality is encapsulated in the `main` function, which performs several tasks:\n\n1. **File Download**:\n   - The script checks the file extension of the URL to determine if it should download a gzipped (`.csv.gz`) file; if so, it saves the file as `output.csv.gz`, otherwise it saves it as `output.csv`. \n   - It uses the `wget` command to download the file.\n\n2. **Database Connection**:\n   - The script creates a connection to the PostgreSQL database using SQLAlchemy's `create_engine`, combining the provided credentials into a connection string.\n\n3. **CSV Chunk Processing**:\n   - It reads the CSV file in chunks of 100,000 rows using `pandas` `read_csv` iterator. This prevents loading the entire dataset into memory at once.\n   - The pickup and dropoff datetime columns are converted to pandas datetime objects to ensure proper handling of date-time data.\n\n4. **Table Preparation**:\n   - The first chunk of data is used to create the table in the PostgreSQL database. If the table already exists, it is replaced.\n   - The chunk is then appended to the database.\n\n## Chunk Ingestion Loop\n\nThe script enters a loop to continue processing the remaining data chunks:\n\n1. **While Loop**:\n   - It attempts to read the next chunk of data from the iterator. The timing of each chunk insertion is recorded to provide performance feedback.\n\n2. **Error Handling**:\n   - Upon reaching the end of the file, a `StopIteration` exception will be raised, which is caught to break the loop and print a completion message.\n\n3. **Data Insertion**:\n   - Each chunk is processed in the same manner as the first, with datetime conversion followed by appending the chunk to the specified table in the database.\n\n## Execution\n\nThe script is designed to be run as a standalone application. To execute it, the user must provide the required parameters through command-line arguments. An example usage would look like:\n\n```bash\npython script.py --user username --password secret --host localhost --port 5432 --db mydatabase --table_name mytable --url http://example.com/data.csv.gz\n```\n\nThis invocation will start the entire data ingestion process described above. \n\n## Performance Considerations\n\nThe script provides timing feedback for each chunk processed, which can help users understand the performance characteristics of their data ingestion. Given that large datasets can take considerable time to process, this feedback allows for monitoring and potential optimization of the process.\n\n### Final Notes\n\nThis tool is designed specifically for dealing with CSV files intended for PostgreSQL databases. Users should ensure that the CSV data is formatted correctly and that the PostgreSQL table schema is compatible with the incoming data. Modifications to the script may be required if the structure of the CSV changes or if additional data processing is necessary before ingestion.",
    "filename": "01-docker-terraform/2_docker_sql/ingest_data.py"
  },
  {
    "code": false,
    "content": "# PostgreSQL Database Interaction and Data Loading\n\nThis document provides a structured overview of how to interact with a PostgreSQL database using Python and the Pandas library. It includes examples of how to create a database connection, execute SQL queries, and load CSV data into the database.\n\n## Setting Up Your Environment\n\nTo start, you'll need to install some essential packages. The following command installs SQLAlchemy and psycopg2-binary, which are required for database connectivity with PostgreSQL.\n\n```bash\npip install sqlalchemy psycopg2-binary \n```\n\n### Importing Necessary Libraries\n\nAfter installing the required packages, you should import the necessary libraries into your Python script. In this case, we are using the Pandas library for data manipulation and SQLAlchemy to manage the database connections.\n\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine\n```\n\n## Creating a Database Connection\n\nTo connect to a PostgreSQL database, you can use SQLAlchemy's `create_engine` function. This function constructs a connection string that specifies the database type, user credentials, host, and database name.\n\n```python\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n```\n\n### Verifying the Connection\n\nOnce you have created the engine, it's good practice to verify that the connection is successful. This can be done by calling the `connect()` method on the engine object.\n\n```python\nengine.connect()\n```\n\n## Executing Simple SQL Queries\n\nWith an established connection, you can execute SQL queries. \n\n### Example Query: Simple SELECT\n\nThe following SQL query selects a constant value and returns it. It serves as a simple test to check if the database is operational.\n\n```python\nquery = \"\"\"\nSELECT 1 as number;\n\"\"\"\n\npd.read_sql(query, con=engine)\n```\n\n### Querying Database Tables\n\nYou can query the existing tables in your database. The next block retrieves all user-defined tables, excluding system tables.\n\n```python\nquery = \"\"\"\nSELECT *\nFROM pg_catalog.pg_tables\nWHERE schemaname != 'pg_catalog' AND \n    schemaname != 'information_schema';\n\"\"\"\n\npd.read_sql(query, con=engine)\n```\n\n## Loading Data from CSV\n\nAfter validating the database queries, you may want to load external data into your database. This example shows how to read a CSV file containing taxi trip data.\n\n```python\ndf = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=100)\n```\n\n### Preprocessing Date Columns\n\nFor proper date-time representation, you need to convert string dates in the dataset to datetime objects. This step ensures that date-related functions operate correctly later on.\n\n```python\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n```\n\n## Saving Data to the Database\n\nAfter loading and preprocessing the data, you can save it to your PostgreSQL database. The following command writes the DataFrame to a specified table without including the DataFrame index.\n\n```python\ndf.to_sql(name='yellow_tripdata_trip', con=engine, index=False)\n```\n\n## Querying the Newly Created Table\n\nFinally, you can verify whether the data has been successfully loaded into your new table by querying it and retrieving a limited number of rows.\n\n```python\nquery = \"\"\"\nSELECT * FROM yellow_tripdata_trip LIMIT 10\n\"\"\"\n\npd.read_sql(query, con=engine)\n```\n\n## Conclusion\n\nThis document outlines the essential steps for connecting to a PostgreSQL database, executing queries, loading CSV data, and saving it back to the database. Following these examples allows for seamless integration of local datasets with a relational database, making data analysis and manipulation straightforward.\n\nFor more advanced operations, consider exploring further SQL functionalities and optimizations available in PostgreSQL.",
    "filename": "01-docker-terraform/2_docker_sql/pg-test-connection.ipynb"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis script is a simple command-line Python application that utilizes pandas, a popular data analysis library, to perform operations based on input received from the command line. The main function of the script is to process data for a specific day, which is indicated by a command-line argument. \n\n## Importing Required Libraries\n\n```python\nimport sys\nimport pandas as pd\n```\n\nThe script begins by importing two essential libraries:\n- **sys**: This library is used to interact with the Python interpreter. It allows access to command-line arguments and system-specific parameters.\n- **pandas**: A powerful data manipulation and analysis library. While the script notes \"some fancy stuff with pandas,\" the specific operations are not detailed in the provided code.\n\n## Command-Line Arguments\n\n```python\nprint(sys.argv)\n```\n\nThe script prints the list of command-line arguments passed to it when executed. This list includes the name of the script itself as the first element, followed by any arguments provided by the user.\n\n## Day Variable Extraction\n\n```python\nday = sys.argv[1]\n```\n\nHere, the script assigns the first command-line argument (after the script name) to the variable `day`. This variable is expected to represent the specific day's data that the script is intended to process. The script currently lacks error handling, so if the user does not supply an argument, this line will raise an `IndexError`.\n\n## Suggestive Placeholder for Data Processing\n\n```python\n# some fancy stuff with pandas\n```\n\nThis line serves as a placeholder where the script is presumably intended to perform data manipulation or analysis using pandas. The specifics of this operation are not provided in the code snippet. The comment suggests room for adding various data processing tasks, such as reading from a CSV file, filtering data, performing calculations, or generating reports based on the `day` variable.\n\n## Completion Message\n\n```python\nprint(f'job finished successfully for day = {day}')\n```\n\nThe script concludes by printing a success message that includes the value of the `day` variable. This output informs the user that the processing has finished successfully, assuming no errors occurred prior.\n\n## Conclusion\n\nIn conclusion, this script is structured primarily to take user input for a specific day via command-line arguments, with an integration point for pandas for data handling. The absence of detailed data manipulation in the provided code suggests it might be a work in progress or serve as a template for further extension. The overall flow is straightforward, allowing for easy modification to incorporate specific data analysis tasks relevant to the provided `day` input.",
    "filename": "01-docker-terraform/2_docker_sql/pipeline.py"
  },
  {
    "code": false,
    "content": "# NYC Taxi Data Processing\n\nThis documentation provides an overview of the process for loading, transforming, and storing NYC taxi trip data using Python, Pandas, and SQLAlchemy. Each section outlines specific steps, along with code examples and explanations.\n\n## Importing Required Libraries\n\nBefore processing the taxi data, it is essential to import the necessary libraries, particularly Pandas for data manipulation. \n\n```python\nimport pandas as pd\n```\n\nTo verify that the correct version of Pandas is installed, you can check the version as follows:\n\n```python\npd.__version__\n```\n\n## Loading Initial Data\n\nIn this section, we read a sample of the NYC taxi trip data from a CSV file. By limiting the rows to 100, we can quickly review the structure of the dataset without exhausting resources.\n\n```python\ndf = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=100)\n```\n\nNext, we transform the pickup and dropoff datetime columns into a compatible format using Pandas' `to_datetime` method, which allows for easier date-time manipulation later.\n\n```python\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n```\n\n## Setting Up the Database Connection\n\nTo store the processed data, we need to establish a connection to a PostgreSQL database using SQLAlchemy.\n\n```python\nfrom sqlalchemy import create_engine\n```\n\nHere, we create an engine with the appropriate connection string pointing to our PostgreSQL database.\n\n```python\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n```\n\nWe can then preview the SQL schema that will be generated based on our dataframe's structure.\n\n```python\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\n```\n\n## Reading Data in Chunks\n\nFor large datasets, it's often impractical to load everything into memory at once. Instead, we can read data in chunks. Here, we set up an iterator to read the taxi trip data in chunks of 100,000 rows at a time.\n\n```python\ndf_iter = pd.read_csv('yellow_tripdata_2021-01.csv', iterator=True, chunksize=100000)\n```\n\nThe following line retrieves the first chunk of the data for processing.\n\n```python\ndf = next(df_iter)\n```\n\nTo confirm the size of the loaded data, we can check the length of the dataframe.\n\n```python\nlen(df)\n```\n\nAs before, we must convert the datetime columns to a proper format.\n\n```python\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n```\n\nFinally, we can preview the data structure with:\n\n```python\ndf\n```\n\n## Storing Data in PostgreSQL\n\nOnce the data is prepared, we can store the empty table schema in the PostgreSQL database. This helps define the structure of the table before inserting any data.\n\n```python\ndf.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace')\n```\n\nNext, we can measure the performance of storing the data by timing the insert operation. This shows how long it takes to append the current chunk to the existing dataset.\n\n```python\n%time df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n```\n\n## Processing All Chunks\n\nTo efficiently store the entire dataset, we\u2019ll implement a loop that goes through each chunk of data until all are processed.\n\n```python\nfrom time import time\n```\n\nHere\u2019s how the loop works:\n\n```python\nwhile True: \n    t_start = time()\n\n    df = next(df_iter)\n\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n    \n    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n\n    t_end = time()\n\n    print('inserted another chunk, took %.3f second' % (t_end - t_start))\n```\n\nFor each chunk, the pickup and dropoff times are converted to datetime format, and then the data is appended to the database. The time taken for each insertion is printed, providing insight into performance.\n\n## Loading Additional Data\n\nTo enrich our dataset, we will download additional information, specifically the taxi zone lookup data. \n\n```python\n!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n```\n\nOnce downloaded, we can load this new data into a Pandas dataframe.\n\n```python\ndf_zones = pd.read_csv('taxi+_zone_lookup.csv')\n```\n\nTo inspect the first few rows and confirm the data's structure:\n\n```python\ndf_zones.head()\n```\n\nFinally, we can store the taxi zone data into the PostgreSQL database.\n\n```python\ndf_zones.to_sql(name='zones', con=engine, if_exists='replace')\n```\n\nThis concludes the basic processing and storage of NYC taxi trip data and related lookup tables. Each step is designed to ensure that the data is efficiently managed and stored for analysis.",
    "filename": "01-docker-terraform/2_docker_sql/upload-data.ipynb"
  },
  {
    "content": "# Introduction\n\n* [![](https://markdown-videos-api.jorgenkh.no/youtube/AtRhA-NfS24)](https://www.youtube.com/watch?v=X8cEEwi8DTM)\n* [Slides](https://docs.google.com/presentation/d/1974w3_zdaK7tDQJCcxHginiAuN0hRRYqXiHbDWcuVVg/edit?usp=drivesdk)\n* Overview of [Architecture](https://github.com/DataTalksClub/data-engineering-zoomcamp#overview), [Technologies](https://github.com/DataTalksClub/data-engineering-zoomcamp#technologies) & [Pre-Requisites](https://github.com/DataTalksClub/data-engineering-zoomcamp#prerequisites)\n\n\nWe suggest watching videos in the same order as in this document.\n\nThe last video (setting up the environment) is optional, but you can check it earlier\nif you have troubles setting up the environment and following along with the videos.\n\n\n# Docker + Postgres\n\n[Code](2_docker_sql)\n\n## :movie_camera: Introduction to Docker\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/EYNwNlOrpr0)](https://youtu.be/EYNwNlOrpr0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=4)\n\n* Why do we need Docker\n* Creating a simple \"data pipeline\" in Docker\n\n\n## :movie_camera: Ingesting NY Taxi Data to Postgres\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/2JM-ziJt0WI)](https://youtu.be/2JM-ziJt0WI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=5)\n\n* Running Postgres locally with Docker\n* Using `pgcli` for connecting to the database\n* Exploring the NY Taxi dataset\n* Ingesting the data into the database\n\n> [!TIP]\n>if you have problems with `pgcli`, check this video for an alternative way to connect to your database in jupyter notebook and pandas.\n>\n> [![](https://markdown-videos-api.jorgenkh.no/youtube/3IkfkTwqHx4)](https://youtu.be/3IkfkTwqHx4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=6)\n\n\n## :movie_camera: Connecting pgAdmin and Postgres\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/hCAIVe9N0ow)](https://youtu.be/hCAIVe9N0ow&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=7)\n\n* The pgAdmin tool\n* Docker networks\n\n\n> [!IMPORTANT]\n>The UI for PgAdmin 4 has changed, please follow the below steps for creating a server:\n>\n>* After login to PgAdmin, right click Servers in the left sidebar.\n>* Click on Register.\n>* Click on Server.\n>* The remaining steps to create a server are the same as in the videos.\n\n\n## :movie_camera: Putting the ingestion script into Docker\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/B1WwATwf-vY)](https://youtu.be/B1WwATwf-vY&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=8)\n\n* Converting the Jupyter notebook to a Python script\n* Parameterizing the script with argparse\n* Dockerizing the ingestion script\n\n## :movie_camera: Running Postgres and pgAdmin with Docker-Compose\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/hKI6PkPhpa0)](https://youtu.be/hKI6PkPhpa0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=9)\n\n* Why do we need Docker-compose\n* Docker-compose YAML file\n* Running multiple containers with `docker-compose up`\n\n## :movie_camera: SQL refresher\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/QEcps_iskgg)](https://youtu.be/QEcps_iskgg&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=10)\n\n* Adding the Zones table\n* Inner joins\n* Basic data quality checks\n* Left, Right and Outer joins\n* Group by\n\n## :movie_camera: Optional: Docker Networking and Port Mapping\n\n> [!TIP]\n> Optional: If you have some problems with docker networking, check **Port Mapping and Networks in Docker video**.\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/tOr4hTsHOzU)](https://youtu.be/tOr4hTsHOzU&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=5)\n\n* Docker networks\n* Port forwarding to the host environment\n* Communicating between containers in the network\n* `.dockerignore` file\n\n## :movie_camera: Optional: Walk-Through on WSL\n\n> [!TIP]\n> Optional: If you are willing to do the steps from \"Ingesting NY Taxi Data to Postgres\" till \"Running Postgres and pgAdmin with Docker-Compose\" with Windows Subsystem Linux please check **Docker Module Walk-Through on WSL**.\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/Mv4zFm2AwzQ)](https://youtu.be/Mv4zFm2AwzQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=33)\n\n\n# GCP\n\n## :movie_camera: Introduction to GCP (Google Cloud Platform)\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/18jIzE41fJ4)](https://youtu.be/18jIzE41fJ4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=3)\n\n# Terraform\n\n[Code](1_terraform_gcp)\n\n## :movie_camera: Introduction Terraform: Concepts and Overview, a primer\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/s2bOYDCKl_M)](https://youtu.be/s2bOYDCKl_M&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=11)\n\n* [Companion Notes](1_terraform_gcp)\n\n## :movie_camera: Terraform Basics: Simple one file Terraform Deployment\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/Y2ux7gq3Z0o)](https://youtu.be/Y2ux7gq3Z0o&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=12)\n\n* [Companion Notes](1_terraform_gcp)\n\n## :movie_camera: Deployment with a Variables File\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/PBi0hHjLftk)](https://youtu.be/PBi0hHjLftk&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=13)\n\n* [Companion Notes](1_terraform_gcp)\n\n## Configuring terraform and GCP SDK on Windows\n\n* [Instructions](1_terraform_gcp/windows.md)\n\n\n# Environment setup\n\nFor the course you'll need:\n\n* Python 3 (e.g. installed with Anaconda)\n* Google Cloud SDK\n* Docker with docker-compose\n* Terraform\n* Git account\n\n> [!NOTE]\n>If you have problems setting up the environment, you can check these videos.\n>\n>If you already have a working coding environment on local machine, these are optional. And only need to select one method. But if you have time to learn it now, these would be helpful if the local environment suddenly do not work one day.\n\n## :movie_camera: GCP Cloud VM\n\n### Setting up the environment on cloud VM\n[![](https://markdown-videos-api.jorgenkh.no/youtube/ae-CV2KfoN0)](https://youtu.be/ae-CV2KfoN0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=14)\n\n* Generating SSH keys\n* Creating a virtual machine on GCP\n* Connecting to the VM with SSH\n* Installing Anaconda\n* Installing Docker\n* Creating SSH `config` file\n* Accessing the remote machine with VS Code and SSH remote\n* Installing docker-compose\n* Installing pgcli\n* Port-forwarding with VS code: connecting to pgAdmin and Jupyter from the local computer\n* Installing Terraform\n* Using `sftp` for putting the credentials to the remote machine\n* Shutting down and removing the instance\n\n## :movie_camera: GitHub Codespaces\n\n### Preparing the environment with GitHub Codespaces\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/XOSUt8Ih3zA)](https://youtu.be/XOSUt8Ih3zA&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=15)\n\n# Homework\n\n* [Homework](../cohorts/2025/01-docker-terraform/homework.md)\n\n\n# Community notes\n\nDid you take notes? You can share them here\n\n* [Notes from Alvaro Navas](https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/1_intro.md)\n* [Notes from Abd](https://itnadigital.notion.site/Week-1-Introduction-f18de7e69eb4453594175d0b1334b2f4)\n* [Notes from Aaron](https://github.com/ABZ-Aaron/DataEngineerZoomCamp/blob/master/week_1_basics_n_setup/README.md)\n* [Notes from Faisal](https://github.com/FaisalMohd/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/Notes/DE%20Zoomcamp%20Week-1.pdf)\n* [Michael Harty's Notes](https://github.com/mharty3/data_engineering_zoomcamp_2022/tree/main/week01)\n* [Blog post from Isaac Kargar](https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/18/data-engineering-w1.html)\n* [Handwritten Notes By Mahmoud Zaher](https://github.com/zaherweb/DataEngineering/blob/master/week%201.pdf)\n* [Notes from Candace Williams](https://teacherc.github.io/data-engineering/2023/01/18/zoomcamp1.html)\n* [Notes from Marcos Torregrosa](https://www.n4gash.com/2023/data-engineering-zoomcamp-semana-1/)\n* [Notes from Vincenzo Galante](https://binchentso.notion.site/Data-Talks-Club-Data-Engineering-Zoomcamp-8699af8e7ff94ec49e6f9bdec8eb69fd)\n* [Notes from Victor Padilha](https://github.com/padilha/de-zoomcamp/tree/master/week1)\n* [Notes from froukje](https://github.com/froukje/de-zoomcamp/blob/main/week_1_basics_n_setup/notes/notes_week_01.md)\n* [Notes from adamiaonr](https://github.com/adamiaonr/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/NOTES.md)\n* [Notes from Xia He-Bleinagel](https://xiahe-bleinagel.com/2023/01/week-1-data-engineering-zoomcamp-notes/)\n* [Notes from Balaji](https://github.com/Balajirvp/DE-Zoomcamp/blob/main/Week%201/Detailed%20Week%201%20Notes.ipynb)\n* [Notes from Erik](https://twitter.com/ehub96/status/1621351266281730049)\n* [Notes by Alain Boisvert](https://github.com/boisalai/de-zoomcamp-2023/blob/main/week1.md)\n* Notes on [Docker, Docker Compose, and setting up a proper Python environment](https://medium.com/@verazabeida/zoomcamp-2023-week-1-f4f94cb360ae), by Vera\n* [Setting up the development environment on Google Virtual Machine](https://itsadityagupta.hashnode.dev/setting-up-the-development-environment-on-google-virtual-machine), blog post by Aditya Gupta\n* [Notes from Zharko Cekovski](https://www.zharconsulting.com/contents/data/data-engineering-bootcamp-2024/week-1-postgres-docker-and-ingestion-scripts/)\n* [2024 Module-01 Walkthough video by ellacharmed on youtube](https://youtu.be/VUZshlVAnk4)\n* [2024 Companion Module Walkthough slides by ellacharmed](https://github.com/ellacharmed/data-engineering-zoomcamp/blob/ella2024/cohorts/2024/01-docker-terraform/walkthrough-01.pdf)\n* [2024 Module-01 Environment setup video by ellacharmed on youtube](https://youtu.be/Zce_Hd37NGs)\n* [Docker Notes by Linda](https://github.com/inner-outer-space/de-zoomcamp-2024/blob/main/1a-docker_sql/readme.md) \u2022 [Terraform Notes by Linda](https://github.com/inner-outer-space/de-zoomcamp-2024/blob/main/1b-terraform_gcp/readme.md)\n* [Notes from Hammad Tariq](https://github.com/hamad-tariq/HammadTariq-ZoomCamp2024/blob/9c8b4908416eb8cade3d7ec220e7664c003e9b11/week_1_basics_n_setup/README.md)\n* [Hung's Notes](https://hung.bearblog.dev/docker/) & [Docker Cheatsheet](https://github.com/HangenYuu/docker-cheatsheet)\n* [Kemal's Notes](https://github.com/kemaldahha/data-engineering-course/blob/main/week_1_notes.md)\n* [Notes from Manuel Guerra (Windows+WSL2 Environment)](https://github.com/ManuelGuerra1987/data-engineering-zoomcamp-notes/blob/main/1_Containerization-and-Infrastructure-as-Code/README.md)\n* [Notes from Horeb SEIDOU](https://spotted-hardhat-eea.notion.site/Week-1-Containerization-and-Infrastructure-as-Code-15729780dc4a80a08288e497ba937a37)\n* [2025 Gitbook Notes from Tinker0425](https://data-engineering-zoomcamp-2025-t.gitbook.io/tinker0425/introduction/introduction-and-set-up)\n* [Alex's Docker Notes](https://github.com/alexg9010/2025_data_engineering_zoomcamp/blob/master/01_docker/README.md) | [Alex's Terraform Notes](https://github.com/alexg9010/2025_data_engineering_zoomcamp/blob/master/01_3_terraform/README.md)\n* [2025 SQL Refresher - Notes by Gabi Fonseca](https://github.com/fonsecagabriella/data_engineering/blob/main/01_docker_postgress/0_sql_refresh.ipynb)\n* [2025 Setting up the Environment - Notes by Gabi Fonseca](https://github.com/fonsecagabriella/data_engineering/blob/main/01_docker_postgress/_setting_up.md)\n* [Notes from Mercy Markus: Linux/Fedora Tweaks and Tips](https://mercymarkus.com/posts/2025/series/dtc-dez-jan-2025/dtc-dez-2025-module-1/)\n* Add your notes above this line",
    "filename": "01-docker-terraform/README.md"
  },
  {
    "content": "# Workflow Orchestration\n\nWelcome to Module 2 of the Data Engineering Zoomcamp! This week, we\u2019ll dive into workflow orchestration using [Kestra](https://go.kestra.io/de-zoomcamp/github). \n\nKestra is an open-source, event-driven orchestration platform that simplifies building both scheduled and event-driven workflows. By adopting Infrastructure as Code practices for data and process orchestration, Kestra enables you to build reliable workflows with just a few lines of YAML.\n\n> [!NOTE]  \n>You can find all videos for this week in this [YouTube Playlist](https://go.kestra.io/de-zoomcamp/yt-playlist).\n\n---\n\n# Course Structure\n\n## 1. Conceptual Material: Introduction to Orchestration and Kestra\n\nIn this section, you\u2019ll learn the foundations of workflow orchestration, its importance, and how Kestra fits into the orchestration landscape.\n\n### Videos\n- **2.2.1 - Introduction to Workflow Orchestration**  \n  [![2.2.1 - Workflow Orchestration Introduction](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FNp6QmmcgLCs)](https://youtu.be/Np6QmmcgLCs)\n\n- **2.2.2 - Learn the Concepts of Kestra**  \n  [![Learn Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2Fo79n-EVpics)](https://youtu.be/o79n-EVpics)\n\n### Resources\n- [Quickstart Guide](https://go.kestra.io/de-zoomcamp/quickstart)\n- [Install Kestra with Docker Compose](https://go.kestra.io/de-zoomcamp/docker-compose)\n- [Tutorial](https://go.kestra.io/de-zoomcamp/tutorial)\n- [What is an Orchestrator?](https://go.kestra.io/de-zoomcamp/what-is-an-orchestrator)\n\n---\n\n## 2. Hands-On Coding Project: Build Data Pipelines with Kestra\n\nThis week, we're gonna build ETL pipelines for Yellow and Green Taxi data from NYC\u2019s Taxi and Limousine Commission (TLC). You will:\n1. Extract data from [CSV files](https://github.com/DataTalksClub/nyc-tlc-data/releases).\n2. Load it into Postgres or Google Cloud (GCS + BigQuery).\n3. Explore scheduling and backfilling workflows.\n\n>[!NOTE] \nIf you\u2019re using the PostgreSQL and PgAdmin docker setup from Module 1 for this week\u2019s Kestra Workflow Orchestration exercise, ensure your PostgreSQL image version is 15 or later (preferably the latest). The MERGE statement, introduced in PostgreSQL 15, won\u2019t work on earlier versions and will likely cause syntax errors in your kestra flows.\n\n### File Structure\n\nThe project is organized as follows:\n```\n.\n\u251c\u2500\u2500 flows/\n\u2502   \u251c\u2500\u2500 01_getting_started_data_pipeline.yaml\n\u2502   \u251c\u2500\u2500 02_postgres_taxi.yaml\n\u2502   \u251c\u2500\u2500 02_postgres_taxi_scheduled.yaml\n\u2502   \u251c\u2500\u2500 03_postgres_dbt.yaml\n\u2502   \u251c\u2500\u2500 04_gcp_kv.yaml\n\u2502   \u251c\u2500\u2500 05_gcp_setup.yaml\n\u2502   \u251c\u2500\u2500 06_gcp_taxi.yaml\n\u2502   \u251c\u2500\u2500 06_gcp_taxi_scheduled.yaml\n\u2502   \u2514\u2500\u2500 07_gcp_dbt.yaml\n```\n\n### Setup Kestra\n\nWe'll set up Kestra using Docker Compose containing one container for the Kestra server and another for the Postgres database:\n\n```bash\ncd 02-workflow-orchestration/docker/combined\ndocker compose up -d\n```\n\nOnce the container starts, you can access the Kestra UI at [http://localhost:8080](http://localhost:8080).\n\nIf you prefer to add flows programmatically using Kestra's API, run the following commands:\n\n```bash\ncurl -X POST http://localhost:8080/api/v1/flows/import -F fileUpload=@flows/01_getting_started_data_pipeline.yaml\ncurl -X POST http://localhost:8080/api/v1/flows/import -F fileUpload=@flows/02_postgres_taxi.yaml\ncurl -X POST http://localhost:8080/api/v1/flows/import -F fileUpload=@flows/02_postgres_taxi_scheduled.yaml\ncurl -X POST http://localhost:8080/api/v1/flows/import -F fileUpload=@flows/03_postgres_dbt.yaml\ncurl -X POST http://localhost:8080/api/v1/flows/import -F fileUpload=@flows/04_gcp_kv.yaml\ncurl -X POST http://localhost:8080/api/v1/flows/import -F fileUpload=@flows/05_gcp_setup.yaml\ncurl -X POST http://localhost:8080/api/v1/flows/import -F fileUpload=@flows/06_gcp_taxi.yaml\ncurl -X POST http://localhost:8080/api/v1/flows/import -F fileUpload=@flows/06_gcp_taxi_scheduled.yaml\ncurl -X POST http://localhost:8080/api/v1/flows/import -F fileUpload=@flows/07_gcp_dbt.yaml\n```\n\n---\n\n## 3. ETL Pipelines in Kestra: Detailed Walkthrough\n\n### Getting Started Pipeline\n\nThis introductory flow is added just to demonstrate a simple data pipeline which extracts data via HTTP REST API, transforms that data in Python and then queries it using DuckDB. For this stage, a new separate Postgres database is created for the exercises. \n\n**Note:** Check that `pgAdmin` isn't running on the same ports as Kestra. If so, check out the [FAQ](#troubleshooting-tips) at the bottom of the README.\n\n### Videos\n\n- **2.2.3 - Create an ETL Pipeline with Postgres in Kestra**   \n  [![Create an ETL Pipeline with Postgres in Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FOkfLX28Ecjg%3Fsi%3DvKbIyWo1TtjpNnvt)](https://youtu.be/OkfLX28Ecjg?si=vKbIyWo1TtjpNnvt)\n- **2.2.4 - Manage Scheduling and Backfills using Postgres in Kestra**  \n  [![Manage Scheduling and Backfills using Postgres in Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2F_-li_z97zog%3Fsi%3DG6jZbkfJb3GAyqrd)](https://youtu.be/_-li_z97zog?si=G6jZbkfJb3GAyqrd)\n- **2.2.5 - Transform Data with dbt and Postgres in Kestra**  \n  [![Transform Data with dbt and Postgres in Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FZLp2N6p2JjE%3Fsi%3DtWhcvq5w4lO8v1_p)](https://youtu.be/ZLp2N6p2JjE?si=tWhcvq5w4lO8v1_p)\n\n\n```mermaid\ngraph LR\n  Extract[Extract Data via HTTP REST API] --> Transform[Transform Data in Python]\n  Transform --> Query[Query Data with DuckDB]\n```\n\nAdd the flow [`01_getting_started_data_pipeline.yaml`](flows/01_getting_started_data_pipeline.yaml) from the UI if you haven't already and execute it to see the results. Inspect the Gantt and Logs tabs to understand the flow execution.\n\n### Local DB: Load Taxi Data to Postgres\n\nBefore we start loading data to GCP, we'll first play with the Yellow and Green Taxi data using a local Postgres database running in a Docker container. We'll create a new Postgres database for these examples using this [Docker Compose file](docker/postgres/docker-compose.yml). Download it into a new directory, navigate to it and run the following command to start it:\n\n```bash\ndocker compose up -d\n```\n\nThe flow will extract CSV data partitioned by year and month, create tables, load data to the monthly table, and finally merge the data to the final destination table.\n\n```mermaid\ngraph LR\n  Start[Select Year & Month] --> SetLabel[Set Labels]\n  SetLabel --> Extract[Extract CSV Data]\n  Extract -->|Taxi=Yellow| YellowFinalTable[Create Yellow Final Table]:::yellow\n  Extract -->|Taxi=Green| GreenFinalTable[Create Green Final Table]:::green\n  YellowFinalTable --> YellowMonthlyTable[Create Yellow Monthly Table]:::yellow\n  GreenFinalTable --> GreenMonthlyTable[Create Green Monthly Table]:::green\n  YellowMonthlyTable --> YellowCopyIn[Load Data to Monthly Table]:::yellow\n  GreenMonthlyTable --> GreenCopyIn[Load Data to Monthly Table]:::green\n  YellowCopyIn --> YellowMerge[Merge Yellow Data]:::yellow\n  GreenCopyIn --> GreenMerge[Merge Green Data]:::green\n\n  classDef yellow fill:#FFD700,stroke:#000,stroke-width:1px;\n  classDef green fill:#32CD32,stroke:#000,stroke-width:1px;\n```\n\nThe flow code: [`02_postgres_taxi.yaml`](flows/02_postgres_taxi.yaml).\n\n\n> [!NOTE]  \n> The NYC Taxi and Limousine Commission (TLC) Trip Record Data provided on the [nyc.gov](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) website is currently available only in a Parquet format, but this is NOT the dataset we're going to use in this course. For the purpose of this course, we'll use the **CSV files** available [here on GitHub](https://github.com/DataTalksClub/nyc-tlc-data/releases). This is because the Parquet format can be challenging to understand by newcomers, and we want to make the course as accessible as possible \u2014 the CSV format can be easily introspected using tools like Excel or Google Sheets, or even a simple text editor.\n\n### Local DB: Learn Scheduling and Backfills\n\nWe can now schedule the same pipeline shown above to run daily at 9 AM UTC. We'll also demonstrate how to backfill the data pipeline to run on historical data.\n\nNote: given the large dataset, we'll backfill only data for the green taxi dataset for the year 2019.\n\nThe flow code: [`02_postgres_taxi_scheduled.yaml`](flows/02_postgres_taxi_scheduled.yaml).\n\n### Local DB: Orchestrate dbt Models (Optional)\n\nNow that we have raw data ingested into a local Postgres database, we can use dbt to transform the data into meaningful insights. The flow will sync the dbt models from Git to Kestra and run the `dbt build` command to build the models.\n\n```mermaid\ngraph LR\n  Start[Select dbt command] --> Sync[Sync Namespace Files]\n  Sync --> DbtBuild[Run dbt CLI]\n```\n\nThis gives you a quick showcase of dbt inside of Kestra so the homework tasks do not depend on it. The course will go into more detail of dbt in [Week 4](../04-analytics-engineering).\n\nThe flow code: [`03_postgres_dbt.yaml`](flows/03_postgres_dbt.yaml).\n\n### Resources\n- [pgAdmin Download](https://www.pgadmin.org/download/)\n- [Postgres DB Docker Compose](docker/postgres/docker-compose.yml)\n\n---\n\n## 4. ETL Pipelines in Kestra: Google Cloud Platform\n\nNow that you've learned how to build ETL pipelines locally using Postgres, we are ready to move to the cloud. In this section, we'll load the same Yellow and Green Taxi data to Google Cloud Platform (GCP) using: \n1. Google Cloud Storage (GCS) as a data lake  \n2. BigQuery as a data warehouse.\n\n### Videos\n\n- **2.2.6 - Create an ETL Pipeline with GCS and BigQuery in Kestra**  \n  [![Create an ETL Pipeline with BigQuery in Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FnKqjjLJ7YXs)](https://youtu.be/nKqjjLJ7YXs)\n- **2.2.7 - Manage Scheduling and Backfills using BigQuery in Kestra**   \n  [![Manage Scheduling and Backfills using BigQuery in Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FDoaZ5JWEkH0)](https://youtu.be/DoaZ5JWEkH0)\n- **2.2.8 - Transform Data with dbt and BigQuery in Kestra**   \n  [![Transform Data with dbt and BigQuery in Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FeF_EdV4A1Wk)](https://youtu.be/eF_EdV4A1Wk)\n\n### Setup Google Cloud Platform (GCP)\n\nBefore we start loading data to GCP, we need to set up the Google Cloud Platform. \n\nFirst, adjust the following flow [`04_gcp_kv.yaml`](flows/04_gcp_kv.yaml) to include your service account, GCP project ID, BigQuery dataset and GCS bucket name (_along with their location_) as KV Store values:\n- GCP_CREDS\n- GCP_PROJECT_ID\n- GCP_LOCATION\n- GCP_BUCKET_NAME\n- GCP_DATASET.\n\n\n> [!WARNING]  \n> The `GCP_CREDS` service account contains sensitive information. Ensure you keep it secure and do not commit it to Git. Keep it as secure as your passwords.\n\n### Create GCP Resources\n\nIf you haven't already created the GCS bucket and BigQuery dataset in the first week of the course, you can use this flow to create them: [`05_gcp_setup.yaml`](flows/05_gcp_setup.yaml).\n\n\n### GCP Workflow: Load Taxi Data to BigQuery\n\n```mermaid\ngraph LR\n  SetLabel[Set Labels] --> Extract[Extract CSV Data]\n  Extract --> UploadToGCS[Upload Data to GCS]\n  UploadToGCS -->|Taxi=Yellow| BQYellowTripdata[Main Yellow Tripdata Table]:::yellow\n  UploadToGCS -->|Taxi=Green| BQGreenTripdata[Main Green Tripdata Table]:::green\n  BQYellowTripdata --> BQYellowTableExt[External Table]:::yellow\n  BQGreenTripdata --> BQGreenTableExt[External Table]:::green\n  BQYellowTableExt --> BQYellowTableTmp[Monthly Table]:::yellow\n  BQGreenTableExt --> BQGreenTableTmp[Monthly Table]:::green\n  BQYellowTableTmp --> BQYellowMerge[Merge to Main Table]:::yellow\n  BQGreenTableTmp --> BQGreenMerge[Merge to Main Table]:::green\n  BQYellowMerge --> PurgeFiles[Purge Files]\n  BQGreenMerge --> PurgeFiles[Purge Files]\n\n  classDef yellow fill:#FFD700,stroke:#000,stroke-width:1px;\n  classDef green fill:#32CD32,stroke:#000,stroke-width:1px;\n```\n\nThe flow code: [`06_gcp_taxi.yaml`](flows/06_gcp_taxi.yaml).\n\n### GCP Workflow: Schedule and Backfill Full Dataset\n\nWe can now schedule the same pipeline shown above to run daily at 9 AM UTC for the green dataset and at 10 AM UTC for the yellow dataset. You can backfill historical data directly from the Kestra UI.\n\nSince we now process data in a cloud environment with infinitely scalable storage and compute, we can backfill the entire dataset for both the yellow and green taxi data without the risk of running out of resources on our local machine.\n\nThe flow code: [`06_gcp_taxi_scheduled.yaml`](flows/06_gcp_taxi_scheduled.yaml).\n\n### GCP Workflow: Orchestrate dbt Models (Optional)\n\nNow that we have raw data ingested into BigQuery, we can use dbt to transform that data. The flow will sync the dbt models from Git to Kestra and run the `dbt build` command to build the models:\n\n```mermaid\ngraph LR\n  Start[Select dbt command] --> Sync[Sync Namespace Files]\n  Sync --> Build[Run dbt Build Command]\n```\n\nThis gives you a quick showcase of dbt inside of Kestra so the homework tasks do not depend on it. The course will go into more detail of dbt in [Week 4](../04-analytics-engineering).\n\nThe flow code: [`07_gcp_dbt.yaml`](flows/07_gcp_dbt.yaml).\n\n---\n\n## 5. Bonus: Deploy to the Cloud (Optional)\n\nNow that we've got our ETL pipeline working both locally and in the cloud, we can deploy Kestra to the cloud so it can continue to orchestrate our ETL pipelines monthly with our configured schedules, We'll cover how you can install Kestra on Google Cloud in Production, and automatically sync and deploy your workflows from a Git repository.\n\nNote: When committing your workflows to Kestra, make sure your workflow doesn't contain any sensitive information. You can use [Secrets](https://go.kestra.io/de-zoomcamp/secret) and the [KV Store](https://go.kestra.io/de-zoomcamp/kv-store) to keep sensitive data out of your workflow logic.\n\n### Videos\n\n- **2.2.9 - Deploy Workflows to the Cloud with Git**   \n  [![Deploy Workflows to the Cloud with Git](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2Fl-wC71tI3co)](https://youtu.be/l-wC71tI3co)\n\nResources\n\n- [Install Kestra on Google Cloud](https://go.kestra.io/de-zoomcamp/gcp-install)\n- [Moving from Development to Production](https://go.kestra.io/de-zoomcamp/dev-to-prod)\n- [Using Git in Kestra](https://go.kestra.io/de-zoomcamp/git)\n- [Deploy Flows with GitHub Actions](https://go.kestra.io/de-zoomcamp/deploy-github-actions)\n\n## 6. Additional Resources \ud83d\udcda\n\n- Check [Kestra Docs](https://go.kestra.io/de-zoomcamp/docs)\n- Explore our [Blueprints](https://go.kestra.io/de-zoomcamp/blueprints) library\n- Browse over 600 [plugins](https://go.kestra.io/de-zoomcamp/plugins) available in Kestra\n- Give us a star on [GitHub](https://go.kestra.io/de-zoomcamp/github)\n- Join our [Slack community](https://go.kestra.io/de-zoomcamp/slack) if you have any questions\n- Find all the videos in this [YouTube Playlist](https://go.kestra.io/de-zoomcamp/yt-playlist)\n\n\n### Troubleshooting tips\n\nIf you face any issues with Kestra flows in Module 2, make sure to use the following Docker images/ports:\n- `kestra/kestra:latest` is correct = latest stable release, while `kestra/kestra:develop` is incorrect as this is a bleeding-edge development version that might contain bugs\n- `postgres:latest` \u2014 make sure to use Postgres image, which uses **PostgreSQL 15** or higher\n- If you run `pgAdmin` or something else on port 8080, you can adjust Kestra docker-compose to use a different port, e.g. change port mapping to 18080 instead of 8080, and then access Kestra UI in your browser from http://localhost:18080/ instead of from http://localhost:8080/\n\nIf you're using Linux, you might encounter `Connection Refused` errors when connecting to the Postgres DB from within Kestra. This is because `host.docker.internal` works differently on Linux. Using the modified Docker Compose file below, you can run both Kestra and its dedicated Postgres DB, as well as the Postgres DB for the exercises all together. You can access it within Kestra by referring to the container name `postgres_zoomcamp` instead of `host.docker.internal` in `pluginDefaults`. This applies to pgAdmin as well. If you'd prefer to keep it in separate Docker Compose files, you'll need to setup a Docker network so that they can communicate with each other.\n\n<details>\n<summary>Docker Compose Example</summary>\n\nThis Docker Compose has the Zoomcamp DB container and pgAdmin container added to it, so it's all in one file.\n\nChanges include:\n- New `volume` for the Zoomcamp DB container\n- Zoomcamp DB container is added and renamed to prevent clashes with the Kestra DB container\n- Depends on condition is added to make sure Kestra is running before it starts\n- pgAdmin is added and running on Port 8085 so it doesn't clash wit Kestra which uses 8080 and 8081\n\n```yaml\nvolumes:\n  postgres-data:\n    driver: local\n  kestra-data:\n    driver: local\n  zoomcamp-data:\n    driver: local\n\nservices:\n  postgres:\n    image: postgres\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_DB: kestra\n      POSTGRES_USER: kestra\n      POSTGRES_PASSWORD: k3str4\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\"]\n      interval: 30s\n      timeout: 10s\n      retries: 10\n\n  kestra:\n    image: kestra/kestra:latest\n    pull_policy: always\n    # Note that this setup with a root user is intended for development purpose.\n    # Our base image runs without root, but the Docker Compose implementation needs root to access the Docker socket\n    # To run Kestra in a rootless mode in production, see: https://kestra.io/docs/installation/podman-compose\n    user: \"root\"\n    command: server standalone\n    volumes:\n      - kestra-data:/app/storage\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /tmp/kestra-wd:/tmp/kestra-wd\n    environment:\n      KESTRA_CONFIGURATION: |\n        datasources:\n          postgres:\n            url: jdbc:postgresql://postgres:5432/kestra\n            driverClassName: org.postgresql.Driver\n            username: kestra\n            password: k3str4\n        kestra:\n          server:\n            basicAuth:\n              enabled: false\n              username: \"admin@kestra.io\" # it must be a valid email address\n              password: kestra\n          repository:\n            type: postgres\n          storage:\n            type: local\n            local:\n              basePath: \"/app/storage\"\n          queue:\n            type: postgres\n          tasks:\n            tmpDir:\n              path: /tmp/kestra-wd/tmp\n          url: http://localhost:8080/\n    ports:\n      - \"8080:8080\"\n      - \"8081:8081\"\n    depends_on:\n      postgres:\n        condition: service_started\n    \n  postgres_zoomcamp:\n    image: postgres\n    environment:\n      POSTGRES_USER: kestra\n      POSTGRES_PASSWORD: k3str4\n      POSTGRES_DB: postgres-zoomcamp\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - zoomcamp-data:/var/lib/postgresql/data\n    depends_on:\n      kestra:\n        condition: service_started\n\n  pgadmin:\n    image: dpage/pgadmin4\n    environment:\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n      - PGADMIN_DEFAULT_PASSWORD=root\n    ports:\n      - \"8085:80\"\n    depends_on:\n      postgres_zoomcamp:\n        condition: service_started\n```\n\n</details>\n\nIf you are still facing any issues, stop and remove your existing Kestra + Postgres containers and start them again using `docker-compose up -d`. If this doesn't help, post your question on the DataTalksClub Slack or on Kestra's Slack http://kestra.io/slack.\n\n- **DE Zoomcamp FAQ - PostgresDB Setup and Installing pgAdmin**   \n  [![DE Zoomcamp FAQ - PostgresDB Setup and Installing pgAdmin](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FywAPYNYFaB4%3Fsi%3D5X9AD0nFAT2WLWgS)](https://youtu.be/ywAPYNYFaB4?si=5X9AD0nFAT2WLWgS)\n- **DE Zoomcamp FAQ - Port and Images**  \n  [![DE Zoomcamp FAQ - Ports and Images](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2Fl2M2mW76RIU%3Fsi%3DoqyZ7KUaI27vi90V)](https://youtu.be/l2M2mW76RIU?si=oqyZ7KUaI27vi90V)\n- **DE Zoomcamp FAQ - Docker Setup**  \n  [![DE Zoomcamp FAQ - Docker Setup](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2F73g6qJN0HcM)](https://youtu.be/73g6qJN0HcM)\n\n\n\nIf you encounter similar errors to:\n```\nBigQueryError{reason=invalid, location=null, \nmessage=Error while reading table: kestra-sandbox.zooomcamp.yellow_tripdata_2020_01, \nerror message: CSV table references column position 17, but line contains only 14 columns.; \nline_number: 2103925 byte_offset_to_start_of_line: 194863028 \ncolumn_index: 17 column_name: \"congestion_surcharge\" column_type: NUMERIC \nFile: gs://anna-geller/yellow_tripdata_2020-01.csv}\n```\n\nIt means that the CSV file you're trying to load into BigQuery has a mismatch in the number of columns between the external source table (i.e. file in GCS) and the destination table in BigQuery. This can happen when for due to network/transfer issues, the file is not fully downloaded from GitHub or not correctly uploaded to GCS. The error suggests schema issues but that's not the case. Simply rerun the entire execution including redownloading the CSV file and reuploading it to GCS. This should resolve the issue.\n\n---\n\n## Homework \n\nSee the [2025 cohort folder](../cohorts/2025/02-workflow-orchestration/homework.md)\n\n\n---\n\n# Community notes\n\nDid you take notes? You can share them by creating a PR to this file! \n\n* [Notes from Manuel Guerra)](https://github.com/ManuelGuerra1987/data-engineering-zoomcamp-notes/blob/main/2_Workflow-Orchestration-(Kestra)/README.md)\n* [Notes from Horeb Seidou](https://spotted-hardhat-eea.notion.site/Week-2-Workflow-Orchestration-17129780dc4a80148debf61e6453fffe)\n* [Notes from Livia](https://docs.google.com/document/d/1Y_QMonvEtFPbXIzmdpCSVsKNC1BWAHFBA1mpK9qaZko/edit?usp=sharing)\n* [2025 Gitbook Notes from Tinker0425](https://data-engineering-zoomcamp-2025-t.gitbook.io/tinker0425/module-2/introduction-to-module-2)\n* [Notes from Mercy Markus: Linux/Fedora Tweaks and Tips](https://mercymarkus.com/posts/2025/series/dtc-dez-jan-2025/dtc-dez-2025-module-2/)\n* Add your notes above this line\n\n---\n\n# Previous Cohorts\n\n* 2022: [notes](../cohorts/2022/week_2_data_ingestion#community-notes) and [videos](../cohorts/2022/week_2_data_ingestion)\n* 2023: [notes](../cohorts/2023/week_2_workflow_orchestration#community-notes) and [videos](../cohorts/2023/week_2_workflow_orchestration)\n* 2024: [notes](../cohorts/2024/02-workflow-orchestration#community-notes) and [videos](../cohorts/2024/02-workflow-orchestration)",
    "filename": "02-workflow-orchestration/README.md"
  },
  {
    "content": "# Data Warehouse and BigQuery\n\n- [Slides](https://docs.google.com/presentation/d/1a3ZoBAXFk8-EhUsd7rAZd-5p_HpltkzSeujjRGB2TAI/edit?usp=sharing)  \n- [Big Query basic SQL](big_query.sql)\n\n# Videos\n\n## Data Warehouse\n\n- Data Warehouse and BigQuery\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/jrHljAoD6nM)](https://youtu.be/jrHljAoD6nM&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=34)\n\n## :movie_camera: Partitioning and clustering\n\n- Partitioning vs Clustering\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/-CqXf7vhhDs)](https://youtu.be/-CqXf7vhhDs?si=p1sYQCAs8dAa7jIm&t=193&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=35)\n\n## :movie_camera: Best practices\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/k81mLJVX08w)](https://youtu.be/k81mLJVX08w&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=36)\n\n## :movie_camera: Internals of BigQuery\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/eduHi1inM4s)](https://youtu.be/eduHi1inM4s&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=37)\n\n## Advanced topics\n\n### :movie_camera: Machine Learning in Big Query\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/B-WtpB0PuG4)](https://youtu.be/B-WtpB0PuG4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=34)\n\n* [SQL for ML in BigQuery](big_query_ml.sql)\n\n**Important links**\n\n- [BigQuery ML Tutorials](https://cloud.google.com/bigquery-ml/docs/tutorials)\n- [BigQuery ML Reference Parameter](https://cloud.google.com/bigquery-ml/docs/analytics-reference-patterns)\n- [Hyper Parameter tuning](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-glm)\n- [Feature preprocessing](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-preprocess-overview)\n\n### :movie_camera: Deploying Machine Learning model from BigQuery\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/BjARzEWaznU)](https://youtu.be/BjARzEWaznU&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=39)\n\n- [Steps to extract and deploy model with docker](extract_model.md)  \n\n\n\n# Homework\n\n* [2025 Homework](../cohorts/2025/03-data-warehouse/homework.md)\n\n\n# Community notes\n\nDid you take notes? You can share them here.\n\n* [Notes by Alvaro Navas](https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/3_data_warehouse.md)\n* [Isaac Kargar's blog post](https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/30/data-engineering-w3.html)\n* [Marcos Torregrosa's blog post](https://www.n4gash.com/2023/data-engineering-zoomcamp-semana-3/) \n* [Notes by Victor Padilha](https://github.com/padilha/de-zoomcamp/tree/master/week3)\n* [Notes from Xia He-Bleinagel](https://xiahe-bleinagel.com/2023/02/week-3-data-engineering-zoomcamp-notes-data-warehouse-and-bigquery/)\n* [Bigger picture summary on Data Lakes, Data Warehouses, and tooling](https://medium.com/@verazabeida/zoomcamp-week-4-b8bde661bf98), by Vera\n* [Notes by froukje](https://github.com/froukje/de-zoomcamp/blob/main/week_3_data_warehouse/notes/notes_week_03.md)\n* [Notes by Alain Boisvert](https://github.com/boisalai/de-zoomcamp-2023/blob/main/week3.md)\n* [Notes from Vincenzo Galante](https://binchentso.notion.site/Data-Talks-Club-Data-Engineering-Zoomcamp-8699af8e7ff94ec49e6f9bdec8eb69fd)\n* [2024 videos transcript week3](https://drive.google.com/drive/folders/1quIiwWO-tJCruqvtlqe_Olw8nvYSmmDJ?usp=sharing) by Maria Fisher \n* [Notes by Linda](https://github.com/inner-outer-space/de-zoomcamp-2024/blob/main/3a-data-warehouse/readme.md)\n* [Jonah Oliver's blog post](https://www.jonahboliver.com/blog/de-zc-w3)\n* [2024 - steps to send data from Mage to GCS + creating external table](https://drive.google.com/file/d/1GIi6xnS4070a8MUlIg-ozITt485_-ePB/view?usp=drive_link) by Maria Fisher\n* [2024 - mage dataloader script to load the parquet files from a remote URL and push it to Google bucket as parquet file](https://github.com/amohan601/dataengineering-zoomcamp2024/blob/main/week_3_data_warehouse/mage_scripts/green_taxi_2022_v2.py) by Anju Mohan\n* [2024 - steps to send data from Mage to GCS + creating external table](https://drive.google.com/file/d/1GIi6xnS4070a8MUlIg-ozITt485_-ePB/view?usp=drive_link) by Maria Fisher \n* [Notes by HongWei](https://github.com/hwchua0209/data-engineering-zoomcamp-submission/blob/main/03-data-warehouse/README.md)\n* [2025 Notes by Manuel Guerra](https://github.com/ManuelGuerra1987/data-engineering-zoomcamp-notes/blob/main/3_Data-Warehouse/README.md)\n* [Notes from Horeb SEIDOU](https://spotted-hardhat-eea.notion.site/Week-3-Data-Warehouse-and-BigQuery-17c29780dc4a80c8a226f372543ae388)\n* [2025 - Notes by Gabi Fonseca](https://github.com/fonsecagabriella/data_engineering/blob/main/03_data_warehouse/00_notes.md)\n* [2025 Gitbook Notes Tinker0425](https://data-engineering-zoomcamp-2025-t.gitbook.io/tinker0425/module-3/introduction-to-module-3)\n* [2025 Notes from Daniel Lachner](https://drive.google.com/file/d/105zjtLFi0sRqqFFgdMSCTzfcLPx2rfv4/view?usp=sharing)\n* Add your notes here (above this line)",
    "filename": "03-data-warehouse/README.md"
  },
  {
    "code": false,
    "content": "# Documentation of SQL Queries for NYC Taxi Data Processing\n\nThis document provides a high-level overview of a series of SQL queries aimed at processing and analyzing NYC taxi and bike share data stored in Google BigQuery. The queries include interacting with both public data sets and external CSV datasets stored in Google Cloud Storage (GCS).\n\n## Querying NYC Citibike Stations\n\nThe initial query retrieves information from a public dataset that contains information about Citibike stations in New York City:\n\n```sql\nSELECT station_id, name FROM\n    bigquery-public-data.new_york_citibike.citibike_stations\nLIMIT 100;\n```\n\n- **Purpose**: This query selects the `station_id` and `name` of the first 100 Citibike stations from the `bigquery-public-data.new_york_citibike.citibike_stations` table. The result provides a quick reference to the available stations in NYC.\n\n## Creating an External Table\n\nNext, the code defines an external table that references CSV files stored in Google Cloud Storage:\n\n```sql\nCREATE OR REPLACE EXTERNAL TABLE `taxi-rides-ny.nytaxi.external_yellow_tripdata`\nOPTIONS (\n  format = 'CSV',\n  uris = ['gs://nyc-tl-data/trip data/yellow_tripdata_2019-*.csv', 'gs://nyc-tl-data/trip data/yellow_tripdata_2020-*.csv']\n);\n```\n\n- **Purpose**: This command creates an external table named `external_yellow_tripdata` in the `taxi-rides-ny.nytaxi` dataset. This table allows users to query the yellow taxi trip data stored in CSV files directly in GCS without importing them into BigQuery.\n\n## Validating Data from the External Table\n\nThe following query allows users to validate the creation and structure of the external table by retrieving a few records:\n\n```sql\nSELECT * FROM taxi-rides-ny.nytaxi.external_yellow_tripdata LIMIT 10;\n```\n\n- **Purpose**: This query fetches the first 10 records from the `external_yellow_tripdata` table, serving as a sanity check to confirm that the data is accessible and correctly formatted.\n\n## Creating Non-Partitioned Table\n\nFollowing the validation, a non-partitioned table is created based on the external table:\n\n```sql\nCREATE OR REPLACE TABLE taxi-rides-ny.nytaxi.yellow_tripdata_non_partitioned AS\nSELECT * FROM taxi-rides-ny.nytaxi.external_yellow_tripdata;\n```\n\n- **Purpose**: This command generates a new table called `yellow_tripdata_non_partitioned` that stores all entries from the external yellow taxi trip data. This serves as a basic form of data extraction for further analysis without any partitioning for performance optimizations.\n\n## Creating a Partitioned Table\n\nThe next step involves creating a partitioned version of the yellow taxi trip data:\n\n```sql\nCREATE OR REPLACE TABLE taxi-rides-ny.nytaxi.yellow_tripdata_partitioned\nPARTITION BY DATE(tpep_pickup_datetime) AS\nSELECT * FROM taxi-rides-ny.nytaxi.external_yellow_tripdata;\n```\n\n- **Purpose**: A new table named `yellow_tripdata_partitioned` is created, partitioned by the `tpep_pickup_datetime` field. Partitioning improves query performance by limiting the amount of data scanned when querying by date.\n\n## Comparing Query Performance: Non-Partitioned vs. Partitioned\n\nTwo separate queries evaluate the impact of partitioning on query performance by checking distinct vendor IDs over the same date range:\n\n```sql\nSELECT DISTINCT(VendorID)\nFROM taxi-rides-ny.nytaxi.yellow_tripdata_non_partitioned\nWHERE DATE(tpep_pickup_datetime) BETWEEN '2019-06-01' AND '2019-06-30';\n\nSELECT DISTINCT(VendorID)\nFROM taxi-rides-ny.nytaxi.yellow_tripdata_partitioned\nWHERE DATE(tpep_pickup_datetime) BETWEEN '2019-06-01' AND '2019-06-30';\n```\n\n- **Purpose**: The first query scans 1.6 GB of data from the non-partitioned table, while the second query only scans approximately 106 MB from the partitioned table. This comparison illustrates the performance benefits gained from partitioning.\n\n## Exploring Partitions\n\nThe subsequent query checks the details of each partition created in the partitioned table:\n\n```sql\nSELECT table_name, partition_id, total_rows\nFROM `nytaxi.INFORMATION_SCHEMA.PARTITIONS`\nWHERE table_name = 'yellow_tripdata_partitioned'\nORDER BY total_rows DESC;\n```\n\n- **Purpose**: This query retrieves information regarding the partitions of the `yellow_tripdata_partitioned` table, listing each partition's ID and the total number of rows it contains. This helps understand the distribution of data across partitions.\n\n## Creating a Partitioned and Clustered Table\n\nThe code then creates a table that combines both partitioning and clustering:\n\n```sql\nCREATE OR REPLACE TABLE taxi-rides-ny.nytaxi.yellow_tripdata_partitioned_clustered\nPARTITION BY DATE(tpep_pickup_datetime)\nCLUSTER BY VendorID AS\nSELECT * FROM taxi-rides-ny.nytaxi.external_yellow_tripdata;\n```\n\n- **Purpose**: This command creates a table named `yellow_tripdata_partitioned_clustered`, which is both partitioned by date and clustered by `VendorID`. Clustering helps to further optimize query performance by organizing the data within each partition based on `VendorID`.\n\n## Comparing Query Performance: Partitioned vs. Partitioned and Clustered\n\nLastly, two queries analyze the trip counts while checking the performance of the partitioned vs. partitioned and clustered tables:\n\n```sql\nSELECT count(*) as trips\nFROM taxi-rides-ny.nytaxi.yellow_tripdata_partitioned\nWHERE DATE(tpep_pickup_datetime) BETWEEN '2019-06-01' AND '2020-12-31'\n  AND VendorID=1;\n\nSELECT count(*) as trips\nFROM taxi-rides-ny.nytaxi.yellow_tripdata_partitioned_clustered\nWHERE DATE(tpep_pickup_datetime) BETWEEN '2019-06-01' AND '2020-12-31'\n  AND VendorID=1;\n```\n\n- **Purpose**: The first query scans 1.1 GB of data, while the second query only scans 864.5 MB from the clustered table. This demonstrates the effectiveness of clustering in reducing the amount of data scanned during queries that use filters on clustered fields. \n\n## Conclusion\n\nThe series of SQL queries presented are designed to effectively utilize Google BigQuery features for managing and analyzing large datasets, like NYC yellow taxi and Citibike data. The use of external tables, partitioning, and clustering not only optimizes the performance of queries but also enhances data management practices for large-scale data analysis.",
    "filename": "03-data-warehouse/big_query.sql"
  },
  {
    "code": false,
    "content": "# Analysis of the SQL Script\n\nThis SQL script primarily deals with creating and manipulating external tables related to taxi ride data in New York City, specifically focused on for-hire vehicle (FHV) trip data for the year 2019. The script includes operations for creating external tables, counting records, and creating new tables based on the external data.\n\n## Table Creation\n\n### External Table Definition\n\n```sql\nCREATE OR REPLACE EXTERNAL TABLE `taxi-rides-ny.nytaxi.fhv_tripdata`\nOPTIONS (\n  format = 'CSV',\n  uris = ['gs://nyc-tl-data/trip data/fhv_tripdata_2019-*.csv']\n);\n```\n\nThis part of the script creates or replaces an external table named `fhv_tripdata` in the `taxi-rides-ny` project, specifically under the `nytaxi` dataset. The external table is defined to read CSV files stored in a Google Cloud Storage (GCS) bucket. The `uris` option points to all CSV files matching the pattern `fhv_tripdata_2019-*.csv`, which includes all FHV trip data files for 2019. By using an external table, the content in the files can be queried directly without needing to load it into a regular database table.\n\n## Data Overview and Count\n\n### Total Records Count\n\n```sql\nSELECT count(*) FROM `taxi-rides-ny.nytaxi.fhv_tripdata`;\n```\n\nThis query counts the total number of records present in the `fhv_tripdata` external table. This count can provide insights into the overall amount of trip data collected for the designated period.\n\n### Unique Dispatch Base Count\n\n```sql\nSELECT COUNT(DISTINCT(dispatching_base_num)) FROM `taxi-rides-ny.nytaxi.fhv_tripdata`;\n```\n\nHere, the script counts the number of distinct dispatching base numbers found in the `fhv_tripdata`. The dispatching base number is likely a unique identifier for the different taxi service providers. This query helps in understanding how many unique operators are present in the dataset.\n\n## Data Transformation\n\n### Non-Partitioned Table Creation\n\n```sql\nCREATE OR REPLACE TABLE `taxi-rides-ny.nytaxi.fhv_nonpartitioned_tripdata`\nAS SELECT * FROM `taxi-rides-ny.nytaxi.fhv_tripdata`;\n```\n\nIn this query, a new table named `fhv_nonpartitioned_tripdata` is created, which contains all the data from the external `fhv_tripdata` table. This is a regular table, meaning the data is persisted in the dataset and not dependent on the files in GCS. This conversion allows for more flexible query options and might improve performance compared to querying the external table directly.\n\n### Partitioned Table Creation\n\n```sql\nCREATE OR REPLACE TABLE `taxi-rides-ny.nytaxi.fhv_partitioned_tripdata`\nPARTITION BY DATE(dropoff_datetime)\nCLUSTER BY dispatching_base_num AS (\n  SELECT * FROM `taxi-rides-ny.nytaxi.fhv_tripdata`\n);\n```\n\nThis part of the script creates another table called `fhv_partitioned_tripdata`. This table is both partitioned and clustered: it is partitioned by the drop-off date (from the `dropoff_datetime` column) and clustered by the dispatching base number. Partitioning helps in optimizing query performance by allowing the database engine to scan only relevant partitions of data. Clustering groups data with similar attributes closer together, enhancing performance during query operations.\n\n## Filtered Counts for Selected Dispatch Bases\n\n### Non-Partitioned Data Count\n\n```sql\nSELECT count(*) FROM  `taxi-rides-ny.nytaxi.fhv_nonpartitioned_tripdata`\nWHERE DATE(dropoff_datetime) BETWEEN '2019-01-01' AND '2019-03-31'\n  AND dispatching_base_num IN ('B00987', 'B02279', 'B02060');\n```\n\nThis query counts the records in the `fhv_nonpartitioned_tripdata` table where the drop-off date falls between January 1 and March 31, 2019, and the dispatching base number matches a specified set of values (`'B00987', 'B02279', 'B02060'`). This count helps to evaluate the number of trips made by specific taxi companies during the first quarter of 2019.\n\n### Partitioned Data Count\n\n```sql\nSELECT count(*) FROM `taxi-rides-ny.nytaxi.fhv_partitioned_tripdata`\nWHERE DATE(dropoff_datetime) BETWEEN '2019-01-01' AND '2019-03-31'\n  AND dispatching_base_num IN ('B00987', 'B02279', 'B02060');\n```\n\nThe final query performs a similar count, but this time against the `fhv_partitioned_tripdata`. The filtering criteria are the same as the previous query, allowing for direct comparison of performance and results between querying a partitioned and non-partitioned table. This will demonstrate how partitioning may affect query execution time and efficiency.\n\n## Summary\n\nIn summary, this script effectively sets up a workflow for managing and analyzing New York City's taxi ride data through various SQL operations. It:\n\n1. Creates an external representation of FHV trip data.\n2. Provides methods for counting total and unique entries.\n3. Transforms the external dataset into both non-partitioned and partitioned tables to enhance data management and query performance.\n4. Executes filtered queries to retrieve data for specified dispatch bases during the first quarter of 2019.\n\nBy structuring the data in partitioned formats, it aims to facilitate quicker access and analysis, particularly useful for large datasets typical in transportation data analytics.",
    "filename": "03-data-warehouse/big_query_hw.sql"
  },
  {
    "code": false,
    "content": "# Documentation for Taxi Ride Data Analysis Code\n\nThis document outlines the functionality of a SQL script designed for processing taxi ride data, training a machine learning model to predict tips, and evaluating its performance. \n\n## 1. Data Selection\n\n```sql\nSELECT passenger_count, trip_distance, PULocationID, DOLocationID, payment_type, fare_amount, tolls_amount, tip_amount\nFROM `taxi-rides-ny.nytaxi.yellow_tripdata_partitioned` WHERE fare_amount != 0;\n```\n\nThe initial step involves querying a dataset containing information on taxi rides. The script selects specific columns that include passenger count, trip distance, pickup and drop-off location IDs, payment type, fare amounts, tolls, and tip amounts. Importantly, it filters out rows where the fare amount is zero, indicating that only valid trips are considered for further analysis.\n\n## 2. Creation of ML Table\n\n```sql\nCREATE OR REPLACE TABLE `taxi-rides-ny.nytaxi.yellow_tripdata_ml` (\n`passenger_count` INTEGER,\n`trip_distance` FLOAT64,\n`PULocationID` STRING,\n`DOLocationID` STRING,\n`payment_type` STRING,\n`fare_amount` FLOAT64,\n`tolls_amount` FLOAT64,\n`tip_amount` FLOAT64\n) AS (\nSELECT passenger_count, trip_distance, cast(PULocationID AS STRING), CAST(DOLocationID AS STRING),\nCAST(payment_type AS STRING), fare_amount, tolls_amount, tip_amount\nFROM `taxi-rides-ny.nytaxi.yellow_tripdata_partitioned` WHERE fare_amount != 0\n);\n```\n\nNext, a new table named `yellow_tripdata_ml` is created to store a Machine Learning (ML) relevant dataset. It includes the same selected columns and specifies appropriate data types for each column. The use of `CREATE OR REPLACE TABLE` indicates that if this table already exists, it will be replaced with the new data, ensuring that it always contains the most current data.\n\n## 3. Model Creation\n\n```sql\nCREATE OR REPLACE MODEL `taxi-rides-ny.nytaxi.tip_model`\nOPTIONS\n(model_type='linear_reg',\ninput_label_cols=['tip_amount'],\nDATA_SPLIT_METHOD='AUTO_SPLIT') AS\nSELECT\n*\nFROM\n`taxi-rides-ny.nytaxi.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL;\n```\n\nIn this part, a linear regression model named `tip_model` is created using the data from the `yellow_tripdata_ml` table. The model is designed to predict the `tip_amount`, which is specified as the label column. The `AUTO_SPLIT` method automatically handles data division for training and evaluation purposes.\n\n## 4. Feature Information Check\n\n```sql\nSELECT * FROM ML.FEATURE_INFO(MODEL `taxi-rides-ny.nytaxi.tip_model`);\n```\n\nThis SQL query retrieves the feature information from the `tip_model`. This includes statistics on the input features, allowing users to evaluate which features may be significant in predicting the tip amounts. This insight is crucial for understanding the model's landscape and the relationship between the inputs and outputs.\n\n## 5. Model Evaluation\n\n```sql\nSELECT\n*\nFROM\nML.EVALUATE(MODEL `taxi-rides-ny.nytaxi.tip_model`,\n(\nSELECT\n*\nFROM\n`taxi-rides-ny.nytaxi.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL\n));\n```\n\nThe evaluation step assesses the performance of the `tip_model` using metrics such as accuracy, precision, recall, and other relevant statistics. By querying the same data without null tip amounts, the model's predictions can be rigorously evaluated, ensuring that the model generalizes well to unseen data.\n\n## 6. Making Predictions\n\n```sql\nSELECT\n*\nFROM\nML.PREDICT(MODEL `taxi-rides-ny.nytaxi.tip_model`,\n(\nSELECT\n*\nFROM\n`taxi-rides-ny.nytaxi.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL\n));\n```\n\nOnce evaluated, the model is employed to make predictions on the existing dataset. This query produces a set of predictions based on the inputs supplied to the `tip_model`. The output will include the predicted tip amounts, alongside the input features for context.\n\n## 7. Explanation of Predictions\n\n```sql\nSELECT\n*\nFROM\nML.EXPLAIN_PREDICT(MODEL `taxi-rides-ny.nytaxi.tip_model`,\n(\nSELECT\n*\nFROM\n`taxi-rides-ny.nytaxi.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL\n), STRUCT(3 as top_k_features));\n```\n\nThis query dives deeper by explaining the predictions made by the model. It outlines which features are most influential in determining the predicted tip amounts. By specifying `top_k_features`, the model reveals the top contributing features for each prediction, offering interpretability to the ML model.\n\n## 8. Hyperparameter Tuning\n\n```sql\nCREATE OR REPLACE MODEL `taxi-rides-ny.nytaxi.tip_hyperparam_model`\nOPTIONS\n(model_type='linear_reg',\ninput_label_cols=['tip_amount'],\nDATA_SPLIT_METHOD='AUTO_SPLIT',\nnum_trials=5,\nmax_parallel_trials=2,\nl1_reg=hparam_range(0, 20),\nl2_reg=hparam_candidates([0, 0.1, 1, 10])) AS\nSELECT\n*\nFROM\n`taxi-rides-ny.nytaxi.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL;\n```\n\nFinally, this section focuses on optimizing the model through hyperparameter tuning. A new model named `tip_hyperparam_model` is created with various hyperparameter settings that influence regularization (L1 and L2). By conducting multiple trials and parallel executions, the script aims to find the most effective model configuration, potentially improving prediction accuracy and reducing overfitting.\n\n## Conclusion\n\nIn summary, this script effectively processes NYC taxi ride data to train a machine learning model predicting tip amounts. It includes steps for data selection, model creation, evaluation, prediction, and hyperparameter tuning, providing a comprehensive pipeline for analyzing and predicting tips based on ride characteristics. Through these processes, it enables better insights into tipping behaviors in the New York taxi industry.",
    "filename": "03-data-warehouse/big_query_ml.sql"
  },
  {
    "content": "## Model deployment\n[Tutorial](https://cloud.google.com/bigquery-ml/docs/export-model-tutorial)\n### Steps\n- gcloud auth login\n- bq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\n- mkdir /tmp/model\n- gsutil cp -r gs://taxi_ml_model/tip_model /tmp/model\n- mkdir -p serving_dir/tip_model/1\n- cp -r /tmp/model/tip_model/* serving_dir/tip_model/1\n- docker pull tensorflow/serving\n- docker run -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=\n  /models/tip_model -e MODEL_NAME=tip_model -t tensorflow/serving &\n- curl -d '{\"instances\": [{\"passenger_count\":1, \"trip_distance\":12.2, \"PULocationID\":\"193\", \"DOLocationID\":\"264\", \"payment_type\":\"2\",\"fare_amount\":20.4,\"tolls_amount\":0.0}]}' -X POST http://localhost:8501/v1/models/tip_model:predict\n- http://localhost:8501/v1/models/tip_model",
    "filename": "03-data-warehouse/extract_model.md"
  },
  {
    "content": "Quick hack to load files directly to GCS, without Airflow. Downloads csv files from https://nyc-tlc.s3.amazonaws.com/trip+data/ and uploads them to your Cloud Storage Account as parquet files.\n\n1. Install pre-reqs (more info in `web_to_gcs.py` script)\n2. Run: `python web_to_gcs.py`",
    "filename": "03-data-warehouse/extras/README.md"
  },
  {
    "code": false,
    "content": "# Google Cloud Storage Upload Script Documentation\n\nThis Python script is designed to download, transform, and upload taxi trip data from NYC's Taxi & Limousine Commission (TLC) to a Google Cloud Storage (GCS) bucket. It makes use of pandas for data manipulation, requests for HTTP operations, and the Google Cloud Storage client for uploading files.\n\n## Prerequisites\n\nBefore executing this script, ensure the following prerequisites are met:\n\n1. Install necessary Python packages:\n   ```\n   pip install pandas pyarrow google-cloud-storage\n   ```\n\n2. Set the environment variable `GOOGLE_APPLICATION_CREDENTIALS` to point to your project/service account key.\n\n3. Define the `GCP_GCS_BUCKET` environment variable to specify your GCS bucket. If not set, it will default to \"dtc-data-lake-bucketname\".\n\n## Constants and URL Setup\n\nThe script initializes a constant URL pointing to the GitHub repository where the NYC TLC data is hosted:\n```python\ninit_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/'\n```\nAdditionally, it retrieves the name of the GCS bucket from the environment variable:\n```python\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\", \"dtc-data-lake-bucketname\")\n```\n\n## Function: upload_to_gcs\n\nThis function is responsible for uploading files to Google Cloud Storage. It takes three parameters: \n\n- `bucket`: The name of the GCS bucket.\n- `object_name`: The name/path under which the file will be stored in GCS.\n- `local_file`: The path of the local file to be uploaded.\n\n### Implementation Details\n\nThe function initializes a GCS client, retrieves the specified bucket, and uses it to upload the specified local file. It includes a reference to workarounds for file upload limitations typically faced when files exceed 6 MB in size.\n\n## Function: web_to_gcs\n\nThis function downloads datasets from the web, converts them from CSV format to Parquet format, and uploads them to Google Cloud Storage. It takes two parameters:\n\n- `year`: The year of the data to be processed (e.g., '2019', '2020').\n- `service`: The type of taxi service (e.g., 'green', 'yellow').\n\n### Step-by-Step Walkthrough\n\n1. **Monthly Loop**: The function iterates over the months from January to December.\n  \n2. **File Naming**: For each month, it constructs the name of the CSV file using the service type and year. Months are formatted to ensure two digits (e.g., '01', '02').\n\n3. **HTTP Request**: It downloads the CSV file from the constructed URL using the `requests` library. The downloaded content is saved as a local file.\n\n4. **Data Reading and Conversion**: The local CSV file is read into a pandas DataFrame with gzip compression. It is then converted and saved as a Parquet file for efficient storage and querying.\n\n5. **GCS Upload**: The Parquet file is uploaded to the specified GCS bucket using the `upload_to_gcs` function.\n\n6. **Logging**: Throughout the process, the script outputs messages to provide feedback about the local file's creation and successful uploads to GCS.\n\n## Dataset Processing Execution\n\nAt the bottom of the script, two function calls for processing specific years and services are provided:\n\n```python\nweb_to_gcs('2019', 'green')\nweb_to_gcs('2020', 'green')\n```\n\nThese lines indicate that the script is set up to process green taxi data for the years 2019 and 2020. Additional lines for yellow taxi data are commented out, which implies they may be intended for future execution.\n\n## Conclusion\n\nThis script serves as a straightforward utility for downloading NYC taxi trip data, transforming it into a more efficient format, and uploading it to cloud storage. It can be easily extended by modifying the function calls for different years or service types, enhancing its versatility for various data processing needs.",
    "filename": "03-data-warehouse/extras/web_to_gcs.py"
  },
  {
    "content": "# Module 4: Analytics Engineering \nGoal: Transforming the data loaded in DWH into Analytical Views developing a [dbt project](taxi_rides_ny/README.md).\n\n### Prerequisites\nBy this stage of the course you should have already: \n\n- A running warehouse (BigQuery or postgres) \n- A set of running pipelines ingesting the project dataset (week 3 completed)\n- The following datasets ingested from the course [Datasets list](https://github.com/DataTalksClub/nyc-tlc-data/): \n  * Yellow taxi data - Years 2019 and 2020\n  * Green taxi data - Years 2019 and 2020 \n  * fhv data - Year 2019. \n\n> [!NOTE]\n> * We have prepared a python script for loading the data that can be found through [week3/extras](../03-data-warehouse/extras).\n\n## Setting up your environment \n  \n> [!NOTE]  \n>  the *cloud* setup is the preferred option.\n>\n> the *local* setup does not require a cloud database.\n\n| Alternative A | Alternative B |\n---|---|\n| Setting up dbt for using BigQuery (cloud) | Setting up dbt for using Postgres locally  |\n|- Open a free developer dbt cloud account following [this link](https://www.getdbt.com/signup/)|- Open a free developer dbt cloud account following [this link](https://www.getdbt.com/signup/)<br><br> |\n| - [Following these instructions to connect to your BigQuery instance]([https://docs.getdbt.com/docs/dbt-cloud/cloud-configuring-dbt-cloud/cloud-setting-up-bigquery-oauth](https://docs.getdbt.com/guides/bigquery?step=4)) | - follow the [official dbt documentation]([https://docs.getdbt.com/dbt-cli/installation](https://docs.getdbt.com/docs/core/installation-overview)) or <br>- follow the [dbt core with BigQuery on Docker](docker_setup/README.md) guide to setup dbt locally on docker or <br>- use a docker image from official [Install with Docker](https://docs.getdbt.com/docs/core/docker-install). |\n|- More detailed instructions in [dbt_cloud_setup.md](dbt_cloud_setup.md)  | - You will need to install the latest version with the BigQuery adapter (dbt-bigquery).|\n| | - You will need to install the latest version with the postgres adapter (dbt-postgres).|\n| | After local installation you will have to set up the connection to PG in the `profiles.yml`, you can find the templates [here](https://docs.getdbt.com/docs/core/connect-data-platform/postgres-setup) |\n\n\n## Content\n\n### Introduction to analytics engineering\n\n* What is analytics engineering?\n* ETL vs ELT \n* Data modeling concepts (fact and dim tables)\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/uF76d5EmdtU)](https://youtu.be/uF76d5EmdtU&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=40)\n\n### What is dbt? \n\n* Introduction to dbt \n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/4eCouvVOJUw)](https://www.youtube.com/watch?v=gsKuETFJr54&list=PLaNLNpjZpzwgneiI-Gl8df8GCsPYp_6Bs&index=5)\n\n## Starting a dbt project\n\n| Alternative A  | Alternative B   |\n|-----------------------------|--------------------------------|\n| Using BigQuery + dbt cloud | Using Postgres + dbt core (locally) |\n| - Starting a new project with dbt init (dbt cloud and core)<br>- dbt cloud setup<br>- project.yml<br><br> | - Starting a new project with dbt init (dbt cloud and core)<br>- dbt core local setup<br>- profiles.yml<br>- project.yml                                  |\n| [![](https://markdown-videos-api.jorgenkh.no/youtube/iMxh6s_wL4Q)](https://www.youtube.com/watch?v=J0XCDyKiU64&list=PLaNLNpjZpzwgneiI-Gl8df8GCsPYp_6Bs&index=4) | [![](https://markdown-videos-api.jorgenkh.no/youtube/1HmL63e-vRs)](https://youtu.be/1HmL63e-vRs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=43) |\n\n### dbt models\n\n* Anatomy of a dbt model: written code vs compiled Sources\n* materializations: table, view, incremental, ephemeral  \n* Seeds, sources and ref  \n* Jinja and Macros \n* Packages \n* Variables\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/UVI30Vxzd6c)](https://www.youtube.com/watch?v=ueVy2N54lyc&list=PLaNLNpjZpzwgneiI-Gl8df8GCsPYp_6Bs&index=3)\n\n> [!NOTE]  \n> *This video is shown entirely on dbt cloud IDE but the same steps can be followed locally on the IDE of your choice*\n\n> [!TIP] \n>* If you receive an error stating \"Permission denied while globbing file pattern.\" when attempting to run `fact_trips.sql` this video may be helpful in resolving the issue\n>\n>[![](https://markdown-videos-api.jorgenkh.no/youtube/kL3ZVNL9Y4A)](https://youtu.be/kL3ZVNL9Y4A&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=34)\n\n### Testing and documenting dbt models\n* Tests  \n* Documentation \n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/UishFmq1hLM)](https://www.youtube.com/watch?v=2dNJXHFCHaY&list=PLaNLNpjZpzwgneiI-Gl8df8GCsPYp_6Bs&index=2)\n\n>[!NOTE]  \n> *This video is shown entirely on dbt cloud IDE but the same steps can be followed locally on the IDE of your choice*\n\n## Deployment\n\n| Alternative A  | Alternative B   |\n|-----------------------------|--------------------------------|\n| Using BigQuery + dbt cloud | Using Postgres + dbt core (locally) |\n| - Deployment: development environment vs production<br>- dbt cloud: scheduler, sources and hosted documentation  | - Deployment: development environment vs production<br>-  dbt cloud: scheduler, sources and hosted documentation |\n| [![](https://markdown-videos-api.jorgenkh.no/youtube/rjf6yZNGX8I)](https://www.youtube.com/watch?v=V2m5C0n8Gro&list=PLaNLNpjZpzwgneiI-Gl8df8GCsPYp_6Bs&index=6) | [![](https://markdown-videos-api.jorgenkh.no/youtube/Cs9Od1pcrzM)](https://youtu.be/Cs9Od1pcrzM&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=47) |\n\n## visualizing the transformed data\n\n:movie_camera: Google data studio Video (Now renamed to Looker studio)\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/39nLTs74A3E)](https://youtu.be/39nLTs74A3E&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=48)\n\n:movie_camera: Metabase Video\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/BnLkrA7a6gM)](https://youtu.be/BnLkrA7a6gM&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=49)\n\n## Extra resources\n\n\n> [!NOTE]\n> If you find the videos above overwhelming, we recommend completing the [dbt Fundamentals](https://learn.getdbt.com/courses/dbt-fundamentals) course and then rewatching the module. It provides a solid foundation for all the key concepts you need in this module.\n\n \n## Advanced concepts\n\n * [Make a model Incremental](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models)\n * [Use of tags](https://docs.getdbt.com/reference/resource-configs/tags)\n * [Hooks](https://docs.getdbt.com/docs/building-a-dbt-project/hooks-operations)\n * [Analysis](https://docs.getdbt.com/docs/building-a-dbt-project/analyses)\n * [Snapshots](https://docs.getdbt.com/docs/building-a-dbt-project/snapshots)\n * [Exposure](https://docs.getdbt.com/docs/building-a-dbt-project/exposures)\n * [Metrics](https://docs.getdbt.com/docs/building-a-dbt-project/metrics)\n\n\n\n## SQL refresher\n\nThe homework for this module focuses heavily on window functions and CTEs. If you need a refresher on these topics, you can refer to these notes.\n\n* [SQL refresher](SQL_refresher.md)\n\n## Homework\n\n* [2025 Homework](../cohorts/2025/04-analytics-engineering/homework.md)\n\n\n## Community notes\n\nDid you take notes? You can share them here.\n\n* [Notes by Alvaro Navas](https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/4_analytics.md)\n* [Sandy's DE learning blog](https://learningdataengineering540969211.wordpress.com/2022/02/17/week-4-setting-up-dbt-cloud-with-bigquery/)\n* [Notes by Victor Padilha](https://github.com/padilha/de-zoomcamp/tree/master/week4)\n* [Marcos Torregrosa's blog (spanish)](https://www.n4gash.com/2023/data-engineering-zoomcamp-semana-4/)\n* [Notes by froukje](https://github.com/froukje/de-zoomcamp/blob/main/week_4_analytics_engineering/notes/notes_week_04.md)\n* [Notes by Alain Boisvert](https://github.com/boisalai/de-zoomcamp-2023/blob/main/week4.md)\n* [Setting up Prefect with dbt by Vera](https://medium.com/@verazabeida/zoomcamp-week-5-5b6a9d53a3a0)\n* [Blog by Xia He-Bleinagel](https://xiahe-bleinagel.com/2023/02/week-4-data-engineering-zoomcamp-notes-analytics-engineering-and-dbt/)\n* [Setting up DBT with BigQuery by Tofag](https://medium.com/@fagbuyit/setting-up-your-dbt-cloud-dej-9-d18e5b7c96ba)\n* [Blog post by Dewi Oktaviani](https://medium.com/@oktavianidewi/de-zoomcamp-2023-learning-week-4-analytics-engineering-with-dbt-53f781803d3e)\n* [Notes from Vincenzo Galante](https://binchentso.notion.site/Data-Talks-Club-Data-Engineering-Zoomcamp-8699af8e7ff94ec49e6f9bdec8eb69fd)\n* [Notes from Balaji](https://github.com/Balajirvp/DE-Zoomcamp/blob/main/Week%204/Data%20Engineering%20Zoomcamp%20Week%204.ipynb)\n* [Notes by Linda](https://github.com/inner-outer-space/de-zoomcamp-2024/blob/main/4-analytics-engineering/readme.md)\n* [2024 - Videos transcript week4](https://drive.google.com/drive/folders/1V2sHWOotPEMQTdMT4IMki1fbMPTn3jOP?usp=drive)\n* [Blog Post](https://www.jonahboliver.com/blog/de-zc-w4) by Jonah Oliver\n* [2025 Notes by Manuel Guerra](https://github.com/ManuelGuerra1987/data-engineering-zoomcamp-notes/blob/main/4_Analytics-Engineering/README.md)\n* [2025 Notes by Horeb SEIDOU](https://spotted-hardhat-eea.notion.site/Week-4-Analytics-Engineering-18929780dc4a808692e4e0ee488bf49c?pvs=74)\n* [2025 Notes by Daniel Lachner](https://github.com/mossdet/dlp_data_eng/blob/main/Notes/04_01_Analytics_Engineering.pdf)\n* Add your notes here (above this line)\n\n## Useful links\n- [Slides used in the videos](https://docs.google.com/presentation/d/1xSll_jv0T8JF4rYZvLHfkJXYqUjPtThA/edit?usp=sharing&ouid=114544032874539580154&rtpof=true&sd=true)\n- [Visualizing data with Metabase course](https://www.metabase.com/learn/visualization/)\n- [dbt free courses](https://courses.getdbt.com/collections)",
    "filename": "04-analytics-engineering/README.md"
  },
  {
    "content": "# SQL Refresher for module 4\n\n### Table of contents\n\n\n- [Window Functions](#window-funtions)\n    - [Row Number](#row-number)\n    - [Rank and Dense Rank](#rank-and-dense-rank)    \n    - [Lag and Lead](#lag-and-lead)   \n    - [Percentile Cont](#percentile-cont)         \n- [Common Table Expression](#common-table-expression)\n- [dbt models and CTEs](#dbt-models-and-ctes)\n\n\n\n## Window Functions    \n\nA window function performs a calculation across a set of table rows that are related to the current row within a specific \"window\" or subset of data. This is comparable to the type of calculation that can be done with an aggregate function  (such as SUM(), AVG(), COUNT(), etc.).\n\nBut unlike regular aggregate functions, use of a window function does not cause rows to become grouped into a single output row \u2014 the rows retain their separate identities.\n\n\n**Syntax:**\n\n```sql\nFUNCTION() OVER (PARTITION BY column_name ORDER BY column_name)\n```\n\nA window function always has two components. This second part here defines your window:\n\n```sql\nOVER (PARTITION BY column_name ORDER BY column_name)\n```\n\nYour window here is how you want to be viewing your data when you're applying your function\n\n- PARTITION BY: divides the result set into groups (optional).\n\n- ORDER BY: defines the order of processing rows within the partition.\n\n\n**Common Window Functions:**\n\nRanking Functions:\n\n- ROW_NUMBER(): Assigns a unique row number within a partition.\n- RANK(): Similar to ROW_NUMBER(), but assigns the same rank to duplicate values, skipping numbers.\n- DENSE_RANK(): Like RANK(), but without gaps in numbering.\n\nAggregate Functions as Window Functions:\n\n- SUM() OVER(): Computes a running total.\n- AVG() OVER(): Computes a moving average.\n\nLag and Lead Functions:\n\n- LAG(): Retrieves the value from a previous row.\n- LEAD(): Retrieves the value from the next row.\n\n\n\n\n### Row Number\n\nROW_NUMBER() does just what it sounds like\u2014displays the number of a given row. It starts at 1 and numbers the rows according to the ORDER BY part of the window statement. Using the PARTITION BY clause will allow you to begin counting 1 again in each partition.\n\n**Syntax:**\n\n```sql\nROW_NUMBER() OVER (PARTITION BY column_name ORDER BY column_name)\n```\n\n**Common Uses:**\n\n- Removing Duplicates: You can use ROW_NUMBER() to identify duplicate rows and keep only one by filtering out rows with a row number greater than 1.\n\n- Ranking Data: Used when ranking rows based on specific criteria but requiring unique row numbers.\n\n- Selecting the Latest Record: Helps in selecting the most recent entry per category when combined with PARTITION BY.\n\n**Example 1:**\n\n```sql\n\nSELECT \n  total_amount,\n  ROW_NUMBER() OVER (ORDER BY total_amount DESC) AS ranking\n\nFROM `greentaxi_trips` \nLIMIT 10;\n\n```\n\nThe query returns the top 10 highest total_amount values from the table, along with a row number indicating their ranking.\n\n\n| total_amount | ranking |\n|--------|--------|\n| 4012.3 | 1      |\n| 2878.3 | 2      |\n| 2438.8 | 3      |\n| 2156.3 | 4      |\n| 2109.8 | 5      |\n| 2017.3 | 6      |\n| 1971.05| 7      |\n| 1958.8 | 8      |\n| 1762.8 | 9      |\n| 1600.8 | 10     |\n\nThe column generated with ROW_NUMBER() is temporary and does not modify the original table. It is just a calculation applied to the data in the query result.\n\n**Example 2:**\n\nLet's modify the previous query to add a partition by pick up location ID\n\n```sql\n\nSELECT \n\n  total_amount,\n  PULocationID,\n  ROW_NUMBER() OVER (PARTITION BY PULocationID ORDER BY total_amount DESC) AS ranking\n\nFROM `greentaxi_trips` \nLIMIT 10;\n\n```\n\nThis SQL query  assigns a ranking to each row based on total_amount in descending order within each \nPULocationID group:\n\n| total_amount | PULocationID | ranking |\n|-----------|-----------|-----------|\n| 8.51      | 224       | 432       |\n| 8.3       | 224       | 433       |\n| 8.3       | 224       | 434       |\n| 7.3       | 224       | 435       |\n| 3.3       | 224       | 436       |\n| 86.42     | 234       | 1         |\n| 73.5      | 234       | 2         |\n| 62.7      | 234       | 3         |\n| 61.94     | 234       | 4         |\n| 61.94     | 234       | 5         |\n\nUsing the PARTITION BY clause will allow you to begin counting 1 again in each partition.\n\n### Rank and Dense Rank\n\nROW_NUMBER(), RANK(), and DENSE_RANK() are window functions used to assign a ranking to rows based on a specified order. However, they behave differently when there are duplicate values in the ranking column.\n\nRANK() assigns a ranking, but skips numbers if there are ties. DENSE_RANK() its similar to RANK(), but does not skip numbers when there are ties.\n\nFor example:\n\n| Score | ROW_NUMBER() | RANK() | DENSE_RANK() |\n|-------|--------------|--------|--------------|\n| 95    | 1            | 1      | 1            |\n| 90    | 2            | 2      | 2            |\n| 90    | 3            | 2      | 2            |\n| 85    | 4            | 4      | 3            |\n\n\n### Lag and Lead\n\nIt can often be useful to compare rows to preceding or following rows. You can use LAG or LEAD to create columns that pull values from other rows without the need for a self-join. All you need to do is enter which column to pull from and how many rows away you'd like to do the pull. LAG pulls from previous rows and LEAD pulls from following rows\n\n\n**Syntax:**\n\n```sql\n\nLAG(expression) OVER (PARTITION BY partition_expression ORDER BY order_expression)\n```\n\n- expression: The column whose value you want to retrieve from the previous row\n- offset (optional): The number of rows back from the current row to look. The default is 1, meaning it looks at the immediate previous row.\n- PARTITION BY (optional): Divides the result set into partitions to apply the function to each partition separately.\n- ORDER BY: Specifies the order in which the rows are processed.\n\n**Example:**\n\n```sql\n\nSELECT \n\nlpep_pickup_datetime,\ntotal_amount,\nLAG(total_amount) OVER (ORDER BY lpep_pickup_datetime) as prev_total_amount,\nLEAD(total_amount) OVER (ORDER BY lpep_pickup_datetime) as next_total_amount\n\nFROM `greentaxi_trips` \nORDER BY lpep_pickup_datetime\n\n```\n\nThe query retrieves the lpep_pickup_datetime, total_amount, the previous trip's total_amount, and the next trip's total_amount.\n\n| lpep_pickup_datetime      | total_amount | prev_total_amount | next_total_amount |\n|---------------------------|--------------|-------------------|-------------------|\n| 2008-12-31 23:33:38 UTC   | 7.3          | 6.3               | 5.3               |\n| 2008-12-31 23:42:31 UTC   | 5.3          | 7.3               | 14.55             |\n| 2008-12-31 23:47:51 UTC   | 14.55        | 5.3               | 19.55             |\n| 2008-12-31 23:57:46 UTC   | 19.55        | 14.55             | 9.8               |\n| 2009-01-01 00:00:00 UTC   | 9.8          | 19.55             | 81.3              |\n\n\n### Percentile Cont\n\nComputes the specified percentile value for the value_expression, with linear interpolation.\n\n**Syntax:**\n\n```sql\n\nPERCENTILE_CONT(value_expression, percentile ) OVER (PARTITION BY partition_expression)\n```\n\n**Example:**\n\nLet's calculate the 90th percentile of total_amount for each unique pickup location (PULocationID)\n\n```sql\n\nSELECT \n  PULocationID,\n  total_amount,\n  PERCENTILE_CONT(total_amount, 0.9 ) OVER (PARTITION BY PULocationID) AS p90\n\nFROM `greentaxi_trips` \n\n```\n\n- PERCENTILE_CONT(total_amount, 0.9): calculates the 90th percentile (p90) of total_amount\n- PARTITION BY PULocationID: This groups the calculations by PULocationID, so the 90th percentile is computed separately for each location.\n\n\nQuery results looks like this:\n\n| PULocationID | total_amount  | p90  |\n|------|-------|-------|\n| 224  | 17.3    | 51.9  |\n| 224  | 20.67    | 51.9  |\n| 224  | 21    | 51.9  |\n| 224  | 26.06 | 51.9  |\n| 224  | 27.13 | 51.9  |\n| 224  | 40.14 | 51.9  |\n| 224  | 55.46 | 51.9  |\n| 224  | 25.74 | 51.9  |\n| 224  | 27.02 | 51.9  |\n| 224  | 37    | 51.9  |\n\n\nThe P90 value is essentially the amount below which 90% of the values fall. In this table, the P90 \nis constant at 51.9, which means that for location \"224\", 90% of the total amounts are below 51.9.\n\n\n## Common Table Expression\n\nA CTE, short for Common Table Expression, is like a query within a query. With the WITH statement, you can create temporary tables to store results, making complex queries more readable and maintainable. These temporary tables exist only for the duration of the main query.\n\nCTEs and subqueries are both powerful tools and can be used to achieve similar goals, but they have different use cases and advantages. Differences are CTE is reusable during the entire session and more readable\n\nBy declaring CTEs at the beginning of the query, you enhance code readability, enabling a clearer grasp of your analysis logic. \n\n**Syntax:**\n\n```sql\n\nWITH cte_name AS (\n    SELECT column1, column2\n    FROM some_table\n    WHERE condition\n)\nSELECT * FROM cte_name;\n```\n\n**Example: Let's find the trip with the second largest total_amount**\n\n```sql\n\nWITH cte AS(\n\n  SELECT\n  lpep_pickup_datetime,\n  total_amount,\n  RANK() OVER (ORDER BY total_amount DESC) AS rank\n\n  FROM `greentaxi_trips` \n\n)\n\n\nSELECT * FROM cte WHERE rank = 2;\n\n```\n\nThe query starts with a Common Table Expression (CTE) named cte. We use the RANK() window function to \nassign a ranking (rank) to each row based on total_amount in descending order (from highest to lowest).\n\nNow, we use the CTE in the main query: ```SELECT * FROM cte WHERE rank = 2;```\n\nResult of the query:\n\n\n| lpep_pickup_datetime      | total_amount | rank | \n|---------------------------|--------------|-------------------|\n| 2019-10-10 15:22:49 UTC  | 2878.3        | 2             | \n\n\n\n## dbt models and CTEs\n\nCTEs and window functions will be used a lot in module 4 on dbt. Let's see an example of application in dbt models\n\n**Example:**\n\nSuppose we start from the FHV dataset and we want to create a dbt model that enriches the data by calculating the trip duration and the 90th percentile.\n\n```sql\n\nWITH trip_duration_calculated AS (\n\n    SELECT\n        *,\n        timestamp_diff(dropOff_datetime, pickup_datetime, second) as trip_duration\n\n    FROM `fhv_trips`\n)\n\nSELECT \n\n    PUlocationID,\n    trip_duration,\n    PERCENTILE_CONT(trip_duration, 0.90) OVER (PARTITION BY PUlocationID) AS trip_duration_p90\n\n\nFROM trip_duration_calculated\n\n\n```\n\n**Step 1: Understanding the CTE**\n\nThe WITH clause creates a CTE named trip_duration_calculated. This CTE acts as a temporary table that \ncontains all columns from the fhv_trips table. Additionally, it calculates the trip duration for each ride\n\n**Step 2: Main Query using the CTE and Window Function**\n\nThis query computes the 90th percentile of trip duration for each PUlocationID using a window function:\n\nThe PARTITION BY PUlocationID clause ensures that the percentile calculation is performed separately \nfor each unique PUlocationID.\n\nThe percentile 90 means that 90% of the trips have a duration equal to or below this value\n\n**Query result looks like this:**\n\n| PUlocationID | trip_duration | trip_duration_p90 |\n|-------------|---------------|--------------------|\n| 190         | 451           | 2170.0            |\n| 190         | 1373          | 2170.0            |\n| 190         | 817           | 2170.0            |\n| 190         | 589           | 2170.0            |\n| 190         | 1648          | 2170.0            |\n| 32          | 546           | 1988.0            |\n| 32          | 151           | 1988.0            |\n| 32          | 1752          | 1988.0            |\n| 32          | 2426          | 1988.0            |\n| 32          | 888           | 1988.0            |\n\n\n- For PUlocationID = 190, 90% of trips have a duration \u2264 2170.0   seconds.\n- For PUlocationID = 32, 90% of trips have a duration \u2264 1988.0  seconds.",
    "filename": "04-analytics-engineering/SQL_refresher.md"
  },
  {
    "content": "Table of Contents\n=================\n- [How to setup dbt cloud with bigquery](#how-to-setup-dbt-cloud-with-bigquery)\n  * [Create a BigQuery service account](#create-a-bigquery-service-account)\n  * [Create a dbt cloud project](#create-a-dbt-cloud-project)\n  * [Add GitHub repository](#add-github-repository)\n  * [Review your project settings](#review-your-project-settings)\n  * [(Optional) Link to your github account](#optional-link-to-your-github-account)\n\n# How to setup dbt cloud with bigquery\n[Official documentation](https://docs.getdbt.com/tutorial/setting-up)\n\n## Create a BigQuery service account \nIn order to connect we need the service account JSON file generated from bigquery:\n1. Open the [BigQuery credential wizard](https://console.cloud.google.com/apis/credentials/wizard) to create a service account in your taxi project\n\n<table><tr>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152141360-4bc84b53-72f1-4e7c-b42b-7c97fe9aa6ca.png\" style=\"width: 450px;\"/> </td>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152141503-1ad64131-e867-47bf-905e-ee1d7115616c.png\" style=\"width: 450px;\"/> </td>\n</tr></table>\n\n2. You can either grant the specific roles the account will need or simply use bq admin, as you'll be the sole user of both accounts and data. \n\n_Note: if you decide to use specific roles instead of BQ Admin, some users reported that they needed to add also viewer role to avoid encountering denied access errors_\n\n<table><tr>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152141939-9ff88855-7c75-47c9-9088-2bfca0e3c0a3.png\" style=\"width: 450px;\"/> </td>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152142270-5aa8aec7-5cc7-4667-9ecc-721157de83d5.png\" style=\"width: 450px;\"/> </td>\n</tr></table>\n\n\n3. Now that the service account has been created we need to add and download a JSON key, go to the keys section, select \"create new key\". Select key type JSON and once you click on create it will get immediately downloaded for you to use. \n\n<table><tr>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152146423-769bdfee-3846-4296-8dee-d6843081c9b1.png\" style=\"width: 450px;\"/> </td>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152146506-5b3e2e0f-3380-414d-bc20-f35ea3f47726.png\" style=\"width: 450px;\"/> </td>\n</tr></table>\n\n## Create a dbt cloud project \n1. Create a dbt cloud account from [their website](https://www.getdbt.com/pricing/) (free for solo developers)\n2. Once you have logged in into dbt cloud you will be prompt to create a new project \n\nYou are going to need: \n - access to your data warehouse (bigquery - set up in weeks 2 and 3)\n - admin access to your repo, where you will have the dbt project. \n\n _Note: For the sake of showing the creation of a project from scratch I've created a new empty repository just for this week project._ \n\n![image](https://user-images.githubusercontent.com/4315804/152138242-f79bdb71-1fb4-4d8e-83c5-81f7ffc9ccad.png)\n\n3. Name your project\n4. Choose Bigquery as your data warehouse: ![image](https://user-images.githubusercontent.com/4315804/152138772-15950118-b69a-45b1-9c48-9c8a73581a05.png)\n5. Upload the key you downloaded from BQ on the *create from file* option. This will fill out most fields related to the production credentials. Scroll down to the end of the page and set up your development credentials. \n\n_Note: The dataset you'll see under the development credentials is the one you'll use to run and build your models during development. Since BigQuery's default location may not match the one you sued for your source data, it's recommended to create this schema manually to avoid multiregion errors._ \n\n<table><tr>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/153844913-00769b63-3997-42d8-8c1a-1ac5ae451435.png\" style=\"width: 550px;\"/> </td>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152147146-db024d57-d119-4a5b-8e6f-5475664bdf56.png\" style=\"width: 550px;\"/> </td>\n</tr></table>\n\n6. Click on *Test* and after that you can continue with the setup \n\n ## Add GitHub repository \n _Note:_ This step could be skipped by using a managed repository if you don't have your own GitHub repo for the course.\n1. Select git clone and paste the SSH key from your repo. \n \n <table><tr>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152147493-2037bb54-cfed-4843-bef5-5c043fd36ec3.png\" style=\"width: 550px;\"/> </td>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152147547-44ab9d6d-5f3d-41a8-8f73-2d03a568e7aa.png\" style=\"width: 550px;\"/> </td>\n</tr></table>\n\n2. You will get a deploy key, head to your GH repo and go to the settings tab. Under security you'll find the menu *deploy keys*. Click on add key and paste the deploy key provided by dbt cloud. Make sure to tick on \"write access\"\n\n <table><tr>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152147783-264f9da8-ec55-4d07-a9ec-4a8591006ea8.png\" style=\"width: 550px;\"/> </td>\n<td> <img src=\"https://user-images.githubusercontent.com/4315804/152147942-e76ff8b5-986d-4df1-88cc-ed3e98707d62.png\" style=\"width: 550px;\"/> </td>\n</tr></table>\n\n## Review your project settings\nAt the end, if you go to your projects it should look some like this: \n![image](https://user-images.githubusercontent.com/4315804/152606066-f4d70546-7a5e-414a-9df9-8efd090216f8.png)\n\n\n## (Optional) Link to your github account\nYou could simplify the process of adding and creating repositories by linking your GH account. [Official documentation](https://docs.getdbt.com/docs/dbt-cloud/cloud-configuring-dbt-cloud/cloud-installing-the-github-application)",
    "filename": "04-analytics-engineering/dbt_cloud_setup.md"
  },
  {
    "content": "# dbt with BigQuery on Docker\n\nThis is a quick guide on how to setup dbt with BigQuery on Docker.\n\n**Note:** You will need your authentication json key file for this method to work. You can use oauth alternatively.\n\n- Create a directory with the name of your choosing.\n  ```\n  mkdir <dir-name>\n  ```\n- cd into the directory\n  ```\n  cd <dir-name>\n  ```\n- Copy this [Dockerfile](Dockerfile) in your directory borrowed from the official dbt git [here](https://github.com/dbt-labs/dbt-core/blob/main/docker/Dockerfile)\n- Create `docker-compose.yaml` [file](docker-compose.yaml).\n  ```yaml\n  version: '3'\n  services:\n    dbt-bq-dtc:\n      build:\n        context: .\n        target: dbt-bigquery\n      image: dbt/bigquery\n      volumes:\n        - .:/usr/app\n        - ~/.dbt/:/root/.dbt/\n        - ~/.google/credentials/google_credentials.json:/.google/credentials/google_credentials.json\n      network_mode: host\n  ```\n  -   Name the service as you deem right or `dbt-bq-dtc`.\n  -   Use the `Dockerfile` in the current directory to build the image by passing `.` in the context.\n  -   `target` specifies that we want to install the `dbt-bigquery` plugin in addition to `dbt-core`.\n  -  Mount 3 volumes -\n     - for persisting dbt data\n     - path to the dbt `profiles.yml`\n     - path to the `google_credentials.json` file which should be in the `~/.google/credentials/` path\n\n- Create `profiles.yml` file in `~/.dbt/` in your local machine or add the following code in your existing `profiles.yml` - \n  ```yaml\n  bq-dbt-workshop:\n    outputs:\n      dev:\n        dataset: <bigquery-dataset>\n        fixed_retries: 1\n        keyfile: /.google/credentials/google_credentials.json\n        location: EU\n        method: service-account\n        priority: interactive\n        project: <gcp-project-id>\n        threads: 4\n        timeout_seconds: 300\n        type: bigquery\n    target: dev\n  ```\n  - Name the profile. `bq-dbt-workshop` in my case. This will be used in the `dbt_project.yml` file to refer and initiate dbt.\n  - Replace with your `dataset`, `location` (my GCS bucket is in `EU` region, change to `US` if needed), `project` values.\n- Run the following commands -\n  - ```bash \n    docker compose build \n    ```\n  - ```bash \n    docker compose run dbt-bq-dtc init\n    ``` \n    - **Note:** We are essentially running `dbt init` above because the `ENTRYPOINT` in the [Dockerfile](Dockerfile) is `['dbt']`.\n    - Input the required values. Project name will be `taxi_rides_ny`\n    - This should create `dbt/taxi_rides_ny/` and you should see `dbt_project.yml` in there.\n    - In `dbt_project.yml`, replace `profile: 'taxi_rides_ny'` with `profile: 'bq-dbt-workshop'` as we have a profile with the later name in our `profiles.yml`\n  - ```bash\n    docker compose run --workdir=\"//usr/app/dbt/taxi_rides_ny\" dbt-bq-dtc debug\n     ``` \n    - to test your connection. This should output `All checks passed!` in the end.\n    - **Note:** The automatic path conversion in Git Bash will cause the commands to fail with `--workdir` flag. It can be fixed by prefixing the path with `//` as is done above. The solution was found [here](https://github.com/docker/cli/issues/2204#issuecomment-638993192).\n    - Also, we change the working directory to the dbt project because the `dbt_project.yml` file should be in the current directory. Else it will throw `1 check failed: Could not load dbt_project.yml`",
    "filename": "04-analytics-engineering/docker_setup/README.md"
  },
  {
    "content": "Welcome to your new dbt project!\n\n### How to run this project \n### About the project\nThis project is based in [dbt starter project](https://github.com/dbt-labs/dbt-starter-project) (generated by running `dbt init`)\nTry running the following commands:\n- dbt run\n- dbt test\n\nA project includes the following files: \n- dbt_project.yml: file used to configure the dbt project. If you are using dbt locally, make sure the profile here matches the one setup during installation in ~/.dbt/profiles.yml\n- *.yml files under folders models, data, macros: documentation files\n- csv files in the data folder: these will be our sources, files described above\n- Files inside folder models: The sql files contain the scripts to run our models, this will cover staging, core and a datamarts models. At the end, these models will follow this structure: \n\n![image](https://user-images.githubusercontent.com/4315804/152691312-e71b56a4-53ff-4884-859c-c9090dbd0db8.png)\n\n\n#### Workflow\n![image](https://user-images.githubusercontent.com/4315804/148699280-964c4e0b-e685-4c0f-a266-4f3e097156c9.png)\n\n#### Execution\nAfter having installed the required tools and cloning this repo, execute the following commands: \n\n1. Change into the project's directory from the command line: `$ cd [..]/taxi_rides_ny`\n2. Load the CSVs into the database. This materializes the CSVs as tables in your target schema: `$ dbt seed`\n3. Run the models: `$ dbt run`\n4. Test your data: `$ dbt test`\n_Alternative: use `$ dbt build` to execute with one command the 3 steps above together_\n5. Generate documentation for the project: `$ dbt docs generate`\n6. View the documentation for the project, this step should open the documentation page on a webserver, but it can also be accessed from  http://localhost:8080 : `$ dbt docs serve`\n\n### dbt resources:\n- Learn more about dbt [in the docs](https://docs.getdbt.com/docs/introduction)\n- Check out [Discourse](https://discourse.getdbt.com/) for commonly asked questions and answers\n- Join the [chat](http://slack.getdbt.com/) on Slack for live discussions and support\n- Find [dbt events](https://events.getdbt.com) near you\n- Check out [the blog](https://blog.getdbt.com/) for the latest news on dbt's development and best practices",
    "filename": "04-analytics-engineering/taxi_rides_ny/README.md"
  },
  {
    "code": false,
    "content": "# Overview of the SQL Code\n\nThis SQL script is intended for use with Google BigQuery. It creates new data tables for New York taxi trips and modifies their schema to standardize the column names. The script operates in two primary phases: creating tables and modifying their schemas. \n\n## Table Creation\n\n### Green Trip Data\n\n```sql\nCREATE TABLE  `taxi-rides-ny-339813-412521.trips_data_all.green_tripdata` as\nSELECT * FROM `bigquery-public-data.new_york_taxi_trips.tlc_green_trips_2019`;\n```\n\nThe script begins by creating a new table called `green_tripdata`. It populates this table with data sourced from the public dataset `bigquery-public-data.new_york_taxi_trips.tlc_green_trips_2019`, which contains records of green taxi rides in New York from the year 2019.\n\n### Yellow Trip Data\n\n```sql\nCREATE TABLE  `taxi-rides-ny-339813-412521.trips_data_all.yellow_tripdata` as\nSELECT * FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2019`;\n```\n\nNext, a second table named `yellow_tripdata` is created. This table is populated with records from the dataset `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2019`, representing yellow taxi rides from the same year. \n\n### Inserting Additional Records\n\n```sql\ninsert into  `taxi-rides-ny-339813-412521.trips_data_all.green_tripdata` \nSELECT * FROM `bigquery-public-data.new_york_taxi_trips.tlc_green_trips_2020`;\n```\n\nRecords from 2020 are then inserted into the existing `green_tripdata` table. This is done to accumulate data for a subsequent year.\n\n```sql\ninsert into  `taxi-rides-ny-339813-412521.trips_data_all.yellow_tripdata` \nSELECT * FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2020`; \n```\n\nSimilarly, the `yellow_tripdata` table is updated with records from the `tlc_yellow_trips_2020` dataset.\n\n## Schema Modifications\n\nAfter the tables are set up and filled with data from both 2019 and 2020, the script proceeds to modify the column names in both tables for consistency and clarity.\n\n### Altering Yellow Trip Data Schema\n\n```sql\nALTER TABLE `taxi-rides-ny-339813-412521.trips_data_all.yellow_tripdata`\n  RENAME COLUMN vendor_id TO VendorID;\n```\n\nThe first alteration renames the `vendor_id` column in the `yellow_tripdata` table to `VendorID`, likely to enhance readability and maintain a consistent naming convention throughout the database.\n\nAdditional `ALTER TABLE` statements continue to rename several more columns in the `yellow_tripdata` table:\n\n- `pickup_datetime` to `tpep_pickup_datetime`\n- `dropoff_datetime` to `tpep_dropoff_datetime`\n- `rate_code` to `RatecodeID`\n- `imp_surcharge` to `improvement_surcharge`\n- `pickup_location_id` to `PULocationID`\n- `dropoff_location_id` to `DOLocationID`\n\n### Altering Green Trip Data Schema\n\nThe same renaming process is applied to the `green_tripdata` table with similar alterations:\n\n```sql\nALTER TABLE `taxi-rides-ny-339813-412521.trips_data_all.green_tripdata`\n  RENAME COLUMN vendor_id TO VendorID;\n```\n\nSubsequent changes include:\n\n- `pickup_datetime` to `lpep_pickup_datetime`\n- `dropoff_datetime` to `lpep_dropoff_datetime`\n- `rate_code` to `RatecodeID`\n- `imp_surcharge` to `improvement_surcharge`\n- `pickup_location_id` to `PULocationID`\n- `dropoff_location_id` to `DOLocationID`\n\nThis step ensures that both `green_tripdata` and `yellow_tripdata` tables have a similar schema structure, making it easier to analyze data across both datasets.\n\n## Important Notes\n\n- **Replacement Placeholder**: The comments at the start emphasize replacing the dataset name `taxi-rides-ny-339813-412521` with one\u2019s own dataset name before executing the script.\n  \n- **Execution Limitation**: The comment advises limiting executed `ALTER TABLE` statements to five at a time. This is necessary to comply with BigQuery's constraints on altering table structures to avoid errors.\n\n## Summary\n\nIn summary, this SQL script effectively creates two tables for tracking New York taxi trips (green and yellow), inserts additional data from a subsequent year, and standardizes their schemas by renaming various columns. This structured approach enables easier data processing and analysis of taxi ride statistics across both types of trips.",
    "filename": "04-analytics-engineering/taxi_rides_ny/analyses/hack-load-data.sql"
  },
  {
    "code": false,
    "content": "# Documentation for `get_payment_type_description` Macro\n\n## Overview\n\nThe `get_payment_type_description` macro is a reusable component designed to map payment type identifiers to their respective descriptive names. This macro is particularly useful in scenarios where there are various payment methods tracked in a database and human-readable descriptions are required for interpretation or reporting purposes.\n\n## Purpose\n\nThe primary purpose of the macro is to convert a numerical representation of payment types into a descriptive string. This can enhance the clarity of reports, logs, or any output that needs to present user-friendly information instead of raw numerical data.\n\n## Definition\n\nThe macro is defined using Jinja syntax, which is often utilized in templating for web applications and data transformation tools like dbt (Data Build Tool). The macro receives a single parameter:\n\n- `payment_type`: An integer that represents the payment method.\n\n## Logic Flow\n\nUpon receiving the `payment_type`, the macro utilizes a `CASE` statement to interpret the value. The logical flow is described below:\n\n1. **Case Statement**: The macro uses a `CASE` statement to evaluate the `payment_type`.\n2. **Matching Values**:\n    - When `1`, it returns `'Credit card'`.\n    - When `2`, it returns `'Cash'`.\n    - When `3`, it returns `'No charge'`.\n    - When `4`, it returns `'Dispute'`.\n    - When `5`, it returns `'Unknown'`.\n    - When `6`, it returns `'Voided trip'`.\n3. **Default Case**: If none of the specified values match, it defaults to returning `'EMPTY'`, indicating that the input does not correspond to any known payment type.\n\n## Use Cases\n\nThis macro can be applied in various contexts where payment type data needs to be processed:\n\n- **Reporting**: When generating reports that provide insights into payment trends, the macro can substitute numeric values with descriptive strings.\n- **Data Cleaning**: It assists in transforming raw data for better readability and understanding during ETL (Extract, Transform, Load) operations.\n- **User Interfaces**: In applications, it can be used to ensure that users see descriptive labels rather than numeric codes when browsing payment histories.\n\n## Integration\n\nTo use this macro in a project, it would typically be called within a SQL statement or another macro. Here is an example of how it might be incorporated:\n\n```sql\nSELECT \n    {{ get_payment_type_description(payment_type) }} AS payment_description\nFROM payments\n```\n\n## Casting\n\nThe macro utilizes `dbt.safe_cast` to safely handle the type conversion for `payment_type`, ensuring that the interpretation aligns with the expected data type. Using `api.Column.translate_type(\"integer\")` aids in translating the column type correctly, which is critical for avoiding runtime errors related to data types.\n\n## Conclusion\n\nThe `get_payment_type_description` macro is a straightforward yet powerful tool for providing clarity to payment type data. By converting numeric codes to descriptive text, it improves data usability across reporting and analytics tasks. This macro exemplifies how encapsulating logic within reusable components can facilitate more efficient data processing workflows in data modeling projects.",
    "filename": "04-analytics-engineering/taxi_rides_ny/macros/get_payment_type_description.sql"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis SQL code snippet is designed for a data transformation workflow, typically used in a data pipeline or a data warehouse context. It uses a syntax consistent with a tool like dbt (data build tool), specifically for materialization of the results into a physical table. The code is structured to perform a selection of relevant fields from a reference table known as `taxi_zone_lookup`.\n\n## Materialization\n\n```sql\n{{ config(materialized='table') }}\n```\n\nThe first line of the code sets the configuration for the transformation by specifying its materialization type. Here, `materialized='table'` indicates that the result of this query will be stored as a physical table in the database. This means that after the query is executed, a new table will be created (or updated) with the results, allowing for more efficient querying in the future, as opposed to re-running the SQL on-demand to generate results.\n\n## Data Selection\n\n```sql\nselect \n    locationid, \n    borough, \n    zone, \n    replace(service_zone,'Boro','Green') as service_zone \nfrom {{ ref('taxi_zone_lookup') }}\n```\n\n### Fields Selected\n\n- **locationid**: This field likely serves as a unique identifier for each location within the dataset. It is fundamental for identifying and retrieving specific entries in the context of taxi zones.\n\n- **borough**: This field categorizes the locations based on their respective boroughs. It provides geographical context and could be useful for analysis related to service delivery, demographics, or urban planning.\n\n- **zone**: The zone field might represent specific areas within each borough that are significant for taxi operations. Each zone could define taxi service areas for better operational efficiencies.\n\n- **service_zone**: The `service_zone` field originally contains values like 'Boro', which are to be replaced by 'Green'. This transformation likely refines or harmonizes the service zone designation in the dataset for clarity, helping to standardize data representation.\n\n## Data Transformation\n\n```sql\nreplace(service_zone,'Boro','Green') as service_zone\n```\n\nThe `replace` function is employed to modify the `service_zone` field. By replacing occurrences of 'Boro' with 'Green', the transformation acts to standardize the service zones in such a way that it may highlight or redefine service areas in a more meaningful way for further analysis or reporting. \n\nThis transformation is critical as it ensures consistency in data representation and could be essential for downstream analytics or reporting purposes where the specific naming convention of service zones matters.\n\n## Source Reference\n\n```sql\nfrom {{ ref('taxi_zone_lookup') }}\n```\n\nThe `from` clause utilizes the `ref` function to refer to a model called `taxi_zone_lookup`. Using `{{ ref('model_name') }}` is a dbt convention that ensures the code is properly linked to other models within the project. This method allows for efficient dependency management, ensuring that the referenced table is built before this code is executed if necessary. Thus, it promotes modular code development and better maintainability within the data pipeline.\n\n## Summary\n\nIn summary, this SQL snippet extracts crucial fields from the `taxi_zone_lookup` table while ensuring consistent naming conventions in the resulting dataset. By materializing the transformation as a table, it facilitates quicker access for future analytical queries. The overall goal seems to be preparing a refined dataset of taxi service zones that can be used for further analysis, reporting, or integration into broader data products or dashboards.\n\nThis approach exemplifies best practices in data engineering, focusing on clarity, maintainability, and performance-enhancement when dealing with data in a warehouse environment.",
    "filename": "04-analytics-engineering/taxi_rides_ny/models/core/dim_zones.sql"
  },
  {
    "code": false,
    "content": "# Documentation of the DBT Model: Monthly Revenue Analysis for Taxi Trips\n\nThis documentation provides a high-level description of a DBT (Data Build Tool) model that calculates monthly revenue metrics from taxi trip data. The model transforms raw trip data into a structured table for further analysis and reporting.\n\n## Overview\n\nThe model is designed to aggregate taxi trip data to analyze revenue trends by month and pickup zone. It retrieves data from a reference table, performs calculations on various revenue streams, and groups the results by revenue zone, revenue month, and service type.\n\n## Configuration\n\nThe model is configured to be materialized as a table by the following line:\n\n```sql\n{{ config(materialized='table') }}\n```\n\nThis setting ensures that the output of this model is saved as a standalone table in the database, allowing for efficient querying of the aggregated results.\n\n## Data Source\n\nA Common Table Expression (CTE) named `trips_data` is defined to pull data from the reference table `fact_trips`. The base data comprises a wide range of fields related to taxi trips, including fare amounts, service types, pickup zones, and other relevant metrics.\n\n```sql\nwith trips_data as (\n    select * from {{ ref('fact_trips') }}\n)\n```\n\n## Revenue Metrics Calculation\n\nThe primary calculations in this model involve aggregating various revenue components for each revenue zone and month. The metrics calculated include:\n\n- **Revenue Grouping**:\n  - `pickup_zone` as `revenue_zone`: Represents the geographical area where passengers were picked up.\n  - `{{ dbt.date_trunc(\"month\", \"pickup_datetime\") }}` as `revenue_month`: Extracts the month from the pickup datetime to facilitate monthly aggregation.\n  - `service_type`: Specifies the type of service (e.g., standard ride, shared ride).\n\n- **Revenue Calculation**:\n  The following metrics are calculated as sums of individual components:\n  - `revenue_monthly_fare`: Total fare amounts for the month.\n  - `revenue_monthly_extra`: Additional charges beyond the basic fare.\n  - `revenue_monthly_mta_tax`: Monthly Metropolitan Transportation Authority tax totaled.\n  - `revenue_monthly_tip_amount`: Total tips provided by passengers during the month.\n  - `revenue_monthly_tolls_amount`: Total tolls charged on trips within the month.\n  - `revenue_monthly_ehail_fee`: Fees associated with electronic hailing for services.\n  - `revenue_monthly_improvement_surcharge`: Any additional surcharges that contribute to revenue.\n  - `revenue_monthly_total_amount`: The total amount collected from all charges for the month.\n\n## Additional Trip Metrics Calculation\n\nIn addition to revenue calculations, the model also computes the following metrics:\n\n- **Total Monthly Trips**:\n  - `total_monthly_trips`: The count of individual trips taken within the month, which helps indicate demand.\n\n- **Average Metrics**:\n  - `avg_monthly_passenger_count`: Average number of passengers per trip for the month.\n  - `avg_monthly_trip_distance`: Average distance traveled per trip, providing insight into trip patterns and efficiency.\n\n## Grouping the Results\n\nThe final aggregated results are grouped by three dimensions:\n\n```sql\ngroup by 1,2,3\n```\n\nThis means that the resulting data will be organized and calculated at the intersection of each unique combination of `revenue_zone`, `revenue_month`, and `service_type`. This multi-dimensional grouping provides a detailed view of the revenue performance across different areas and over time.\n\n## Summary of Output\n\nThe output of this DBT model will result in a table that contains the following columns:\n\n- `revenue_zone`\n- `revenue_month`\n- `service_type`\n- `revenue_monthly_fare`\n- `revenue_monthly_extra`\n- `revenue_monthly_mta_tax`\n- `revenue_monthly_tip_amount`\n- `revenue_monthly_tolls_amount`\n- `revenue_monthly_ehail_fee`\n- `revenue_monthly_improvement_surcharge`\n- `revenue_monthly_total_amount`\n- `total_monthly_trips`\n- `avg_monthly_passenger_count`\n- `avg_monthly_trip_distance`\n\nThis structured output allows for rich analytics and will enable stakeholders to gain insights into revenue trends, operational efficiency, and customer behavior across different service types and zones over time. \n\n## Conclusion\n\nIn conclusion, this DBT model serves as a fundamental analysis tool for understanding taxi trip revenues on a monthly basis. By aggregating and summarizing the trip data, it provides analysts and decision-makers with crucial information that can be utilized for improving service offerings and financial forecasting.",
    "filename": "04-analytics-engineering/taxi_rides_ny/models/core/dm_monthly_zone_revenue.sql"
  },
  {
    "code": false,
    "content": "# Trip Data Union and Enrichment\n\nThis script is designed to process and combine trip data from two different service types (`Green` and `Yellow`) into a unified dataset. It also enhances this dataset by incorporating geographical information about the pickup and dropoff locations through dimensional zone data.\n\n## Configuration\n\n```sql\n{{\n    config(\n        materialized='table'\n    )\n}}\n```\n\nThe script starts by configuring the output data to be materialized as a table. This means the final results of this query will be stored as a permanent table in the database, making them available for subsequent queries or analyses.\n\n## Data Ingestion\n\n### Green Trip Data\n\n```sql\nwith green_tripdata as (\n    select *, \n        'Green' as service_type\n    from {{ ref('stg_green_tripdata') }}\n)\n```\n\nThe first Common Table Expression (CTE) named `green_tripdata` retrieves records from the staging table for green trip data (`stg_green_tripdata`). It adds a new column `service_type`, marking all records as belonging to the `Green` service.\n\n### Yellow Trip Data\n\n```sql\nyellow_tripdata as (\n    select *, \n        'Yellow' as service_type\n    from {{ ref('stg_yellow_tripdata') }}\n)\n```\n\nSimilarly, the second CTE, `yellow_tripdata`, fetches records from the staging table for yellow trip data (`stg_yellow_tripdata`). Like the previous CTE, it adds a `service_type` column specifying this data belongs to the `Yellow` service.\n\n## Combining Trip Data\n\n```sql\ntrips_unioned as (\n    select * from green_tripdata\n    union all \n    select * from yellow_tripdata\n)\n```\n\nThe third CTE, `trips_unioned`, combines the data from both the `green_tripdata` and `yellow_tripdata` CTEs using the `UNION ALL` operator. This operation merges the two datasets into a single table while retaining all records, regardless of duplicates.\n\n## Filtering Dimensional Data\n\n```sql\ndim_zones as (\n    select * from {{ ref('dim_zones') }}\n    where borough != 'Unknown'\n)\n```\n\nNext, the `dim_zones` CTE retrieves zone data from the `dim_zones` table, filtering out any records where the borough is marked as `Unknown`. This step ensures only valid geographical data is used for enriching the trip records.\n\n## Final Data Selection and Join\n\n```sql\nselect trips_unioned.tripid, \n    trips_unioned.vendorid, \n    ...\nfrom trips_unioned\ninner join dim_zones as pickup_zone\non trips_unioned.pickup_locationid = pickup_zone.locationid\ninner join dim_zones as dropoff_zone\non trips_unioned.dropoff_locationid = dropoff_zone.locationid\n```\n\nThe final output selects various attributes from the `trips_unioned` dataset, including trip IDs, vendor IDs, service types, location IDs, timestamps, and financial information related to the trips. It also performs inner joins with the `dim_zones` CTE twice: once to link the pickup location IDs to their respective borough and zone, and again for the dropoff location IDs.\n\n## Output Columns\n\nThe final output includes the following important columns:\n- **Trip Identifiers**: `tripid`, `vendorid`, and `service_type`\n- **Location Data**: `pickup_locationid`, `pickup_borough`, `pickup_zone`, and `dropoff_locationid`, `dropoff_borough`, `dropoff_zone`\n- **Datetime Information**: `pickup_datetime` and `dropoff_datetime`\n- **Trip Attributes**: `passenger_count`, `trip_distance`, `trip_type`, and various financial metrics including fares, tips, and total amount.\n\n## Conclusion\n\nThis SQL script effectively consolidates and enriches trip data from two distinct services, transforming it into a structured format that includes comprehensive details about each trip along with the geographical context of pickup and dropoff locations. The materialization as a table allows for efficient subsequent querying and analysis. This data could be particularly useful for transportation analysis, business intelligence, or operational reporting purposes.",
    "filename": "04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql"
  },
  {
    "code": false,
    "content": "# Overview\n\nThis SQL script is part of a data transformation process using dbt (data build tool), which focuses on preparing taxi trip data for analysis. The goal of the script is to create a view containing structured and cleaned trip data by transforming the raw data from the staging area. The script achieves this through various SQL functions and casting techniques.\n\n## Configuration\n\nThe script begins with a configuration block that specifies the materialization of the resulting data model. Here, the materialization is set to `'view'`, indicating that the output will be a view in the database. Views allow for simpler querying and can represent dynamic data sources.\n\n```sql\n{{\n    config(\n        materialized='view'\n    )\n}}\n```\n\n## Data Preparation\n\nNext, a Common Table Expression (CTE) named `tripdata` is created. This CTE selects all columns from a source table (`green_tripdata`) located in the `staging` schema. It adds a computed column (`rn`), which uses the `row_number()` function to assign a unique row number to each record, partitioning the data by `vendorid` and `lpep_pickup_datetime`. This step essentially allows the script to handle potential duplicate records efficiently.\n\n```sql\nwith tripdata as \n(\n  select *,\n    row_number() over(partition by vendorid, lpep_pickup_datetime) as rn\n  from {{ source('staging','green_tripdata') }}\n  where vendorid is not null \n)\n```\n\n## Selection of Key Fields\n\nFollowing the preparation of the `tripdata` CTE, the script selects various fields from it. The objective is to create a structured output that retains only necessary pieces of information relevant for analysis. \n\n### Identifiers\n\nThe query generates a surrogate key `tripid` by using the `dbt_utils.generate_surrogate_key` function, which helps in uniquely identifying each trip. Additional identifiers such as `vendorid`, `ratecodeid`, `pulocationid`, and `dolocationid` are safely cast to integers.\n\n```sql\nselect\n    {{ dbt_utils.generate_surrogate_key(['vendorid', 'lpep_pickup_datetime']) }} as tripid,\n    {{ dbt.safe_cast(\"vendorid\", api.Column.translate_type(\"integer\")) }} as vendorid,\n    {{ dbt.safe_cast(\"ratecodeid\", api.Column.translate_type(\"integer\")) }} as ratecodeid,\n    {{ dbt.safe_cast(\"pulocationid\", api.Column.translate_type(\"integer\")) }} as pickup_locationid,\n    {{ dbt.safe_cast(\"dolocationid\", api.Column.translate_type(\"integer\")) }} as dropoff_locationid,\n```\n\n### Timestamps\n\nThe script also transforms timestamp fields for when the trip starts and ends. This is done by explicitly casting the fields `lpep_pickup_datetime` and `lpep_dropoff_datetime` as timestamps. \n\n```sql\n    cast(lpep_pickup_datetime as timestamp) as pickup_datetime,\n    cast(lpep_dropoff_datetime as timestamp) as dropoff_datetime,\n```\n\n### Trip Information\n\nVarious trip-related fields are selected or cast to the appropriate data types. Fields such as `store_and_fwd_flag`, `passenger_count`, and `trip_distance` are included for comprehensive trip metrics. The `trip_type` is cast to an integer as well.\n\n```sql\n    store_and_fwd_flag,\n    {{ dbt.safe_cast(\"passenger_count\", api.Column.translate_type(\"integer\")) }} as passenger_count,\n    cast(trip_distance as numeric) as trip_distance,\n    {{ dbt.safe_cast(\"trip_type\", api.Column.translate_type(\"integer\")) }} as trip_type,\n```\n\n### Payment Information\n\nIn this section, the script processes various payment-related fields. These values, such as `fare_amount`, `tip_amount`, `tolls_amount`, and `total_amount`, are cast to numeric types for easier manipulation and analysis in subsequent queries. The payment type is also cast and is supplemented with a description obtained through a `get_payment_type_description` function.\n\n```sql\n    cast(fare_amount as numeric) as fare_amount,\n    cast(extra as numeric) as extra,\n    cast(mta_tax as numeric) as mta_tax,\n    cast(tip_amount as numeric) as tip_amount,\n    cast(tolls_amount as numeric) as tolls_amount,\n    cast(ehail_fee as numeric) as ehail_fee,\n    cast(improvement_surcharge as numeric) as improvement_surcharge,\n    cast(total_amount as numeric) as total_amount,\n    coalesce({{ dbt.safe_cast(\"payment_type\", api.Column.translate_type(\"integer\")) }},0) as payment_type,\n    {{ get_payment_type_description(\"payment_type\") }} as payment_type_description\n```\n\n## Filtering and Execution Control\n\nAfter the main data selection, a `WHERE` clause is used to filter the records to include only those with `rn = 1`, effectively removing duplicate records by keeping only the first occurrence of each trip for every unique combination of `vendorid` and `lpep_pickup_datetime`.\n\n```sql\nfrom tripdata\nwhere rn = 1\n```\n\nAdditionally, the script contains a conditional block to manage test runs. When the variable `is_test_run` evaluates to true, it limits the output to a maximum of 100 records. This is useful for debugging or verifying the model without processing large volumes of data.\n\n```sql\n{% if var('is_test_run', default=true) %}\n  limit 100\n{% endif %}\n```\n\n## Conclusion\n\nIn summary, this SQL script is designed to cleanse and structure motor vehicle trip data for optimized analysis. By employing CTEs, data type casting, and selective field extraction, it prepares a comprehensive view while ensuring quality through filtering duplicate records and offering a mechanism for controlled test runs. The created view can subsequently serve as a reliable source for reporting or further analytics.",
    "filename": "04-analytics-engineering/taxi_rides_ny/models/staging/stg_green_tripdata.sql"
  },
  {
    "code": false,
    "content": "# Overview\n\nThis script is a SQL-based Data Build Tool (dbt) model intended to transform yellow taxi trip data into a more analyzed and structured view. By processing the raw data from a source table, it aggregates, filters, and formats the information relevant to taxi trips, generating a view that can be reused for reporting and analysis.\n\n## Configuration\n\nThe model is set to materialize as a view, which means that the resulting dataset will be generated and stored but not actually duplicated in the database. This materialization provides a dynamic result when queried, always reflecting the latest data available in the source.\n\n```sql\n{{ config(materialized='view') }}\n```\n\n## Data Preparation\n\nThe initial data preparation is encapsulated within a Common Table Expression (CTE) named `tripdata`. This CTE uses a `ROW_NUMBER()` window function to assign a sequential number to each row of trip data based on the `vendorid` and `tpep_pickup_datetime`. This is crucial for ensuring that the subsequent selection operates on unique trips and avoids redundant entries.\n\n```sql\nwith tripdata as \n(\n  select *,\n    row_number() over(partition by vendorid, tpep_pickup_datetime) as rn\n  from {{ source('staging','yellow_tripdata') }}\n  where vendorid is not null \n)\n```\n\nThis preparation also includes filtering out any records where `vendorid` is null, which ensures that only valid vendor entries are retained.\n\n## Selecting Trip Data\n\nThe main selection pulls various fields from the `tripdata` CTE, including identifiers, timestamps, trip information, and payment-related data. It uses utility functions provided by dbt to ensure proper data types and transformations.\n\n```sql\nselect\n   {{ dbt_utils.generate_surrogate_key(['vendorid', 'tpep_pickup_datetime']) }} as tripid,    \n   {{ dbt.safe_cast(\"vendorid\", api.Column.translate_type(\"integer\")) }} as vendorid,\n   ...\nfrom tripdata\nwhere rn = 1\n```\n\n### Identifiers\n\n1. **Trip ID**: A surrogate key is generated combining `vendorid` and `tpep_pickup_datetime` to uniquely identify each trip.\n2. **Vendor ID, Rate Code ID, Location IDs**: These identifiers are cast to integer types to ensure proper formatting for analysis.\n\n### Timestamps\n\nThe script casts trip pickup and dropoff timestamps to the `timestamp` type, enabling time-based analysis of trip durations and patterns:\n\n- **Pickup datetime**: The time at which the trip began.\n- **Dropoff datetime**: The time at which the trip ended.\n\n### Trip Information\n\nRelevant fields related to the trip are selected, including:\n\n- **Passenger Count**: Number of passengers in the trip.\n- **Trip Distance**: Total distance traveled during the trip, cast to a numeric type.\n- **Trip Type**: Set to 1, indicating that all entries represent street-hail taxis.\n\n### Payment Information\n\nPayment-related fields are included to allow for financial analysis of the trips:\n\n- **Fare Amount, Extra Charges, MTA Tax, Tip Amount, Tolls Amount**: All these fields are cast to numeric to support arithmetic operations.\n- **E-Hail Fee and Improvement Surcharge**: Static values initialized to zero and included in the captured financial data.\n- **Total Amount**: The cumulative total for the trip costs.\n- **Payment Type**: Safely cast to an integer, with a default of 0 for null values.\n- **Payment Type Description**: A description of the payment type obtained through a custom function.\n\n## Limiting Results for Testing\n\nAt the end of the query, there\u2019s a condition that limits the output to only 100 records if the variable `is_test_run` is set to true. This is useful for testing purposes, allowing developers to validate their logic without processing the entire dataset:\n\n```sql\n{% if var('is_test_run', default=true) %}\n  limit 100\n{% endif %}\n```\n\n## Conclusion\n\nIn summary, this dbt model effectively provides a transformed, structured view of yellow taxi trip data, making it suitable for analytical purposes. By leveraging window functions, casting to proper data types, and utility functions that enhance data integrity, it prepares the dataset for further analysis in a clear and concise manner. This model also prioritizes efficient performance during development through the conditional limit on result sets.",
    "filename": "04-analytics-engineering/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql"
  },
  {
    "content": "# Module 5: Batch Processing\n\n## 5.1 Introduction\n\n* :movie_camera: 5.1.1 Introduction to Batch Processing\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/dcHe5Fl3MF8)](https://youtu.be/dcHe5Fl3MF8&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=51)\n\n* :movie_camera: 5.1.2 Introduction to Spark\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/FhaqbEOuQ8U)](https://youtu.be/FhaqbEOuQ8U&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=52)\n\n\n## 5.2 Installation\n\nFollow [these instructions](setup/) to install Spark:\n\n* [Windows](setup/windows.md)\n* [Linux](setup/linux.md)\n* [MacOS](setup/macos.md)\n\nAnd follow [this](setup/pyspark.md) to run PySpark in Jupyter\n\n* :movie_camera: 5.2.1 (Optional) Installing Spark (Linux)\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/hqUbB9c8sKg)](https://youtu.be/hqUbB9c8sKg&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=53)\n\nAlternatively, if the setups above don't work, you can run Spark in Google Colab.\n> [!NOTE]  \n> It's advisable to invest some time in setting things up locally rather than immediately jumping into this solution\n\n* [Google Colab Instructions](https://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304)\n* [Google Colab Starter Notebook](https://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb)\n\n\n## 5.3 Spark SQL and DataFrames\n\n* :movie_camera: 5.3.1 First Look at Spark/PySpark\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/r_Sf6fCB40c)](https://youtu.be/r_Sf6fCB40c&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=54)\n\n* :movie_camera: 5.3.2 Spark Dataframes\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/ti3aC1m3rE8)](https://youtu.be/ti3aC1m3rE8&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=55)\n\n* :movie_camera: 5.3.3 (Optional) Preparing Yellow and Green Taxi Data\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/CI3P4tAtru4)](https://youtu.be/CI3P4tAtru4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=56)\n\nScript to prepare the Dataset [download_data.sh](code/download_data.sh)\n\n> [!NOTE]  \n> The other way to infer the schema (apart from pandas) for the csv files, is to set the `inferSchema` option to `true` while reading the files in Spark.\n\n* :movie_camera: 5.3.4 SQL with Spark\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/uAlp2VuZZPY)](https://youtu.be/uAlp2VuZZPY&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=57)\n\n\n## 5.4 Spark Internals\n\n* :movie_camera: 5.4.1 Anatomy of a Spark Cluster\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/68CipcZt7ZA)](https://youtu.be/68CipcZt7ZA&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=58)\n\n* :movie_camera: 5.4.2 GroupBy in Spark\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/9qrDsY_2COo)](https://youtu.be/9qrDsY_2COo&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=59)\n\n* :movie_camera: 5.4.3 Joins in Spark\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/lu7TrqAWuH4)](https://youtu.be/lu7TrqAWuH4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=60)\n\n## 5.5 (Optional) Resilient Distributed Datasets\n\n* :movie_camera: 5.5.1 Operations on Spark RDDs\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/Bdu-xIrF3OM)](https://youtu.be/Bdu-xIrF3OM&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=61)\n\n* :movie_camera: 5.5.2 Spark RDD mapPartition\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/k3uB2K99roI)](https://youtu.be/k3uB2K99roI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=62)\n\n\n## 5.6 Running Spark in the Cloud\n\n* :movie_camera: 5.6.1 Connecting to Google Cloud Storage\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/Yyz293hBVcQ)](https://youtu.be/Yyz293hBVcQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=63)\n\n* :movie_camera: 5.6.2 Creating a Local Spark Cluster\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/HXBwSlXo5IA)](https://youtu.be/HXBwSlXo5IA&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=64)\n\n* :movie_camera: 5.6.3 Setting up a Dataproc Cluster\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/osAiAYahvh8)](https://youtu.be/osAiAYahvh8&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=65)\n\n* :movie_camera: 5.6.4 Connecting Spark to Big Query\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/HIm2BOj8C0Q)](https://youtu.be/HIm2BOj8C0Q&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=66)\n\n\n# Homework\n\n* [2025 Homework](../cohorts/2025/05-batch/homework.md)\n\n\n# Community notes\n\nDid you take notes? You can share them here.\n\n* [Notes by Alvaro Navas](https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/5_batch_processing.md)\n* [Sandy's DE Learning Blog](https://learningdataengineering540969211.wordpress.com/2022/02/24/week-5-de-zoomcamp-5-2-1-installing-spark-on-linux/)\n* [Notes by Alain Boisvert](https://github.com/boisalai/de-zoomcamp-2023/blob/main/week5.md)\n* [Alternative : Using docker-compose to launch spark by rafik](https://gist.github.com/rafik-rahoui/f98df941c4ccced9c46e9ccbdef63a03) \n* [Marcos Torregrosa's blog (spanish)](https://www.n4gash.com/2023/data-engineering-zoomcamp-semana-5-batch-spark)\n* [Notes by Victor Padilha](https://github.com/padilha/de-zoomcamp/tree/master/week5)\n* [Notes by Oscar Garcia](https://github.com/ozkary/Data-Engineering-Bootcamp/tree/main/Step5-Batch-Processing)\n* [Notes by HongWei](https://github.com/hwchua0209/data-engineering-zoomcamp-submission/blob/main/05-batch-processing/README.md)\n* [2024 videos transcript](https://drive.google.com/drive/folders/1XMmP4H5AMm1qCfMFxc_hqaPGw31KIVcb?usp=drive_link) by Maria Fisher \n* [2025 Notes by Manuel Guerra](https://github.com/ManuelGuerra1987/data-engineering-zoomcamp-notes/blob/main/5_Batch-Processing-Spark/README.md)\n* [2025 Notes by Gabi Fonseca](https://github.com/fonsecagabriella/data_engineering/blob/main/05_batch_processing/00_notes.md)\n* [2025 Notes on Installing Spark on MacOS (with Anaconda + brew) by Gabi Fonseca](https://github.com/fonsecagabriella/data_engineering/blob/main/05_batch_processing/01_env_setup.md)\n* [2025 Notes by Daniel Lachner](https://github.com/mossdet/dlp_data_eng/blob/main/Notes/05_01_Batch_Processing_Spark_GCP.pdf)\n* Add your notes here (above this line)",
    "filename": "05-batch/README.md"
  },
  {
    "code": false,
    "content": "# Setting Up PySpark Environment\n\nThis section outlines the process of setting up a PySpark environment, importing necessary modules, and initializing a Spark session. It also demonstrates how to download a dataset for analysis and the subsequent steps to read, display, and save the data.\n\n## Importing the PySpark Library\n\n```python\nimport pyspark\n```\n\nIn this code block, the `pyspark` library is imported, which is essential for working with Apache Spark in a Python environment. PySpark allows users to leverage Spark's distributed computing capabilities directly from Python.\n\n## Verifying PySpark Installation\n\n```python\npyspark.__file__\n```\n\nThis command checks and retrieves the file path of the installed PySpark library. It can be helpful for confirming that PySpark has been set up correctly in your Python environment.\n\n## Creating a Spark Session\n\n```python\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n```\n\nHere, the `SparkSession` class is imported from `pyspark.sql`, which serves as the entry point for working with DataFrames and executing Spark SQL queries. The session is initialized with the following configurations:\n- `.master(\"local[*]\")`: This indicates that Spark should run locally with as many worker threads as there are logical cores on the machine. \n- `.appName('test')`: This sets the name of the Spark application; it can be useful for monitoring and debugging.\n- `.getOrCreate()`: This method either retrieves an existing SparkSession or creates a new one.\n\n## Downloading a Dataset\n\n```python\n!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n```\n\nThis code uses the `wget` command to download a CSV file containing taxi zone lookup information from a public URL. Using the command shell via the exclamation mark (`!`) allows executing shell commands within a Jupyter notebook.\n\n## Inspecting the Downloaded File\n\n```python\n!head taxi+_zone_lookup.csv\n```\n\nThe `head` command is executed to preview the contents of the downloaded CSV file. This command displays the first few lines of the file, providing an overview of the data structure and the information it contains.\n\n## Reading the CSV into a DataFrame\n\n```python\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .csv('taxi+_zone_lookup.csv')\n```\n\nIn this block, the CSV file is read into a Spark DataFrame. The `option(\"header\", \"true\")` specifies that the first row of the CSV file contains column headers, enabling Spark to correctly interpret the data structure. The resulting DataFrame `df` will hold the taxi zone data for subsequent analysis.\n\n## Displaying the DataFrame\n\n```python\ndf.show()\n```\n\nThe `show()` method is called on the DataFrame `df`, which displays the contents of the DataFrame. By default, it shows the first 20 rows, making it easy to visualize the data and verify that it was loaded correctly.\n\n## Writing the DataFrame to Parquet Format\n\n```python\ndf.write.parquet('zones')\n```\n\nThis command saves the DataFrame `df` to disk in Parquet format, which is a columnar storage format that provides efficient data compression and encoding. The dataset will be saved in a directory named \"zones,\" facilitating rapid access and analysis in future operations.\n\n## Listing Files in the Current Directory\n\n```python\n!ls -lh\n```\n\nFinally, the `ls -lh` command is executed to list all files in the current directory in a human-readable format, showing file sizes and other attributes. This helps confirm that the \"zones\" directory has been created and displays available data files, ensuring that the previous operations were successful.",
    "filename": "05-batch/code/03_test.ipynb"
  },
  {
    "code": false,
    "content": "# Spark Data Processing with PySpark\n\nThis document explains the process of loading, processing, and analyzing data using PySpark, a powerful tool for big data processing. The data used in this example concerns taxi trips in New York City.\n\n## Setting Up PySpark\n\nTo begin, we need to import the required libraries and initialize a Spark session. The Spark session acts as an entry point for working with Spark.\n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n```\n\nHere, we import `pyspark` and create a `SparkSession` named 'test'. The `master(\"local[*]\")` configuration indicates that Spark will run locally using as many threads as available.\n\n## Downloading the Dataset\n\nNext, we will download the required dataset, which contains records of New York City taxi trips. The data is stored in a compressed CSV format.\n\n```python\n!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\n```\n\nUsing the `wget` command, we fetch the compressed CSV file from the specified URL.\n\n## Unpacking and Inspecting the Data\n\nOnce we have the data, it's essential to unpack the compressed file and inspect the number of lines it contains to understand its size.\n\n```python\n!gzip -dc fhvhv_tripdata_2021-01.csv.gz\n```\n\nThis command decompresses the CSV file for us to view the contents.\n\n```python\n!wc -l fhvhv_tripdata_2021-01.csv\n```\n\nThe `wc -l` command counts the lines in the CSV file, giving us an estimate of the dataset size.\n\n## Loading Data into Spark\n\nNow we can load the CSV file into a Spark DataFrame. We use the `read.csv()` method with an option to treat the first row as headers.\n\n```python\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .csv('fhvhv_tripdata_2021-01.csv')\n```\n\nThis code reads the CSV file and creates a Spark DataFrame named `df`, which allows for powerful data manipulation and analysis using Spark.\n\n## Exploring the Data Structure\n\nWe can examine the schema of the loaded DataFrame to understand its structure and the types of data it holds.\n\n```python\ndf.schema\n```\n\nYou can call the `schema` method on the DataFrame to display the column names and types, which is crucial for data validation and cleaning tasks.\n\n## Working with Data Samples\n\nTo facilitate easier data analysis, we can create a smaller sample of the data containing just the first 1000 rows.\n\n```python\n!head -n 1001 fhvhv_tripdata_2021-01.csv > head.csv\n```\n\nThe command `head` extracts the first 1001 lines from the original CSV file to a new file called `head.csv`, which we will read into a pandas DataFrame.\n\n```python\nimport pandas as pd\n\ndf_pandas = pd.read_csv('head.csv')\n```\n\nHere, we utilize pandas to read the sample data into a DataFrame named `df_pandas`.\n\n## Checking Data Types\n\nIt's important to validate the data types of each column, which signifies how the data can be processed and analyzed.\n\n```python\ndf_pandas.dtypes\n```\n\nThis command provides us with the data types of each column in the pandas DataFrame, helping us ensure that they align with our intended analysis.\n\n## Defining a Schema in Spark\n\nTo ensure that the Spark DataFrame has the correct structure, we can explicitly define a schema that includes the expected data types.\n\n```python\nfrom pyspark.sql import types\n\nschema = types.StructType([\n    types.StructField('hvfhs_license_num', types.StringType(), True),\n    types.StructField('dispatching_base_num', types.StringType(), True),\n    types.StructField('pickup_datetime', types.TimestampType(), True),\n    types.StructField('dropoff_datetime', types.TimestampType(), True),\n    types.StructField('PULocationID', types.IntegerType(), True),\n    types.StructField('DOLocationID', types.IntegerType(), True),\n    types.StructField('SR_Flag', types.StringType(), True)\n])\n```\n\nIn this block, a new `schema` is defined using `StructType` and `StructField`, specifying the column names and their respective data types to ensure the DataFrame is constructed correctly.\n\n## Loading Data with Defined Schema\n\nWith our schema established, we can now read the CSV file again while enforcing this specified structure.\n\n```python\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .schema(schema) \\\n    .csv('fhvhv_tripdata_2021-01.csv')\n```\n\nThis code reads the CSV file into a DataFrame called `df`, ensuring that the data adheres to the structure we defined.\n\n## Repartitioning and Writing Data\n\nTo optimize processing, we can repartition the DataFrame into a specific number of partitions and write the data in a Parquet format.\n\n```python\ndf = df.repartition(24)\n\ndf.write.parquet('fhvhv/2021/01/')\n```\n\nRepartitioning is crucial for performance on large datasets, and writing as Parquet allows for efficient storage and faster querying of the data later.\n\n## Reading Data Back\n\nWe can easily read the written Parquet data back into Spark for further processing.\n\n```python\ndf = spark.read.parquet('fhvhv/2021/01/')\n```\n\nThis command allows us to load the previous Parquet file back into a Spark DataFrame.\n\n## Displaying the Data Schema\n\nTo understand the structure of the newly loaded DataFrame, we can print its schema again.\n\n```python\ndf.printSchema()\n```\n\nThis command outputs the schema, giving an overview of how the data is organized in the DataFrame.\n\n## Data Selection and Filtering\n\nWe can now perform operations on our DataFrame, such as filtering data based on specific conditions.\n\n```python\ndf.select(*['pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID']) \\\n  .filter(df.hvfhs_license_num == 'HV0003')\n```\n\nThis SQL-like syntax allows us to select specific columns and apply filtering conditions to narrow down our data for analysis.\n\n## Exploring the Data Sample\n\nTo have a quick look at our sample data, we can print the first few rows.\n\n```python\n!head -n 10 head.csv\n```\n\nThis command outputs the first 10 lines of the head.csv file for a quick visual inspection of the data.\n\n## Custom Function for Data Processing\n\nLastly, we define a custom function that performs processing on the `dispatching_base_num`.\n\n```python\ndef crazy_stuff(base_num):\n    num = int(base_num[1:])\n    if num % 7 == 0:\n        return f's/{num:03x}'\n    elif num % 3 == 0:\n        return f'a/{num:03x}'\n    else:\n        return f'e/{num:03x}'\n```\n\nThis function converts the base number into a hexadecimal format based on the provided logic. \n\n## Creating a User Defined Function (UDF)\n\nTo use the custom function within the Spark DataFrame transformations, we need to register it as a User Defined Function (UDF).\n\n```python\ncrazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())\n```\n\nThe `udf` method converts the `crazy_stuff` function so that it can be applied to each row of the DataFrame.\n\n## Applying Transformations\n\nFinally, we can apply several transformations\u2014adding columns for pickup and dropoff dates and including our custom base ID transformation.\n\n```python\ndf \\\n    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n    .show()\n```\n\nIn this last block, we transform the DataFrame to include new columns for `pickup_date` and `dropoff_date`, apply our custom UDF, and select specific columns to display.\n\nNow, you have a comprehensive overview of processing NYC taxi trip data with PySpark, demonstrating how to download, transform, and analyze large datasets efficiently.",
    "filename": "05-batch/code/04_pyspark.ipynb"
  },
  {
    "code": false,
    "content": "# Spark Data Processing Documentation \n\nThis documentation provides a structured overview of a Python script utilizing PySpark for data processing. The script processes green and yellow taxi data, converting CSV files into Parquet format for efficient storage and analysis.\n\n## Setting Up the Spark Environment\n\nThe first step involves importing necessary libraries and initializing a Spark session. \n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\n```\n\nIn this code block, the `SparkSession` is initialized, which is essential for working with DataFrames in PySpark. \n\n```python\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n```\n\nHere, the `master(\"local[*]\")` configuration allows Spark to utilize all available cores on the local machine, while the application is named 'test'. Finally, the `getOrCreate()` method either retrieves an existing session or creates a new one.\n\n## Importing Additional Libraries\n\nBefore defining the data schemas, other libraries are imported.\n\n```python\nimport pandas as pd\n```\n\nIn this code block, `pandas` is imported. While not used directly in this particular script, pandas is a powerful library for data manipulation and may be included for potential additional analysis.\n\n```python\nfrom pyspark.sql import types\n```\n\nThis block imports the `types` module from PySpark, which will be used to define the schemas for our data.\n\n## Defining Data Schemas\n\nThe schemas for the green and yellow taxi datasets are defined using the `StructType` and `StructField` types. \n\n```python\ngreen_schema = types.StructType([\n    types.StructField(\"VendorID\", types.IntegerType(), True),\n    types.StructField(\"lpep_pickup_datetime\", types.TimestampType(), True),\n    types.StructField(\"lpep_dropoff_datetime\", types.TimestampType(), True),\n    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n    types.StructField(\"RatecodeID\", types.IntegerType(), True),\n    types.StructField(\"PULocationID\", types.IntegerType(), True),\n    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n    types.StructField(\"passenger_count\", types.IntegerType(), True),\n    types.StructField(\"trip_distance\", types.DoubleType(), True),\n    types.StructField(\"fare_amount\", types.DoubleType(), True),\n    types.StructField(\"extra\", types.DoubleType(), True),\n    types.StructField(\"mta_tax\", types.DoubleType(), True),\n    types.StructField(\"tip_amount\", types.DoubleType(), True),\n    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n    types.StructField(\"ehail_fee\", types.DoubleType(), True),\n    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n    types.StructField(\"total_amount\", types.DoubleType(), True),\n    types.StructField(\"payment_type\", types.IntegerType(), True),\n    types.StructField(\"trip_type\", types.IntegerType(), True),\n    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n])\n```\n\nIn this block, the schema for the green taxi data is defined. Each field in the schema includes the field name, data type, and nullability. This schema is crucial for ensuring that data is read correctly from the CSV files.\n\n```python\nyellow_schema = types.StructType([\n    types.StructField(\"VendorID\", types.IntegerType(), True),\n    types.StructField(\"tpep_pickup_datetime\", types.TimestampType(), True),\n    types.StructField(\"tpep_dropoff_datetime\", types.TimestampType(), True),\n    types.StructField(\"passenger_count\", types.IntegerType(), True),\n    types.StructField(\"trip_distance\", types.DoubleType(), True),\n    types.StructField(\"RatecodeID\", types.IntegerType(), True),\n    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n    types.StructField(\"PULocationID\", types.IntegerType(), True),\n    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n    types.StructField(\"payment_type\", types.IntegerType(), True),\n    types.StructField(\"fare_amount\", types.DoubleType(), True),\n    types.StructField(\"extra\", types.DoubleType(), True),\n    types.StructField(\"mta_tax\", types.DoubleType(), True),\n    types.StructField(\"tip_amount\", types.DoubleType(), True),\n    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n    types.StructField(\"total_amount\", types.DoubleType(), True),\n    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n])\n```\n\nSimilarly, this block outlines the schema for the yellow taxi dataset, mirroring the structure of the green schema but with slight differences in field names.\n\n## Processing Green Taxi Data\n\nThe script processes data for both green and yellow taxi datasets for the years 2020 and 2021. Below is the procedure for handling the green taxi data for 2020.\n\n```python\nyear = 2020\n\nfor month in range(1, 13):\n    print(f'processing data for {year}/{month}')\n\n    input_path = f'data/raw/green/{year}/{month:02d}/'\n    output_path = f'data/pq/green/{year}/{month:02d}/'\n\n    df_green = spark.read \\\n        .option(\"header\", \"true\") \\\n        .schema(green_schema) \\\n        .csv(input_path)\n\n    df_green \\\n        .repartition(4) \\\n        .write.parquet(output_path)\n```\n\nThis block features a loop that iterates over each month of the year 2020. For each month, it constructs the input and output paths for the data. The dataset is then read using the defined green schema, and the data is unevenly repartitioned into four partitions before being written in Parquet format to the defined output path.\n\n## Processing Yellow Taxi Data\n\nFollowing the same method, the yellow taxi data is processed for the years 2020 and 2021.\n\n```python\nyear = 2020\n\nfor month in range(1, 13):\n    print(f'processing data for {year}/{month}')\n\n    input_path = f'data/raw/yellow/{year}/{month:02d}/'\n    output_path = f'data/pq/yellow/{year}/{month:02d}/'\n\n    df_yellow = spark.read \\\n        .option(\"header\", \"true\") \\\n        .schema(yellow_schema) \\\n        .csv(input_path)\n\n    df_yellow \\\n        .repartition(4) \\\n        .write.parquet(output_path)\n```\n\nThis block packages the same logic but for yellow taxis in the year 2020. Here, the data is read utilizing the yellow schema and is written in Parquet format with similar output configurations.\n\n```python\nyear = 2021\n\nfor month in range(1, 13):\n    print(f'processing data for {year}/{month}')\n\n    input_path = f'data/raw/yellow/{year}/{month:02d}/'\n    output_path = f'data/pq/yellow/{year}/{month:02d}/'\n\n    df_yellow = spark.read \\\n        .option(\"header\", \"true\") \\\n        .schema(yellow_schema) \\\n        .csv(input_path)\n\n    df_yellow \\\n        .repartition(4) \\\n        .write.parquet(output_path)\n```\n\nIn this chunk, the same procedure is repeated for the yellow taxi data from the year 2021. Each month, the corresponding CSV data is read, processed, and stored efficiently in the Parquet format.\n\n## Conclusion\n\nThis script effectively processes both green and yellow taxi datasets, transforming raw CSV files into Parquet files. This transition not only optimizes storage but also enhances performance for subsequent data analysis. The structured approach, including schema definitions and modular code blocks, is designed to facilitate clarity and facilitate performance optimization in real-world scenarios.",
    "filename": "05-batch/code/05_taxi_schema.ipynb"
  },
  {
    "code": false,
    "content": "# Taxi Revenue Analysis with PySpark\n\nThis document provides a comprehensive guide to performing a revenue analysis on New York taxi data using PySpark. The tutorial covers the setup of the Spark environment, data loading, transformation, selection, and final analysis through SQL queries.\n\n## Setting Up PySpark\n\nTo begin, we need to set up a PySpark environment. This involves creating a `SparkSession`, which is the entry point for interacting with Spark. The following code snippet demonstrates how to initialize this session.\n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n```\n\nThe above code specifies that we want to run Spark locally (using all available cores) and names our application 'test'. This session is essential for performing data operations and transformations throughout this analysis.\n\n## Loading Data for Green Taxis\n\nNext, we will load the green taxi data from a specified Parquet file location. The `.read.parquet()` method allows us to easily read the Parquet files stored in the designated folder.\n\n```python\ndf_green = spark.read.parquet('data/pq/green/*/*')\n```\n\nThis line imports all green taxi data into a DataFrame called `df_green`. Parquet format is efficient for reading and processing large datasets, making it ideal for our analysis.\n\n## Renaming Columns in Green Taxi Data\n\nAfter loading the data, it's important to standardize column names for better consistency and usability. The following code renames the relevant columns in the green taxi DataFrame.\n\n```python\ndf_green = df_green \\\n    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n```\n\nHere, we standardize the naming of the pickup and dropoff datetime columns to `pickup_datetime` and `dropoff_datetime` for uniformity across datasets.\n\n## Loading Data for Yellow Taxis\n\nSimilar to the green taxis, we will also load the yellow taxi data using the same methodology to maintain consistency in our dataset shapes.\n\n```python\ndf_yellow = spark.read.parquet('data/pq/yellow/*/*')\n```\n\nThis line reads all yellow taxi data from the specified Parquet files into a DataFrame called `df_yellow`.\n\n## Renaming Columns in Yellow Taxi Data\n\nAs with the green taxi data, we need to rename the columns in the yellow taxi DataFrame to ensure consistency.\n\n```python\ndf_yellow = df_yellow \\\n    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n```\n\nWe rename the datetime columns in the yellow taxi DataFrame to match the green taxi DataFrame, utilizing the same standardized naming convention.\n\n## Identifying Common Columns\n\nBefore combining the two datasets, we should identify the common columns they share. This process helps us ensure that the merged DataFrame will contain consistent columns.\n\n```python\ncommon_colums = []\n\nyellow_columns = set(df_yellow.columns)\n\nfor col in df_green.columns:\n    if col in yellow_columns:\n        common_colums.append(col)\n```\n\nIn this code block, we create a list of common columns by comparing the columns from both DataFrames. The `common_colums` list will be used for further selections in our analysis.\n\n## Preparing Selected Data for Green Taxis\n\nNow that we have the common columns, we will create a new DataFrame that only includes these columns from the green taxi dataset and add a new column to specify the service type.\n\n```python\nfrom pyspark.sql import functions as F\n\ndf_green_sel = df_green \\\n    .select(common_colums) \\\n    .withColumn('service_type', F.lit('green'))\n```\n\nWe select only the common columns and add a new column `service_type`, which indicates that these entries originate from the green taxi dataset.\n\n## Preparing Selected Data for Yellow Taxis\n\nFollowing the same approach for the yellow taxi dataset, we will create a similar DataFrame.\n\n```python\ndf_yellow_sel = df_yellow \\\n    .select(common_colums) \\\n    .withColumn('service_type', F.lit('yellow'))\n```\n\nHere, we execute the same selection and column addition process as we did for the green taxi DataFrame, but now specifying that the records are from yellow cabs.\n\n## Combining the Datasets\n\nNext, we will merge the green and yellow taxi DataFrames into a single DataFrame for a unified view of taxi trips.\n\n```python\ndf_trips_data = df_green_sel.unionAll(df_yellow_sel)\n```\n\nThe `unionAll()` function is utilized to concatenate the two DataFrames vertically, ensuring we maintain all rows from both datasets.\n\n## Counting Records by Service Type\n\nTo observe how many records exist for each type of taxi service, we can group the combined dataset by the `service_type` column and count the occurrences.\n\n```python\ndf_trips_data.groupBy('service_type').count().show()\n```\n\nThis operation groups the data and presents a count of trips for each type of taxi service, providing a quick overview of the distribution of trips between green and yellow taxis.\n\n## Checking Available Columns\n\nAt this stage, it\u2019s prudent to review what columns exist in the combined DataFrame for validation.\n\n```python\ndf_trips_data.columns\n```\n\nThis line fetches the column names of the `df_trips_data` DataFrame, ensuring that we have all the relevant data before proceeding with more complex analyses.\n\n## Registering the DataFrame as a Temporary Table\n\nTo run SQL queries, we need to register our DataFrame as a temporary table.\n\n```python\ndf_trips_data.registerTempTable('trips_data')\n```\n\nThis command registers the `df_trips_data` DataFrame under the name 'trips_data', allowing us to use SQL queries against it.\n\n## Performing the Revenue Analysis with SQL\n\nFinally, we can carry out a comprehensive SQL query to calculate various revenue metrics for both taxi services, grouped by revenue zones and months.\n\n```python\ndf_result = spark.sql(\"\"\"\nSELECT \n    -- Revenue grouping \n    PULocationID AS revenue_zone,\n    date_trunc('month', pickup_datetime) AS revenue_month, \n    service_type, \n\n    -- Revenue calculation \n    SUM(fare_amount) AS revenue_monthly_fare,\n    SUM(extra) AS revenue_monthly_extra,\n    SUM(mta_tax) AS revenue_monthly_mta_tax,\n    SUM(tip_amount) AS revenue_monthly_tip_amount,\n    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n    SUM(total_amount) AS revenue_monthly_total_amount,\n    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n\n    -- Additional calculations\n    AVG(passenger_count) AS avg_monthly_passenger_count,\n    AVG(trip_distance) AS avg_monthly_trip_distance\nFROM\n    trips_data\nGROUP BY\n    1, 2, 3\n\"\"\")\n```\n\nIn this SQL query, we calculate total revenues and average passenger statistics based on pick-up locations and service types, summarizing the financial performance of each taxi service over the months.\n\n## Writing the Results to Parquet\n\nTo conclude our process, we will save the results of our analysis back into a Parquet file format, which is efficient for large datasets.\n\n```python\ndf_result.coalesce(1).write.parquet('data/report/revenue/', mode='overwrite')\n```\n\nThe `coalesce(1)` method ensures that we write a single file to the specified directory, overwriting any existing data. This step is crucial for generating a clean final report of our findings.\n\n## Conclusion\n\nThis document has guided you through the steps necessary for a comprehensive taxi revenue analysis using PySpark. From data loading and transformation to SQL querying and report generation, the provided code establishes a robust framework for handling large datasets and performing actionable insights. ",
    "filename": "05-batch/code/06_spark_sql.ipynb"
  },
  {
    "code": false,
    "content": "# Spark Data Processing Script Documentation\n\nThis script is designed to process taxi trip data from two different sources \u2014 green and yellow taxi trip records \u2014 and then generates a consolidated report on the monthly revenue and trip statistics for these services. It utilizes the Apache Spark framework for distributed data processing with PySpark.\n\n## 1. Imports and Argument Parsing\n```python\nimport argparse\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n```\nThe script begins by importing necessary libraries:\n- `argparse`: for handling command-line arguments.\n- `pyspark`: for working with large-scale data processing.\n- `SparkSession` and `functions` from `pyspark.sql`: for creating a Spark session and using SQL functions.\n\nNext, an `ArgumentParser` object is created to handle input parameters. The script expects three command-line arguments: \n- `input_green`: Path to green taxi trip data in Parquet format.\n- `input_yellow`: Path to yellow taxi trip data in Parquet format.\n- `output`: Path for writing the output results.\n\n## 2. Reading Input Data\n```python\ndf_green = spark.read.parquet(input_green)\ndf_green = df_green.withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n                     .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n\ndf_yellow = spark.read.parquet(input_yellow)\ndf_yellow = df_yellow.withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n                     .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n```\nThe script creates a `SparkSession`, which acts as an entry point for programming Spark with the Dataset and DataFrame API. It reads the Parquet files for both green and yellow taxis into DataFrames. \n\nUpon loading, two columns (`lpep_pickup_datetime` and `lpep_dropoff_datetime` for green; `tpep_pickup_datetime` and `tpep_dropoff_datetime` for yellow) are standardized by renaming them to `pickup_datetime` and `dropoff_datetime`, respectively.\n\n## 3. Column Selection\n```python\ncommon_colums = [\n    'VendorID', 'pickup_datetime', 'dropoff_datetime', 'store_and_fwd_flag', \n    'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count', \n    'trip_distance', 'fare_amount', 'extra', 'mta_tax', \n    'tip_amount', 'tolls_amount', 'improvement_surcharge', \n    'total_amount', 'payment_type', 'congestion_surcharge'\n]\n```\nA list of common columns that both the green and yellow DataFrames share is defined. This includes important trip and payment information, such as the vendor ID, location IDs, passenger count, and financial data related to the fare.\n\n## 4. Data Preparation\n```python\ndf_green_sel = df_green.select(common_colums) \\\n    .withColumn('service_type', F.lit('green'))\n\ndf_yellow_sel = df_yellow.select(common_colums) \\\n    .withColumn('service_type', F.lit('yellow'))\n```\nBoth DataFrames are filtered to retain only the columns specified in `common_colums`. A new column named `service_type` is added to indicate whether the row corresponds to a green or yellow taxi trip.\n\n## 5. Union of DataFrames\n```python\ndf_trips_data = df_green_sel.unionAll(df_yellow_sel)\ndf_trips_data.registerTempTable('trips_data')\n```\nThe processed DataFrames for green and yellow taxis are combined into a single DataFrame called `df_trips_data` using the `unionAll` method. This DataFrame is then registered as a temporary SQL table named `trips_data`, allowing for SQL queries on the consolidated dataset.\n\n## 6. Data Analysis with SQL\n```python\ndf_result = spark.sql(\"\"\"\nSELECT \n    PULocationID AS revenue_zone,\n    date_trunc('month', pickup_datetime) AS revenue_month, \n    service_type, \n    SUM(fare_amount) AS revenue_monthly_fare,\n    SUM(extra) AS revenue_monthly_extra,\n    SUM(mta_tax) AS revenue_monthly_mta_tax,\n    SUM(tip_amount) AS revenue_monthly_tip_amount,\n    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n    SUM(total_amount) AS revenue_monthly_total_amount,\n    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n    AVG(passenger_count) AS avg_montly_passenger_count,\n    AVG(trip_distance) AS avg_montly_trip_distance\nFROM\n    trips_data\nGROUP BY\n    1, 2, 3\n\"\"\")\n```\nA SQL query is executed against the `trips_data` table to perform aggregations. The query:\n- Groups by `PULocationID`, the month of `pickup_datetime`, and `service_type`.\n- Calculates monthly revenue for various fare components, including total fares and additional fees.\n- Averages passenger count and trip distances.\n\nThe result is a summary DataFrame, `df_result`, containing essential statistics for further analysis.\n\n## 7. Writing Output Data\n```python\ndf_result.coalesce(1) \\\n    .write.parquet(output, mode='overwrite')\n```\nFinally, the processed and aggregated results are written back to a specified output location in Parquet format. The `coalesce(1)` method is used to reduce the output to a single partition, which may make it easier to handle for downstream processes, albeit at the cost of potential performance trade-offs.\n\nOverall, this script provides an efficient way to ingest, process, and analyze large taxi trip datasets, leading to valuable insights on monthly revenues and trip characteristics.",
    "filename": "05-batch/code/06_spark_sql.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis script processes taxi trip data from two sources (green and yellow taxis) using PySpark, a Python library for Apache Spark. It reads Parquet files containing trip data, standardizes the column names, combines the data, and performs aggregations to compute monthly revenue and other statistics. Finally, it saves the processed results into a BigQuery table.\n\n## Dependencies\n\n- **argparse**: This module provides a command-line argument parser.\n- **pyspark**: Used for handling big data processing with Apache Spark.\n- **SparkSession**: The entry point for reading data and creating DataFrame work.\n- **functions as F**: This is used to leverage Spark SQL functions conveniently.\n\n## Command-Line Arguments\n\nThe script requires three command-line arguments:\n- `--input_green`: Path to the input Parquet file for green taxi data.\n- `--input_yellow`: Path to the input Parquet file for yellow taxi data.\n- `--output`: Path to the output BigQuery table location for storing results.\n\n## Initializing Spark Session\n\nThe script initializes a Spark session named 'test'. A temporary bucket for GCP Dataproc is also set to handle intermediate files during execution. This ensures that Spark processes the data efficiently.\n\n```python\nspark = SparkSession.builder \\\n    .appName('test') \\\n    .getOrCreate()\n\nspark.conf.set('temporaryGcsBucket', 'dataproc-temp-europe-west6-...')\n```\n\n## Reading and Preparing DataFrames\n\nGreen and yellow taxi data are read from Parquet files into separate DataFrames (`df_green` and `df_yellow`). The relevant datetime columns are renamed to provide a uniform schema.\n\n```python\ndf_green = spark.read.parquet(input_green) \\\n    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n\ndf_yellow = spark.read.parquet(input_yellow) \\\n    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n```\n\n## Selecting Common Columns\n\nThe script extracts a predefined set of common columns from both DataFrames, ensuring that the data merged later will have a consistent schema. Each DataFrame is also tagged with a `service_type` column indicating the type of taxi (green or yellow).\n\n```python\ndf_green_sel = df_green.select(common_columns).withColumn('service_type', F.lit('green'))\ndf_yellow_sel = df_yellow.select(common_columns).withColumn('service_type', F.lit('yellow'))\n```\n\n## Combining DataFrames\n\nThe script combines the two DataFrames into a single DataFrame `df_trips_data` using the `unionAll` method. This union operation stacks the yellow taxi data on top of the green taxi data.\n\n```python\ndf_trips_data = df_green_sel.unionAll(df_yellow_sel)\n```\n\n## Registering Temporary Table\n\nThe combined DataFrame is registered as a temporary SQL table named `trips_data`. This allows SQL-style querying using Spark SQL syntax later in the script.\n\n```python\ndf_trips_data.registerTempTable('trips_data')\n```\n\n## SQL Aggregation Query\n\nA SQL query is executed to generate a report from the temporary table. The query aggregates the data to calculate monthly revenues from different fare components, as well as averages for passenger counts and trip distances. The results are grouped by pickup location and month.\n\n```python\ndf_result = spark.sql(\"\"\"\nSELECT \n    -- Revenue grouping \n    PULocationID AS revenue_zone,\n    date_trunc('month', pickup_datetime) AS revenue_month, \n    service_type, \n    -- Revenue calculation \n    ...\n\"\"\"\n```\n\n## Writing to BigQuery\n\nFinally, the resulting DataFrame `df_result` is written to BigQuery using the specified output path, effectively saving the processed aggregation results in a structured table format.\n\n```python\ndf_result.write.format('bigquery') \\\n    .option('table', output) \\\n    .save()\n```\n\n## Conclusion\n\nIn summary, this script efficiently processes green and yellow taxi trip data, standardizes it, combines it, aggregates relevant metrics, and exports the results to BigQuery. It leverages PySpark's capabilities to handle potentially large datasets in a distributed computing environment. This script could serve as a foundation for further analysis or reporting on taxi services' performance.",
    "filename": "05-batch/code/06_spark_sql_big_query.py"
  },
  {
    "code": false,
    "content": "# Spark ETL Process for Taxi Revenue Analysis\n\nThis document outlines the steps involved in processing taxi revenue data using Apache Spark. The workflow includes reading data, performing SQL operations, and aggregating results for green and yellow taxis. \n\n## Setting Up the Spark Session\n\nBefore initiating any data processing, we need to set up a Spark session. This is the entry point to using Spark SQL.\n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n```\n\nIn the code above, we import the necessary libraries and initialize a `SparkSession`, which will manage the underlying Spark context. The `master(\"local[*]\")` parameter allows Spark to use all available cores on your local machine. `appName('test')` assigns a name to our application for identification.\n\n## Reading Green Taxi Data\n\nNext, we will read the green taxi data from Parquet files stored in the specified directory.\n\n```python\ndf_green = spark.read.parquet('data/pq/green/*/*')\n```\n\nThis command reads all the Parquet files containing green taxi trip data into a DataFrame called `df_green`. Parquet is a columnar storage file format that is optimized for performance and compression.\n\n## Registering the Green Taxi Data Table\n\nTo perform SQL queries on our DataFrame, we need to register it as a temporary table.\n\n```python\ndf_green.registerTempTable('green')\n```\n\nHere, we register the `df_green` DataFrame as a temporary SQL table named `green`. This will allow us to run SQL queries against it easily.\n\n## Aggregating Revenue for Green Taxis\n\nNow we perform an SQL query to calculate the hourly revenue of green taxis.\n\n```python\ndf_green_revenue = spark.sql(\"\"\"\nSELECT \n    date_trunc('hour', lpep_pickup_datetime) AS hour, \n    PULocationID AS zone,\n    SUM(total_amount) AS amount,\n    COUNT(1) AS number_records\nFROM\n    green\nWHERE\n    lpep_pickup_datetime >= '2020-01-01 00:00:00'\nGROUP BY\n    1, 2\n\"\"\")\n```\n\nIn this SQL operation, we:\n- Truncate the pickup datetime to the hour level.\n- Group the results by hour and pick-up zone.\n- Calculate total revenue (`SUM(total_amount)`) and the count of records for each group.\n- Filter the records to include only those from the year 2020 onwards.\n\n## Writing Green Taxi Revenue Data to Disk\n\nAfter aggregating the data, we write the results to a new Parquet file.\n\n```python\ndf_green_revenue \\\n    .repartition(20) \\\n    .write.parquet('data/report/revenue/green', mode='overwrite')\n```\n\nHere, we use `repartition(20)` to optimize the data distribution across 20 partitions for better parallelism when writing to disk. The results are stored in the specified directory, overwriting any existing data.\n\n## Reading Yellow Taxi Data\n\nWe will repeat the earlier steps for yellow taxi data.\n\n```python\ndf_yellow = spark.read.parquet('data/pq/yellow/*/*')\ndf_yellow.registerTempTable('yellow')\n```\n\nSimilar to the green taxi data, we read the yellow taxi trip data into `df_yellow` and register it as a temporary SQL table named `yellow`.\n\n## Aggregating Revenue for Yellow Taxis\n\nNext, we'll execute a SQL query to calculate the hourly revenue for yellow taxis.\n\n```python\ndf_yellow_revenue = spark.sql(\"\"\"\nSELECT \n    date_trunc('hour', tpep_pickup_datetime) AS hour, \n    PULocationID AS zone,\n    SUM(total_amount) AS amount,\n    COUNT(1) AS number_records\nFROM\n    yellow\nWHERE\n    tpep_pickup_datetime >= '2020-01-01 00:00:00'\nGROUP BY\n    1, 2\n\"\"\")\n```\n\nThis query mirrors the previous one but focuses on yellow taxi data. The SQL commands used are functionally the same, just referencing the `yellow` table.\n\n## Writing Yellow Taxi Revenue Data to Disk\n\nAfter gathering the yellow taxi revenue data, we write it to disk.\n\n```python\ndf_yellow_revenue \\\n    .repartition(20) \\\n    .write.parquet('data/report/revenue/yellow', mode='overwrite')\n```\n\nSimilar to the green taxi data, the revenue for yellow taxis is stored in the specified directory with a repartition for optimization.\n\n## Loading Combined Revenue Data\n\nWe now read the aggregated revenue results for both green and yellow taxis for further processing.\n\n```python\ndf_green_revenue = spark.read.parquet('data/report/revenue/green')\ndf_yellow_revenue = spark.read.parquet('data/report/revenue/yellow')\n```\n\nThe previously saved Parquet files for both taxi types are loaded into DataFrames for subsequent joins and analyses.\n\n## Preparing Data for Joining\n\nTo combine the revenue data for both green and yellow taxis, we need to rename columns to avoid conflicts.\n\n```python\ndf_green_revenue_tmp = df_green_revenue \\\n    .withColumnRenamed('amount', 'green_amount') \\\n    .withColumnRenamed('number_records', 'green_number_records')\n\ndf_yellow_revenue_tmp = df_yellow_revenue \\\n    .withColumnRenamed('amount', 'yellow_amount') \\\n    .withColumnRenamed('number_records', 'yellow_number_records')\n```\n\nIn this code, we rename the 'amount' and 'number_records' columns for both DataFrames. This ensures clarity when we join the two DataFrames by retaining distinct column names for each taxi type.\n\n## Joining Green and Yellow Revenue Data\n\nNow we will join the two DataFrames based on the common columns `hour` and `zone`.\n\n```python\ndf_join = df_green_revenue_tmp.join(df_yellow_revenue_tmp, on=['hour', 'zone'], how='outer')\n```\n\nWe perform an outer join, which ensures that all records from both DataFrames are included, even if some entries do not match.\n\n## Writing Combined Revenue Data to Disk\n\nAfter merging the two DataFrames, we save the combined data.\n\n```python\ndf_join.write.parquet('data/report/revenue/total', mode='overwrite')\n```\n\nThe joined DataFrame is written to disk, allowing for future access and analysis.\n\n## Loading Combined Revenue Data for Analysis\n\nWe can now load the combined revenue data from the output.\n\n```python\ndf_join = spark.read.parquet('data/report/revenue/total')\n```\n\nThis command reads the previously saved joined revenue DataFrame back into Spark for analysis.\n\n## Displaying the Joined DataFrame\n\nTo visualize the joined data, we can use the `df_join` DataFrame.\n\n```python\ndf_join\n```\n\nDisplaying this DataFrame provides insight into the combined revenue statistics for both green and yellow taxis.\n\n## Reading Zone Data\n\nNext, we retrieve zone information which will be used to augment our revenue data.\n\n```python\ndf_zones = spark.read.parquet('zones/')\n```\n\nZone data, which provides geographic context for the revenue data, is loaded into a DataFrame for merging operations.\n\n## Joining Revenue Data with Zone Information\n\nWe join the combined revenue data with zone information.\n\n```python\ndf_result = df_join.join(df_zones, df_join.zone == df_zones.LocationID)\n```\n\nThis join connects the revenue data to corresponding zones, allowing us later to analyze revenue geographically.\n\n## Finalizing the Result Dataset\n\nFinally, we clean up the resulting DataFrame by dropping unnecessary columns and saving the final product.\n\n```python\ndf_result.drop('LocationID', 'zone').write.parquet('tmp/revenue-zones')\n```\n\nThe cleaned DataFrame containing only relevant information is written to the specified directory for future use or analysis.",
    "filename": "05-batch/code/07_groupby_join.ipynb"
  },
  {
    "code": false,
    "content": "# Revenue Analysis Using PySpark\n\nThis document outlines the steps to perform revenue analysis on green taxi data using PySpark. It covers the initialization of a Spark session, data loading, filtering, aggregation, and applying a machine learning model to predict trip durations.\n\n## Setting Up the Spark Session\n\nTo start working with PySpark, we first need to import the necessary libraries and set up a Spark session. The Spark session is the entry point for any Spark functionality.\n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n```\n\nIn this code block, we initialize a Spark session in a local environment, which allows us to process data using Spark features. The `master(\"local[*]\")` indicates that the application will use all available cores on the local machine.\n\n## Loading Data from Parquet Files\n\nNext, we load data from Parquet files into a PySpark DataFrame. The data contains green taxi trip records.\n\n```python\ndf_green = spark.read.parquet('data/pq/green/*/*')\n```\n\nThe Parquet format is efficient for both storage and query performance. Loading the data into a DataFrame makes it easier to manipulate and analyze.\n\n## SQL Query for Data Aggregation\n\nWe can also perform SQL-like operations to aggregate data. The query below truncates the pickup datetime to the hour, groups by the hour and location, and calculates the total revenue and number of records.\n\n```sql\nSELECT \n    date_trunc('hour', lpep_pickup_datetime) AS hour, \n    PULocationID AS zone,\n    SUM(total_amount) AS amount,\n    COUNT(1) AS number_records\nFROM\n    green\nWHERE\n    lpep_pickup_datetime >= '2020-01-01 00:00:00'\nGROUP BY\n    1, 2\n```\n\nThis SQL statement calculates the total revenue and counts the number of trips during each hour for different pickup locations starting from January 1, 2020.\n\n## Working with RDDs\n\nAfter loading the data, we can convert a subset of the DataFrame into an RDD (Resilient Distributed Dataset) for further data transformation.\n\n```python\nrdd = df_green \\\n    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n    .rdd\n```\n\nHere, we extract only the necessary columns for analysis and convert them into an RDD to apply functional programming paradigms.\n\n## Date Filtering and Outlier Removal\n\nWe define a function to filter out rows that are older than a certain date.\n\n```python\nfrom datetime import datetime\n\nstart = datetime(year=2020, month=1, day=1)\n\ndef filter_outliers(row):\n    return row.lpep_pickup_datetime >= start\n```\n\nThe `filter_outliers` function ensures that only records from January 1, 2020, or later are considered, discarding any earlier data that might skew the analysis.\n\n## Examining Row Data\n\nWe can view a sample of the RDD to understand its structure and content.\n\n```python\nrows = rdd.take(10)\nrow = rows[0]\n```\n\nBy taking the first ten records from the RDD, we can inspect them to verify that the data has been loaded correctly.\n\n## Preparing Data for Grouping\n\nNext, we implement a function that organizes the data into a structure suitable for aggregation.\n\n```python\ndef prepare_for_grouping(row): \n    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n    zone = row.PULocationID\n    key = (hour, zone)\n    \n    amount = row.total_amount\n    count = 1\n    value = (amount, count)\n\n    return (key, value)\n```\n\nThis function prepares each record for grouping by replacing minutes and seconds of the pickup datetime with zero, allowing aggregation by hour and location.\n\n## Revenue Calculation\n\nTo calculate the total revenue and count of trips, we define a function that merges the values from grouped records.\n\n```python\ndef calculate_revenue(left_value, right_value):\n    left_amount, left_count = left_value\n    right_amount, right_count = right_value\n    \n    output_amount = left_amount + right_amount\n    output_count = left_count + right_count\n    \n    return (output_amount, output_count)\n```\n\nThe `calculate_revenue` function takes two values, representing different records, and sums their revenue and count for a single key.\n\n## Structuring the Output Data\n\nWe define a `namedtuple` to structure the output data conveniently.\n\n```python\nfrom collections import namedtuple\n\nRevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])\n```\n\nThe `RevenueRow` structure will hold the results of our revenue calculations, making it easier to convert back into a DataFrame later.\n\n## Unwrapping Grouped Data\n\nTo convert the grouped data back into our defined structure, we use the function below.\n\n```python\ndef unwrap(row):\n    return RevenueRow(\n        hour=row[0][0], \n        zone=row[0][1],\n        revenue=row[1][0],\n        count=row[1][1]\n    )\n```\n\nThis function takes the key-value pairs generated from the `reduceByKey` operation and wraps them in the `RevenueRow` namedtuple.\n\n## Defining the Output Schema\n\nWe need to define a schema for our resulting DataFrame to ensure proper data types.\n\n```python\nfrom pyspark.sql import types\n\nresult_schema = types.StructType([\n    types.StructField('hour', types.TimestampType(), True),\n    types.StructField('zone', types.IntegerType(), True),\n    types.StructField('revenue', types.DoubleType(), True),\n    types.StructField('count', types.IntegerType(), True)\n])\n```\n\nThe defined schema includes fields for hour, zone (pickup location ID), revenue, and the count of trips, which will help in creating a structured DataFrame later.\n\n## Aggregating Data and Writing Results\n\nFinally, we perform the filtering, grouping, and aggregation, and write the results back to a Parquet file.\n\n```python\ndf_result = rdd \\\n    .filter(filter_outliers) \\\n    .map(prepare_for_grouping) \\\n    .reduceByKey(calculate_revenue) \\\n    .map(unwrap) \\\n    .toDF(result_schema) \n\ndf_result.write.parquet('tmp/green-revenue')\n```\n\nIn this code block, we filter out outliers, prepare data for grouping, reduce by key to calculate revenue, and unwrap the data into a DataFrame that gets saved in Parquet format.\n\n## Preparing for Trip Duration Predictions\n\nNext, we prepare the data for predicting trip durations by selecting specific columns and converting them into an RDD.\n\n```python\ncolumns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n\nduration_rdd = df_green \\\n    .select(columns) \\\n    .rdd\n```\n\nThis RDD will be used to predict the duration of trips based on the selected features.\n\n## Converting RDD to Pandas DataFrame\n\nTo inspect the data and make predictions, we take a sample of the RDD and convert it to a Pandas DataFrame.\n\n```python\nimport pandas as pd\n\nrows = duration_rdd.take(10)\ndf = pd.DataFrame(rows, columns=columns)\n```\n\nBy converting the RDD to a Pandas DataFrame, we gain easier access to data manipulation and model application.\n\n## Making Predictions\n\nWe define a simple prediction function to estimate trip durations.\n\n```python\ndef model_predict(df):\n    # y_pred = model.predict(df)\n    y_pred = df.trip_distance * 5\n    return y_pred\n```\n\nIn this example, a mock prediction model estimates trip duration based on a straightforward calculation related to the trip distance.\n\n## Applying Predictions in Batches\n\nTo apply the model across the RDD, we define a function that processes rows in batches.\n\n```python\ndef apply_model_in_batch(rows):\n    df = pd.DataFrame(rows, columns=columns)\n    predictions = model_predict(df)\n    df['predicted_duration'] = predictions\n\n    for row in df.itertuples():\n        yield row\n```\n\nThe `apply_model_in_batch` function leverages the model to predict durations for each batch of rows, yielding results as named tuples for further processing.\n\n## Collecting Predictions into a DataFrame\n\nFinally, we map our batch predictions back into a DataFrame format.\n\n```python\ndf_predicts = duration_rdd \\\n    .mapPartitions(apply_model_in_batch) \\\n    .toDF() \\\n    .drop('Index')\n```\n\nThis code collects the predicted durations across all partitions into a new DataFrame, excluding the index column for cleanliness.\n\n## Displaying the Predictions\n\nWe can now display the predicted durations from our processed DataFrame.\n\n```python\ndf_predicts.select('predicted_duration').show()\n```\n\nThis command will show the predicted trip durations, allowing us to finalize our analysis and review the results.",
    "filename": "05-batch/code/08_rdds.ipynb"
  },
  {
    "code": false,
    "content": "# Setting Up PySpark With Google Cloud Storage\n\nThis documentation guides you through setting up a PySpark environment to read and manipulate data stored in Google Cloud Storage (GCS). The code snippets provided demonstrate the steps required to configure Spark and access data.\n\n## Importing Required Libraries\n\nFirst, we need to import the necessary PySpark libraries to utilize Spark functionalities. These libraries enable us to create Spark contexts and sessions, which are essential for processing big data.\n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext\n```\n\nThis code initializes the PySpark framework by importing the primary classes used for configuration and session management.\n\n## Configuring Spark for Google Cloud Storage\n\nNext, we'll configure Spark to use Google Cloud Storage by setting the required properties, such as application name and authentication details.\n\n```python\ncredentials_location = '/home/alexey/.google/credentials/google_credentials.json'\n\nconf = SparkConf() \\\n    .setMaster('local[*]') \\\n    .setAppName('test') \\\n    .set(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.5.jar\") \\\n    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n```\n\nIn this setup:\n- `setMaster('local[*]')` specifies that Spark runs locally, using all available cores.\n- `setAppName('test')` assigns a name to your Spark application.\n- The configuration string for the Google Cloud Storage connector is provided along with the path to your Google service account credentials.\n\n## Initializing Spark Context and Configuring Hadoop\n\nNext, we need to create a Spark Context object and configure it with Hadoop settings necessary for GCS interaction.\n\n```python\nsc = SparkContext(conf=conf)\n\nhadoop_conf = sc._jsc.hadoopConfiguration()\n\nhadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nhadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nhadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\nhadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n```\n\nThis section:\n- Initializes a Spark context using the specified configuration.\n- Sets Hadoop configurations needed to interact with Google Cloud Storage.\n- The relevant class names for the GCS filesystem are configured to ensure proper file operations in the cloud environment.\n\n## Creating a Spark Session\n\nWith the Spark Context set up, we proceed to initialize a Spark Session, which is the entry point for Spark SQL functionality.\n\n```python\nspark = SparkSession.builder \\\n    .config(conf=sc.getConf()) \\\n    .getOrCreate()\n```\n\nThis code creates a new Spark session, using the existing configurations from the Spark context, which allows us to leverage Spark's DataFrame API and SQL capabilities.\n\n## Reading Data from Google Cloud Storage\n\nNow that the environment is set up, we can read data stored in Google Cloud Storage. In this example, we read Parquet files from a specific GCS bucket.\n\n```python\ndf_green = spark.read.parquet('gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/*/*')\n```\n\nThis command reads all Parquet files from the specified path in the GCS bucket and creates a DataFrame named `df_green`. This DataFrame can now be used for further data processing or analysis.\n\n## Counting Rows in the DataFrame\n\nTo verify that we have successfully read the data, we can count the number of rows in our DataFrame.\n\n```python\ndf_green.count()\n```\n\nThis method returns the total number of rows in the `df_green` DataFrame, confirming that the data retrieval from Google Cloud Storage was successful.\n\n## Conclusion\n\nWith the above steps, you have successfully set up a PySpark environment that connects to Google Cloud Storage, reads Parquet files, and performs a simple operation on the data. You can now extend this setup to perform more complex data analyses or transformations as needed.",
    "filename": "05-batch/code/09_spark_gcs.ipynb"
  },
  {
    "content": "## Running Spark in the Cloud\n\n### Connecting to Google Cloud Storage \n\nUploading data to GCS:\n\n```bash\ngsutil -m cp -r pq/ gs://dtc_data_lake_de-zoomcamp-nytaxi/pq\n```\n\nDownload the jar for connecting to GCS to any location (e.g. the `lib` folder):\n\n**Note**: For other versions of GCS connector for Hadoop see [Cloud Storage connector ](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#connector-setup-on-non-dataproc-clusters).\n\n```bash\ngsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar ./lib/\n```\n\nSee the notebook with configuration in [09_spark_gcs.ipynb](09_spark_gcs.ipynb)\n\n(Thanks Alvin Do for the instructions!)\n\n\n### Local Cluster and Spark-Submit\n\nCreating a stand-alone cluster ([docs](https://spark.apache.org/docs/latest/spark-standalone.html)):\n\n```bash\n./sbin/start-master.sh\n```\n\nCreating a worker:\n\n```bash\nURL=\"spark://de-zoomcamp.europe-west1-b.c.de-zoomcamp-nytaxi.internal:7077\"\n./sbin/start-slave.sh ${URL}\n\n# for newer versions of spark use that:\n#./sbin/start-worker.sh ${URL}\n```\n\nTurn the notebook into a script:\n\n```bash\njupyter nbconvert --to=script 06_spark_sql.ipynb\n```\n\nEdit the script and then run it:\n\n```bash \npython 06_spark_sql.py \\\n    --input_green=data/pq/green/2020/*/ \\\n    --input_yellow=data/pq/yellow/2020/*/ \\\n    --output=data/report-2020\n```\n\nUse `spark-submit` for running the script on the cluster\n\n```bash\nURL=\"spark://de-zoomcamp.europe-west1-b.c.de-zoomcamp-nytaxi.internal:7077\"\n\nspark-submit \\\n    --master=\"${URL}\" \\\n    06_spark_sql.py \\\n        --input_green=data/pq/green/2021/*/ \\\n        --input_yellow=data/pq/yellow/2021/*/ \\\n        --output=data/report-2021\n```\n\n### Data Proc\n\nUpload the script to GCS:\n\n```bash\ngsutil -m cp -r 06_spark_sql.py gs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py\n```\n\nParams for the job:\n\n* `--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2021/*/`\n* `--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2021/*/`\n* `--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2021`\n\n\nUsing Google Cloud SDK for submitting to dataproc\n([link](https://cloud.google.com/dataproc/docs/guides/submit-job#dataproc-submit-job-gcloud))\n\n```bash\ngcloud dataproc jobs submit pyspark \\\n    --cluster=de-zoomcamp-cluster \\\n    --region=europe-west6 \\\n    gs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\n    -- \\\n        --input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\n        --input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\n        --output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020\n```\n\n### Big Query\n\nUpload the script to GCS:\n\n```bash\ngsutil -m cp -r 06_spark_sql_big_query.py gs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql_big_query.py\n```\n\nWrite results to big query ([docs](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example#pyspark)):\n\n```bash\ngcloud dataproc jobs submit pyspark \\\n    --cluster=de-zoomcamp-cluster \\\n    --region=europe-west6 \\\n    --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar \\\n    gs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql_big_query.py \\\n    -- \\\n        --input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\n        --input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\n        --output=trips_data_all.reports-2020\n```\n\nThere can be issue with latest Spark version and the Big query connector. Download links to the jar file for respective Spark versions can be found at:\n[Spark and Big query connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n\n**Note**: Dataproc on GCE 2.1+ images pre-install Spark BigQquery connector: [DataProc Release 2.2](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.2). Therefore, no need to include the jar file in the job submission.",
    "filename": "05-batch/code/cloud.md"
  },
  {
    "code": false,
    "content": "# PySpark Analysis of Taxi Trip Data\n\nThis document outlines a PySpark data analysis workflow using the `fhvhv_tripdata_2021-02.csv` dataset. The steps include initializing a Spark session, defining schemas, reading data, performing various queries, and extracting insights on taxi trips.\n\n## Setting Up Spark\n\nTo begin, we import the necessary PySpark libraries, which provide the tools needed to work with Spark data structures.\n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import types\n```\n\nNext, we initialize a `SparkSession`, which is the entry point for any PySpark application. By specifying the master node as local and naming the app \"test\", we can run this code locally.\n\n```python\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n```\n\nVerifying the Spark version is important to ensure compatibility with the code we'll run.\n\n```python\nspark.version\n```\n\nLastly, we check that the required CSV file is present in the working directory.\n\n```python\n!ls -lh fhvhv_tripdata_2021-02.csv\n```\n\n## Defining the Data Schema\n\nBefore loading the data, we define the schema. This step specifies the structure of our dataset, ensuring that each column has the correct data type.\n\n```python\nschema = types.StructType([\n    types.StructField('hvfhs_license_num', types.StringType(), True),\n    types.StructField('dispatching_base_num', types.StringType(), True),\n    types.StructField('pickup_datetime', types.TimestampType(), True),\n    types.StructField('dropoff_datetime', types.TimestampType(), True),\n    types.StructField('PULocationID', types.IntegerType(), True),\n    types.StructField('DOLocationID', types.IntegerType(), True),\n    types.StructField('SR_Flag', types.StringType(), True)\n])\n```\n\n## Loading the Data\n\nAfter defining the schema, we read the CSV file into a DataFrame. This operation includes options to treat the first row as the header and apply the schema we just defined.\n\n```python\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .schema(schema) \\\n    .csv('fhvhv_tripdata_2021-02.csv')\n\ndf = df.repartition(24)\n```\n\nWe then save the DataFrame as a Parquet file, allowing for efficient storage and access in the future.\n\n```python\ndf.write.parquet('data/pq/fhvhv/2021/02/', compression='snappy')\n```\n\nSubsequently, we can load the saved Parquet file for further analysis.\n\n```python\ndf = spark.read.parquet('data/pq/fhvhv/2021/02/')\n```\n\n## Analyzing Taxi Trips on February 15\n\nTo determine how many taxi trips were made on February 15, we utilize SQL functions. First, we add a column for the pickup date and filter the DataFrame accordingly.\n\n```python\nfrom pyspark.sql import functions as F\n\ndf \\\n    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n    .filter(\"pickup_date = '2021-02-15'\") \\\n    .count()\n```\n\nIn addition, we can run an SQL query to obtain the same result. We start by registering a temporary table for SQL operations.\n\n```python\ndf.registerTempTable('fhvhv_2021_02')\n```\n\nThen, we execute a SQL query that counts the trips on the specified date.\n\n```python\nspark.sql(\"\"\"\nSELECT\n    COUNT(1)\nFROM \n    fhvhv_2021_02\nWHERE\n    to_date(pickup_datetime) = '2021-02-15';\n\"\"\").show()\n```\n\n## Longest Trip Analysis\n\nTo analyze the longest trip for each day, we need to calculate trip duration by subtracting `pickup_datetime` from `dropoff_datetime`.\n\n```python\ndf.columns\n```\n\nNext, we create a new column for duration, group the results by pickup date, and then find the maximum duration for each day.\n\n```python\ndf \\\n    .withColumn('duration', df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long')) \\\n    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n    .groupBy('pickup_date') \\\n        .max('duration') \\\n    .orderBy('max(duration)', ascending=False) \\\n    .limit(5) \\\n    .show()\n```\n\nAlternatively, we can achieve the same analysis using an SQL query which provides flexibility for more complex queries.\n\n```python\nspark.sql(\"\"\"\nSELECT\n    to_date(pickup_datetime) AS pickup_date,\n    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60) AS duration\nFROM \n    fhvhv_2021_02\nGROUP BY\n    1\nORDER BY\n    2 DESC\nLIMIT 10;\n\"\"\").show()\n```\n\n## Dispatching Base Analysis\n\nIn order to find the most frequent `dispatching_base_num`, two methods are illustrated: using SQL and DataFrame operations. First, we run the SQL query.\n\n```python\nspark.sql(\"\"\"\nSELECT\n    dispatching_base_num,\n    COUNT(1)\nFROM \n    fhvhv_2021_02\nGROUP BY\n    1\nORDER BY\n    2 DESC\nLIMIT 5;\n\"\"\").show()\n```\n\nSimilarly, we can utilize DataFrame operations to get the same top results based on `dispatching_base_num`.\n\n```python\ndf \\\n    .groupBy('dispatching_base_num') \\\n        .count() \\\n    .orderBy('count', ascending=False) \\\n    .limit(5) \\\n    .show()\n```\n\n## Most Common Locations Pair\n\nIn this section, we focus on identifying the most common pickup and drop-off location pairs. First, we load in the zones data.\n\n```python\ndf_zones = spark.read.parquet('zones')\n```\n\nWe check the columns of both the main dataset and zones dataset to understand the available fields.\n\n```python\ndf_zones.columns\n```\n\n```python\ndf.columns\n```\n\nNext, we register the zones DataFrame as a temporary table for SQL queries.\n\n```python\ndf_zones.registerTempTable('zones')\n```\n\nFinally, we run a SQL query to find the most common pick-up and drop-off location pairs.\n\n```python\nspark.sql(\"\"\"\nSELECT\n    CONCAT(pul.Zone, ' / ', dol.Zone) AS pu_do_pair,\n    COUNT(1)\nFROM \n    fhvhv_2021_02 fhv LEFT JOIN zones pul ON fhv.PULocationID = pul.LocationID\n                      LEFT JOIN zones dol ON fhv.DOLocationID = dol.LocationID\nGROUP BY \n    1\nORDER BY\n    2 DESC\nLIMIT 5;\n\"\"\").show()\n```\n\nThrough these steps, we efficiently explored and analyzed the taxi trip data, providing insights into trip counts, durations, and popular locations.",
    "filename": "05-batch/code/homework.ipynb"
  },
  {
    "content": "## Spark on YARN \n\nFor the Spark and Docker module, we need YARN, which\ncomes together with Hadoop. So we need to install Hadoop\n\nIn this document, we'll assume you use Linux. For Windows, use WSL. It should work (supposedly) on MacOS as well. \n\nWe'll need to run it in a pseudo-distributed mode.\n\n\n### Configuring ssh\n\nYou need to run be able to `ssh` to your localhost without having to type any password. In other words, you execute \n\n```bash\nssh localhost\n```\n\nAnd you get ssh access. \n\nIf you don't have it, add your `id_rsa.pub` key to the list of keys authorized to access your computer:\n\n```bash\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys\n```\n\n(This assumes you already have `id_rsa.pub` in `~/.ssh`)\n\nOn WSL, you may need to start the ssh service:\n\n```bash\nsudo service ssh start\n```\n\n### Download Hadoop binaries\n\nWe use Spark that expects Hadoop 3.2 version. So we'll install it.\n\nGo to the [Hadoop's website](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz) to get the closest mirror. And then download it:\n\n```bash\nwget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n```\n\nUnpack it and go to this directory\n\n```bash\ntar xzfv hadoop-3.2.3.tar.gz\ncd hadoop-3.2.3/\n```\n\n\n### YARN on a Single Node\n\nSet `JAVA_HOME` in `etc/hadoop/hadoop-env.sh`:\n\n```bash\necho \"export JAVA_HOME=${JAVA_HOME}\" >> etc/hadoop/hadoop-env.sh\n```\n\nStart YARN\n\n```bash\n./sbin/start-yarn.sh\n```\n\nYARN should work on port 8088: http://localhost:8088/\n\n\n### Running Spark on YARN\n\nFor submitting spark jobs, we'll need to use `master=\"yarn\"`.\n\nSpark needs to know where to look for YARN config files, so we need to set it:\n\n\n```bash\nexport HADOOP_HOME=\"${HOME}/spark/hadoop-3.2.3\"\nexport YARN_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\"\n```\n\nThen run Jupyter or use spark-submit.\n\n\n### Connecting Spark and YARN to GCS\n\nDownload the GCS connector:\n\n```bash\ngsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar .\n```\n\nConfig changes:\n\n* Change `${SPARK_HOME}/conf/spark-defaults.conf` (see [here]())\n* Change `${YARN_CONF_DIR}/core-site.xml` (see [here](config/core-site.xml))\n\nTemplate for hadoop properties:\n\n```xml\n  <property>\n    <name></name>\n    <value></value>\n  </property>\n```\n\n### Spark and YARN with Docker\n\nCopy the config from [here](https://hadoop.apache.org/docs/r3.2.3/hadoop-yarn/hadoop-yarn-site/DockerContainers.html)\n\nRunning spark-submit:\n\n```bash\nMOUNTS=\"$HADOOP_HOME:$HADOOP_HOME:ro,/etc/passwd:/etc/passwd:ro,/etc/group:/etc/group:ro\"\nIMAGE_ID=\"pyspark-docker:test\"\n\nspark-submit \\\n    --master yarn \\\n    --conf spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_TYPE=docker \\\n    --conf spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=${IMAGE_ID} \\\n    --conf spark.executorEnv.YARN_CONTAINER_RUNTIME_TYPE=docker \\\n    --conf spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=${IMAGE_ID} \\\n    06_spark_sql.py \\\n        --input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2021/*/ \\\n        --input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2021/*/ \\\n        --output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2021\n```\n\n\n\n### Sources\n\n* https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-common/SingleCluster.html\n* https://spark.apache.org/docs/latest/configuration.html#custom-hadoophive-configuration",
    "filename": "05-batch/setup/hadoop-yarn.md"
  },
  {
    "content": "## Linux\n\nHere we'll show you how to install Spark 3.3.2 for Linux.\nWe tested it on Ubuntu 20.04 (also WSL), but it should work\nfor other Linux distros as well\n\n\n### Installing Java\n\nDownload OpenJDK 11 or Oracle JDK 11 (It's important that the version is 11 - spark requires 8 or 11)\n\nWe'll use [OpenJDK](https://jdk.java.net/archive/)\n\nDownload it (e.g. to `~/spark`):\n\n```\nwget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n```\n\nUnpack it:\n\n```bash\ntar xzfv openjdk-11.0.2_linux-x64_bin.tar.gz\n```\n\ndefine `JAVA_HOME` and add it to `PATH`:\n\n```bash\nexport JAVA_HOME=\"${HOME}/spark/jdk-11.0.2\"\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\n```\n\ncheck that it works:\n\n```bash\njava --version\n```\n\nOutput:\n\n```\nopenjdk 11.0.2 2019-01-15\nOpenJDK Runtime Environment 18.9 (build 11.0.2+9)\nOpenJDK 64-Bit Server VM 18.9 (build 11.0.2+9, mixed mode)\n```\n\nRemove the archive:\n\n```bash\nrm openjdk-11.0.2_linux-x64_bin.tar.gz\n```\n\n### Installing Spark\n\n\nDownload Spark. Use 3.3.2 version:\n\n```bash\nwget https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n```\n\nUnpack:\n\n```bash\ntar xzfv spark-3.3.2-bin-hadoop3.tgz\n```\n\nRemove the archive:\n\n```bash\nrm spark-3.3.2-bin-hadoop3.tgz\n```\n\nAdd it to `PATH`:\n\n```bash\nexport SPARK_HOME=\"${HOME}/spark/spark-3.3.2-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\n```\n\n### Testing Spark\n\nExecute `spark-shell` and run the following:\n\n```scala\nval data = 1 to 10000\nval distData = sc.parallelize(data)\ndistData.filter(_ < 10).collect()\n```\n\n### PySpark\n\nIt's the same for all platforms. Go to [pyspark.md](pyspark.md).",
    "filename": "05-batch/setup/linux.md"
  },
  {
    "content": "## MacOS\n\nHere we'll show you how to install Spark 3.5.5 for MacOS.\nWe tested it on MacOS Monterey 12.0.1, but it should work\nfor other MacOS versions as well\n\n### Anaconda-based Spark set up\n\nIf you are having anaconda setup, you can skip the spark installation and instead Pyspark package to run the spark.\n\n#### Installing Java\n\nEnsure Brew and Java installed in your system:\n\n```bash\nxcode-select --install\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\nbrew install java\n```\n\nAdd the following environment variables to your `.bash_profile` or `.zshrc`:\n\n```bash\nexport JAVA_HOME=/usr/local/Cellar/openjdk@11/11.0.12\nexport PATH=\"$JAVA_HOME/bin/:$PATH\"\n```\n\nMake sure Java was installed to `/usr/local/Cellar/openjdk@11/11.0.12`: Open Finder > Press Cmd+Shift+G > paste \"/usr/local/Cellar/openjdk@11/11.0.12\". If you can't find it, then change the path location to appropriate path on your machine. You can also run `brew info java` to check where java was installed on your machine.\n\n#### Anaconda\n\nWith Anaconda and Mac we can spark set by first installing pyspark and then for environment variable set up findspark\n\nOpen Anaconda Activate the environment where you want to apply these changes\n\nRun pyspark and install it as a package in this environment <br>\nRun findspark and install it as a package in this environment\n\nEnsure that open JDK is already set up. This allows us to not have to install Spark separately and manually set up the environment Also with this we may have to use Jupyter Lab (instead of Jupyter Notebook) to open a Jupyter notebook for running the programs. \nOnce the Spark is set up start the conda environment and open Jupyter Lab. \nRun the program below in notebook to check everything is running fine.\n```\nimport pyspark\nfrom pyspark.sql import SparkSession\n\n!spark-shell --version\n\n# Create SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('test-spark') \\\n                    .getOrCreate()\n\nprint(f'The PySpark {spark.version} version is running...')\n```\n\n### Homebrew-based Spark set up\n#### Installing Spark\n\n1. Install Apache Spark. Java and Scala will be installed as Spark's dependencies.\n\n```bash\nbrew install apache-spark\n```\n\n2. Copy openjdk and apache-spark paths from installation output.\n\nYou may see something like this in your terminal after the installation is complete:\n```\n==> Pouring openjdk@17--17.0.14.arm64_sequoia.bottle.1.tar.gz\n\ud83c\udf7a  /opt/homebrew/Cellar/openjdk@17/17.0.14: 636 files, 304.2MB\n...\n==> Pouring apache-spark--3.5.5.all.bottle.tar.gz\n\ud83c\udf7a  /opt/homebrew/Cellar/apache-spark/3.5.5: 1,823 files, 423.7MB\n```\n\n3. Add environment variables: \n\nAdd the following environment variables to your `.bash_profile` or `.zshrc`. Replace the path to `JAVA_HOME` and `SPARK_HOME` to the paths on your own host. Run `brew info apache-spark` to get this.\n\n```bash\nexport JAVA_HOME=/opt/homebrew/Cellar/openjdk@17/17.0.14\nexport PATH=\"$JAVA_HOME/bin/:$PATH\"\n\nexport SPARK_HOME=/opt/homebrew/Cellar/apache-spark/3.5.5/libexec\nexport PATH=\"$SPARK_HOME/bin/:$PATH\"\n```\n\n\n#### Testing Spark\n\nExecute `spark-shell` and run the following in scala:\n\n```scala\nval data = 1 to 10000\nval distData = sc.parallelize(data)\ndistData.filter(_ < 10).collect()\n```\n\n\n### PySpark\n\nIt's the same for all platforms. Go to [pyspark.md](pyspark.md).",
    "filename": "05-batch/setup/macos.md"
  },
  {
    "content": "## PySpark\n\nThis document assumes you already have python.\n\nTo run PySpark, we first need to add it to `PYTHONPATH`:\n\n```bash\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH\"\n```\n\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will\nencounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n\nFor example, if the file under `${SPARK_HOME}/python/lib/` is `py4j-0.10.9.3-src.zip`, then the\n`export PYTHONPATH` statement above should be changed to\n\n```bash\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"\n```\n\nOn Windows, you may have to do path conversion from unix-style to windows-style:\n\n```bash\nSPARK_WIN=`cygpath -w ${SPARK_HOME}`\n\nexport PYTHONPATH=\"${SPARK_WIN}\\\\python\\\\\"\nexport PYTHONPATH=\"${SPARK_WIN}\\\\python\\\\lib\\\\py4j-0.10.9-src.zip;$PYTHONPATH\"\n```\n\nNow you can run Jupyter or IPython to test if things work. Go to some other directory, e.g. `~/tmp`.\n\nDownload a CSV file that we'll use for testing:\n\n```bash\nwget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n```\n\nNow let's run `ipython` (or `jupyter notebook`) and execute:\n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .csv('taxi_zone_lookup.csv')\n\ndf.show()\n```\n\nTest that writing works as well:\n\n```python\ndf.write.parquet('zones')\n```",
    "filename": "05-batch/setup/pyspark.md"
  },
  {
    "content": "## Windows\n\nHere we'll show you how to install Spark 3.3.2 for Windows.\nWe tested it on Windows 10 and 11 Home edition, but it should work\nfor other versions distros as well\n\nIn this tutorial, we'll use [MINGW](https://www.mingw-w64.org/)/[Gitbash](https://gitforwindows.org/) for command line\n\nIf you use WSL, follow the instructions from [linux.md](linux.md) \n\n\n### Installing Java\n\nSpark needs Java 11. Download it from here: [https://www.oracle.com/de/java/technologies/javase/jdk11-archive-downloads.html](https://www.oracle.com/de/java/technologies/javase/jdk11-archive-downloads.html). Select \u201cWindows x64 Compressed Archive\u201d (you may have to create an oracle account for that)\n\nUnpack it to a folder with no space in the path. We use `C:/tools` - so the full path to JDK is `/c/tools/jdk-11.0.13`\n\n\nNow let\u2019s configure it and add it to `PATH`:\n\n```bash\nexport JAVA_HOME=\"/c/tools/jdk-11.0.13\"\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\n```\n\nCheck that Java works correctly:\n\n```bash\njava --version\n```\n\nOutput:\n\n```\njava 11.0.13 2021-10-19 LTS\nJava(TM) SE Runtime Environment 18.9 (build 11.0.13+10-LTS-370)\nJava HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.13+10-LTS-370, mixed mode)\n```\n\n### Hadoop\n\nNext, we need to have Hadoop binaries. \n\nWe'll need Hadoop 3.2 which we'll get from [here](https://github.com/cdarlint/winutils/tree/master/hadoop-3.2.0).\n\nCreate a folder (`/c/tools/hadoop-3.2.0`) and put the files there \n\n```bash\nHADOOP_VERSION=\"3.2.0\"\nPREFIX=\"https://raw.githubusercontent.com/cdarlint/winutils/master/hadoop-${HADOOP_VERSION}/bin/\"\n\nFILES=\"hadoop.dll hadoop.exp hadoop.lib hadoop.pdb libwinutils.lib winutils.exe winutils.pdb\"\n\nfor FILE in ${FILES}; do\n  wget \"${PREFIX}/${FILE}\"\ndone\n```\n\nIf you don't have wget, you can use curl:\n\n```bash\nHADOOP_VERSION=\"3.2.0\"\nPREFIX=\"https://raw.githubusercontent.com/cdarlint/winutils/master/hadoop-${HADOOP_VERSION}/bin/\"\n\nFILES=\"hadoop.dll hadoop.exp hadoop.lib hadoop.pdb libwinutils.lib winutils.exe winutils.pdb\"\n\nfor FILE in ${FILES}; do\n  curl -o \"${FILE}\" \"${PREFIX}/${FILE}\";\ndone\n```\n\nAdd it to `PATH`:\n\n```bash\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\n```\n\n### Spark\n\nNow download Spark. Select version 3.3.2 \n\n```bash\nwget https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n```\n\n\nUnpack it in some location without spaces, e.g. `c:/tools/`: \n\n```bash\ntar xzfv spark-3.3.2-bin-hadoop3.tgz\n```\n\nLet's also add it to `PATH`:\n\n```bash\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\n```\n\n### Testing it\n\nGo to this directory\n\n```bash\ncd spark-3.3.2-bin-hadoop3\n```\n\nAnd run spark-shell:\n\n```bash\n./bin/spark-shell.cmd\n```\n\nAt this point you may get a message from windows firewall \u2014 allow it.\n\n\nThere could be some warnings (like this):\n\n```\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/tools/spark-3.3.2-bin-hadoop3/jars/spark-unsafe_2.12-3.3.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n```\n\nYou can safely ignore them.\n\nNow let's run this:\n\n```\nval data = 1 to 10000\nval distData = sc.parallelize(data)\ndistData.filter(_ < 10).collect()\n```\n\n### PySpark\n\nIt's the same for all platforms. Go to [pyspark.md](pyspark.md).",
    "filename": "05-batch/setup/windows.md"
  },
  {
    "content": "# Module 6: Stream Processing\n\n## Practical part: PyFlink\n\nIn this video, we show how to use PyFlink to consume data \nfrom a Kafka stream\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/P2loELMUUeI)](https://youtu.be/P2loELMUUeI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=51)\n\n\n* Code: [PyFlink examples](pyflink/)\n* [2025 Homework](../cohorts/2025/06-streaming/homework.md)\n\n\n## Theoretical part: Kafka + Java (optional)\n\nIn this set of videos, we cover Kafka and give examples in Java\n\nCode: [Java examples](java/)\n\n\n### Stream processing\n\n- [:movie_camera: 6.0.1 Introduction](https://youtu.be/hfvju3iOIP0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=67)\n- [:movie_camera: 6.0.2 What is stream processing](https://youtu.be/WxTxKGcfA-k&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=68)\n- [:movie_camera: 6.3  What is kafka?](https://youtu.be/zPLZUDPi4AY&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=69)\n- [:movie_camera: 6.4 Confluent cloud](https://youtu.be/ZnEZFEYKppw&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=70)\n- [:movie_camera: 6.5 Kafka producer consumer](https://youtu.be/aegTuyxX7Yg&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=71)\n- [:movie_camera: 6.6 Kafka configuration](https://youtu.be/SXQtWyRpMKs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=72)\n\nLinks:\n\n- [Slides](https://docs.google.com/presentation/d/1bCtdCba8v1HxJ_uMm9pwjRUC-NAMeB-6nOG2ng3KujA/edit?usp=sharing)\n- [Kafka Configuration Reference](https://docs.confluent.io/platform/current/installation/configuration/)\n- [Confluent cloud trial](https://www.confluent.io/confluent-cloud/tryfree/)\n\n### Kafka Streams\n\n- [:movie_camera: 6.7 Kafka stream basics](https://youtu.be/dUyA_63eRb0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=73)\n- [:movie_camera: 6.8 Kafka stream join](https://youtu.be/NcpKlujh34Y&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=74)\n- [:movie_camera: 6.9 Kafka stream testing](https://youtu.be/TNx5rmLY8Pk&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=75)\n- [:movie_camera: 6.10 Kafka stream windowing](https://youtu.be/r1OuLdwxbRc&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=76)\n- [:movie_camera: 6.11 Kafka ksqldb & Connect](https://youtu.be/DziQ4a4tn9Y&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=77)\n- [:movie_camera: 6.12 Kafka Schema registry](https://youtu.be/tBY_hBuyzwI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=78)\n\nLinks:\n\n- [Slides](https://docs.google.com/presentation/d/1fVi9sFa7fL2ZW3ynS5MAZm0bRSZ4jO10fymPmrfTUjE/edit?usp=sharing)  \n- [Streams Concepts](https://docs.confluent.io/platform/current/streams/concepts.html)\n\n\n### PySpark - Structured Streaming (optional)\n\nCode: [Python examples](python/)\n\nPlease follow the steps described under [pyspark-streaming](python/streams-example/pyspark/README.md)\n\n- [:movie_camera: 6.13 Kafka Streaming with Python](https://youtu.be/BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=79)\n- [:movie_camera: 6.14 Pyspark Structured Streaming](https://youtu.be/VIVr7KwRQmE&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80)\n\n\n## Other streaming resources\n\nKafka Streams with JVM library\n\n- [Confluent Kafka Streams](https://kafka.apache.org/documentation/streams/)\n- [Scala Example](https://github.com/AnkushKhanna/kafka-helper/tree/master/src/main/scala/kafka/schematest)\n\nKSQL and ksqlDB\n\n- [Introducing KSQL: Streaming SQL for Apache Kafka](https://www.confluent.io/blog/ksql-streaming-sql-for-apache-kafka/)\n- [ksqlDB](https://ksqldb.io/)\n\nKafka Connect\n\n- [Making Sense of Stream Data](https://medium.com/analytics-vidhya/making-sense-of-stream-data-b74c1252a8f5)\n\n\n\n## Community notes\n\nDid you take notes? You can share them here.\n\n* [Notes by Alvaro Navas](https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/6_streaming.md )\n* [Marcos Torregrosa's blog (spanish)](https://www.n4gash.com/2023/data-engineering-zoomcamp-semana-6-stream-processing/)\n* [Notes by Oscar Garcia](https://github.com/ozkary/Data-Engineering-Bootcamp/tree/main/Step6-Streaming)\n* [2024 videos transcript](https://drive.google.com/drive/folders/1UngeL5FM-GcDLM7QYaDTKb3jIS6CQC14?usp=drive_link) by Maria Fisher \n* [Notes by Shayan Shafiee Moghadam](https://github.com/shayansm2/eng-notebook/blob/main/kafka/readme.md)\n* Add your notes here (above this line)",
    "filename": "06-streaming/README.md"
  },
  {
    "code": false,
    "content": "# Documentation for `RideRecord` Class\n\n## Overview \n\nThe `RideRecord` class is an autogenerated Java class based on the Apache Avro framework. This class is part of the `schemaregistry` package and defines a schema for ride-sharing records, which includes details such as vendor ID, passenger count, and trip distance. The class implements serialization and deserialization functionalities to handle instances of `RideRecord` efficiently.\n\n## Schema Definition\n\nThe `RideRecord` class describes a record schema defined as follows:\n\n```json\n{\n  \"type\": \"record\",\n  \"name\": \"RideRecord\",\n  \"namespace\": \"schemaregistry\",\n  \"fields\": [\n    {\"name\": \"vendor_id\", \"type\": {\"type\": \"string\", \"avro.java.string\": \"String\"}},\n    {\"name\": \"passenger_count\", \"type\": \"int\"},\n    {\"name\": \"trip_distance\", \"type\": \"double\"}\n  ]\n}\n```\n\nHere, the fields are:\n- `vendor_id`: A string that represents the ID of the ride vendor.\n- `passenger_count`: An integer indicating the number of passengers.\n- `trip_distance`: A double representing the distance of the trip.\n\n## Class Structure\n\n### Fields and Constructors\n\nThe class contains three private fields: `vendor_id`, `passenger_count`, and `trip_distance`. There are two constructors defined:\n- **Default Constructor:** Initializes a `RideRecord` instance without setting any fields.\n- **All-Args Constructor:** Initializes the `RideRecord` with specified values for all fields.\n\n### Getters and Setters\n\nThe class includes getter and setter methods for each of the fields:\n- `getVendorId()` / `setVendorId(String value)`: Accesses and modifies the vendor ID.\n- `getPassengerCount()` / `setPassengerCount(int value)`: Accesses and modifies the count of passengers.\n- `getTripDistance()` / `setTripDistance(double value)`: Accesses and modifies the distance of the trip.\n\nThese methods allow for encapsulated access and modification of the class attributes.\n\n## Encoding and Decoding \n\n### Serialization\n\nThe class provides the `toByteBuffer()` method to serialize an instance of `RideRecord` to a ByteBuffer using a `BinaryMessageEncoder`. This allows for compact and efficient storage or transmission of the record over the network.\n\n### Deserialization\n\nThe static method `fromByteBuffer(ByteBuffer b)` allows for the reconstruction of a `RideRecord` instance from a ByteBuffer using a `BinaryMessageDecoder`. This facilitates reading records back into Java objects from a binary format.\n\n### Custom Encoding and Decoding\n\nThe class implements custom encoding and decoding methods (`customEncode()` and `customDecode()`) to support specific serialization logic with Avro's codec mechanisms. These methods define how to write and read the field data when encoding the object.\n\n## Builder Pattern\n\nThe `RideRecord` class also incorporates a Builder pattern through the static nested `Builder` class. This allows users to construct `RideRecord` instances step-by-step in a fluent manner. \n\n### Builder Methods\n\nThe `Builder` class includes methods for:\n- Setting field values (`setVendorId`, `setPassengerCount`, `setTripDistance`).\n- Getting current values (`getVendorId`, `getPassengerCount`, `getTripDistance`).\n- Checking if fields have been set (`hasVendorId`, `hasPassengerCount`, `hasTripDistance`).\n- Clearing fields when necessary (`clearVendorId`, `clearPassengerCount`, `clearTripDistance`).\n\nThe final step in the building process is calling `build()`, which constructs and returns a fully populated `RideRecord` instance.\n\n## Schema Management\n\n### Schema Access\n\nThe class provides methods such as `getClassSchema()` to retrieve the Avro schema associated with `RideRecord`. This can be useful for validating instances or for debugging.\n\n### Encoder and Decoder Retrieval\n\nStatic methods `getEncoder()` and `getDecoder()` allow access to the instance of `BinaryMessageEncoder` and `BinaryMessageDecoder`, respectively, being utilized by the `RideRecord`. This centralized management simplifies encoding and decoding operations across possibly multiple instances of `RideRecord`.\n\n## Exception Handling\n\nThroughout its methods, the class handles various I/O exceptions (specifically `java.io.IOException`). Care is taken to check for potential issues during serialization and deserialization. If any problems arise, they result in an IOException or an Avro runtime exception. \n\n## Conclusion \n\nThe `RideRecord` class effectively encapsulates data related to ride-sharing records, providing both simplicity in user access through getter/setter methods and advanced features such as serialization, deserialization, and a builder pattern for instance creation. The design benefits from Avro's capabilities for efficient data encoding and schema evolution while managing the intricacies of data representation in a clear and maintainable way.",
    "filename": "06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java"
  },
  {
    "code": false,
    "content": "# Documentation for `RideRecordCompatible` Class\n\n## Overview\n\nThe `RideRecordCompatible` class is an autogenerated Java class for managing ride records in an Avro-compatible manner. It implements the `SpecificRecord` interface, which allows it to interact with Avro serialization and deserialization processes efficiently. This class is part of the `schemaregistry` package and is designed to represent a record containing various details about a ride, such as vendor ID, passenger count, trip distance, and pickup location ID.\n\n## Class Structure\n\n### Fields\n\nThe class contains the following fields as defined by its Avro schema:\n\n- `vendorId`: A string representing the ID of the vendor providing the ride service.\n- `passenger_count`: An integer denoting the number of passengers involved in the ride.\n- `trip_distance`: A double indicating the distance of the trip.\n- `pu_location_id`: A nullable long representing the pickup location ID.\n\nThese fields are encapsulated as private variables within the class and are accessed via getter and setter methods.\n\n### Schema Definition\n\nThe schema for the `RideRecordCompatible` is represented in Avro format as follows:\n\n```json\n{\n  \"type\": \"record\",\n  \"name\": \"RideRecordCompatible\",\n  \"namespace\": \"schemaregistry\",\n  \"fields\": [\n    {\"name\": \"vendorId\", \"type\": {\"type\": \"string\", \"avro.java.string\": \"String\"}},\n    {\"name\": \"passenger_count\", \"type\": \"int\"},\n    {\"name\": \"trip_distance\", \"type\": \"double\"},\n    {\"name\": \"pu_location_id\", \"type\": [\"null\", \"long\"], \"default\": null}\n  ]\n}\n```\n\n## Serialization and Deserialization\n\n### Encoder and Decoder\n\nThe class includes static instances for encoding and decoding:\n\n- `ENCODER`: A `BinaryMessageEncoder` that converts `RideRecordCompatible` instances into a byte buffer for serialization.\n- `DECODER`: A `BinaryMessageDecoder` used to convert byte buffers back into `RideRecordCompatible` instances for deserialization.\n\nThese static methods provide convenient access to encoding and decoding functionality, ensuring efficient data exchange and storage.\n\n### Conversion Methods\n\n- **toByteBuffer**: Serializes the current object into a `ByteBuffer` for data transmission or storage.\n  \n  ```java\n  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException\n  ```\n\n- **fromByteBuffer**: Deserializes a `RideRecordCompatible` object from the provided `ByteBuffer`.\n  \n  ```java\n  public static RideRecordCompatible fromByteBuffer(java.nio.ByteBuffer b) throws java.io.IOException\n  ```\n\n## Constructors\n\n### Default Constructor\n\n- **RideRecordCompatible()**: Initializes a new instance without setting any attributes.\n\n### All-Args Constructor\n\n- **RideRecordCompatible(String vendorId, Integer passenger_count, Double trip_distance, Long pu_location_id)**: Initializes a new instance with the specified values for each field.\n\n## Field Access Methods\n\nThe class provides getter and setter methods for each field to facilitate access and modification of the field values:\n\n```java\npublic String getVendorId()\npublic void setVendorId(String value)\n\npublic int getPassengerCount()\npublic void setPassengerCount(int value)\n\npublic double getTripDistance()\npublic void setTripDistance(double value)\n\npublic Long getPuLocationId()\npublic void setPuLocationId(Long value)\n```\n\n## RecordBuilder\n\nThe `Builder` class inside `RideRecordCompatible` follows the Builder design pattern to enable fluent construction of `RideRecordCompatible` instances. The Builder class provides methods for setting each field, alongside validation mechanisms to ensure that values comply with the defined Avro schema.\n\n### Builder Methods\n\n- **newBuilder()**: Static method to create a new `Builder` instance.\n  \n- **setVendorId, setPassengerCount, setTripDistance, setPuLocationId**: Methods for setting each field within the builder.\n\n- **build()**: Constructs a new `RideRecordCompatible` instance reflecting the state of the builder.\n\n## Custom Encoding and Decoding\n\n### Custom Encode\n\nThe class implements custom encoding methods to manage how data is serialized:\n\n```java\npublic void customEncode(org.apache.avro.io.Encoder out) throws java.io.IOException\n```\n\n### Custom Decode\n\nIt also implements custom decoding to manage data reconstruction from serialized format:\n\n```java\npublic void customDecode(org.apache.avro.io.ResolvingDecoder in) throws java.io.IOException\n```\n\nThese methods provide detailed control over how the class data is processed during serialization and deserialization, ensuring that it adheres to Avro's encoding expectations while allowing for the nullability of certain fields.\n\n## Conclusion\n\nThe `RideRecordCompatible` class is a comprehensive representation of ride records designed for interoperability within Avro-based systems. It includes functionalities for serialization, deserialization, validation, and controlled field access, making it a robust choice for managing ride-related data in distributed systems and applications. The use of the Builder pattern facilitates easy creation and modification of ride records while maintaining structural integrity as defined by its Avro schema.",
    "filename": "06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java"
  },
  {
    "code": false,
    "content": "# RideRecordNoneCompatible Class Documentation\n\n## Overview\nThe `RideRecordNoneCompatible` class is an autogenerated Java representation of an Avro schema for ride-sharing data. This class encapsulates three main fields: `vendorId`, `passenger_count`, and `trip_distance`, facilitating serialization and deserialization of ride records in a binary format using the Apache Avro framework. \n\n## Package Information\nThe class resides in the `schemaregistry` package and is designed to be non-editable directly, as indicated by the autogenerated comment at the beginning of the file.\n\n## Schema Definition\n```java\npublic static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse(\"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"RideRecordNoneCompatible\\\",\\\"namespace\\\":\\\"schemaregistry\\\",\\\"fields\\\":[{\\\"name\\\":\\\"vendorId\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"passenger_count\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"trip_distance\\\",\\\"type\\\":\\\"double\\\"}]}\");\n```\nThis line defines the Avro schema for the `RideRecordNoneCompatible`, specifying that it contains three fields:\n- `vendorId` (int)\n- `passenger_count` (int)\n- `trip_distance` (double)\n\n## Constructors\n### Default Constructor\n```java\npublic RideRecordNoneCompatible() {}\n```\nThis constructor initializes a new instance of `RideRecordNoneCompatible` without setting any field values.\n\n### All-Args Constructor\n```java\npublic RideRecordNoneCompatible(java.lang.Integer vendorId, java.lang.Integer passenger_count, java.lang.Double trip_distance) {\n    ...\n}\n```\nThis constructor initializes a new instance with specified values for `vendorId`, `passenger_count`, and `trip_distance`.\n\n## Serialization and Deserialization\nThe class provides methods to serialize and deserialize ride record instances using Avro's binary message encoding.\n\n### Serialization\n```java\npublic java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {\n    return ENCODER.encode(this);\n}\n```\nThis method converts the ride record into a `ByteBuffer` representation, which can be transmitted or stored.\n\n### Deserialization\n```java\npublic static RideRecordNoneCompatible fromByteBuffer(java.nio.ByteBuffer b) throws java.io.IOException {\n    return DECODER.decode(b);\n}\n```\nThis static method takes a `ByteBuffer` containing a serialized ride record and converts it back into a `RideRecordNoneCompatible` instance.\n\n## Field Accessors\nThe class provides getter and setter methods to access and modify the values of its fields:\n\n### Vendor ID\n```java\npublic int getVendorId() { ... }\npublic void setVendorId(int value) { ... }\n```\n- Retrieves and sets the `vendorId` field.\n\n### Passenger Count\n```java\npublic int getPassengerCount() { ... }\npublic void setPassengerCount(int value) { ... }\n```\n- Retrieves and sets the `passenger_count` field.\n\n### Trip Distance\n```java\npublic double getTripDistance() { ... }\npublic void setTripDistance(double value) { ... }\n```\n- Retrieves and sets the `trip_distance` field.\n\n## Builder Pattern\nThe class implements a Builder pattern to facilitate the creation of `RideRecordNoneCompatible` instances:\n\n### Builder Class\n```java\npublic static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<RideRecordNoneCompatible> { ... }\n```\nThis inner class provides a fluent interface for constructing `RideRecordNoneCompatible` instances. It includes methods for setting field values and checking if they are set.\n\n### Builder Methods\nExample methods within the Builder include:\n```java\npublic Builder setVendorId(int value) { ... }\npublic Builder setPassengerCount(int value) { ... }\npublic Builder setTripDistance(double value) { ... }\n```\nEach method facilitates the setting of a respective field and returns the `Builder` instance for chaining.\n\n## Custom Encoding and Decoding\nThe class overrides methods for custom encoding and decoding, allowing for fine-grained control over data serialization.\n\n### Custom Encode\n```java\n@Override public void customEncode(org.apache.avro.io.Encoder out) throws java.io.IOException { ... }\n```\nThis method specifies how to write the `RideRecordNoneCompatible` fields to an Avro encoder.\n\n### Custom Decode\n```java\n@Override public void customDecode(org.apache.avro.io.ResolvingDecoder in) throws java.io.IOException { ... }\n```\nThis method specifies how to read the fields from an Avro decoder, handling the order of fields as necessary.\n\n## Conclusion\nThe `RideRecordNoneCompatible` class provides an effective mechanism to represent ride-sharing records, adhering to a well-defined schema while offering serialization, deserialization, and flexible instance creation through the Builder pattern. This makes it optimal for use in applications dealing with ride-sharing data utilizing the Apache Avro framework for efficient data exchange and storage.",
    "filename": "06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java"
  },
  {
    "code": false,
    "content": "# AvroProducer Documentation\n\n## Overview\nThe `AvroProducer` class is designed to read ride data from a CSV file, serialize it to Avro format, and then publish it to a Kafka topic. It utilizes the OpenCSV library for reading the CSV and the Kafka client libraries for producing messages to the Kafka topic. The use of Avro ensures that the data adheres to a defined schema, which can facilitate data integrity and interoperability across different systems consuming the messages.\n\n## Dependencies\nThe class relies on several external libraries:\n\n- **OpenCSV**: A library for reading and writing CSV files.\n- **Kafka Clients**: Libraries for producing messages to Kafka, including configuration for security using SASL_SSL.\n- **Confluent Kafka Avro Serializer**: For serializing the messages into Avro format, which is crucial when interacting with a schema registry.\n\n## Initialization\n### Constructor\nThe constructor of the `AvroProducer` class initializes a `Properties` object required for configuring the Kafka producer. It sets the following parameters:\n\n- **Bootstrap Servers**: The address of the Kafka cluster.\n- **Security Protocol**: Ensures secure communication using SSL.\n- **SASL Configuration**: Provides credentials for authentication to access Kafka and the Schema Registry.\n- **Producer Configurations**: Defines how data is serialized and acknowledged.\n\nThese configurations ensure that the `AvroProducer` can securely communicate with Kafka and properly serialize messages in Avro format.\n\n## Reading Ride Data\n### Method: `getRides()`\nThe `getRides()` method is responsible for reading ride data from a CSV file located in the resources. It performs the following tasks:\n\n- Accesses the `rides.csv` file as a resource.\n- Skips the header row of the CSV file using `CSVReader`.\n- Reads all remaining rows, mapping each row into a `RideRecord` object, which is a structured data type representing the rides.\n\nThe fields filled in the `RideRecord` include:\n\n- `vendorId`: Extracted from the CSV file to represent the ride's vendor.\n- `tripDistance`: Converted to a `Double` from the CSV.\n- `passengerCount`: Parsed as an `Integer`.\n\nThe method returns a `List<RideRecord>` containing all the ride records read from the CSV.\n\n## Publishing Ride Data\n### Method: `publishRides(List<RideRecord> rides)`\nThe `publishRides()` method sends the list of `RideRecord` objects to the Kafka topic \"rides_avro\". The method works as follows:\n\n- Creates a new instance of `KafkaProducer` using the previously set properties.\n- Iterates through each `RideRecord`, sending it as a `ProducerRecord` where the key is the `vendorId` and the value is the `RideRecord`.\n- Uses a callback to log any errors that occur during sending.\n- Prints the offset of each successfully sent record to the console.\n- Introduces a small delay (500 milliseconds) between sending records, to manage production rates and reduce load.\n\nThis method handles the core functionality of producing messages to Kafka, ensuring reliable communication and acknowledgment of sent messages.\n\n## Execution Flow\n### Method: `main(String[] args)`\nThe `main()` method serves as the entry point for the application. It orchestrates the overall flow of the application:\n\n1. **Instantiation**: It creates an instance of `AvroProducer`.\n2. **Reading Rides**: Calls `getRides()` to fetch the ride records from the CSV.\n3. **Publishing to Kafka**: Passes the retrieved `RideRecord` list to `publishRides()` to send them to Kafka.\n\nThis straightforward flow encapsulates the functionality of the program, transforming CSV data into Kafka messages for further processing.\n\n## Conclusion\nThe `AvroProducer` class effectively bridges the gap between static ride data contained in a CSV file and dynamic stream processing through Kafka. By leveraging Avro for serialization and following a structured approach to error handling and message acknowledgment, it ensures efficient and reliable data transfer. This design can be extended or modified for other data sources or target topics with minimal adjustments to the existing methods.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java"
  },
  {
    "code": false,
    "content": "# JsonConsumer Class Documentation\n\n## Overview\nThe `JsonConsumer` class is designed to consume JSON messages from an Apache Kafka topic named \"rides\". It utilizes the Kafka client library and Confluent's Kafka JSON deserializers to process messages related to ride locations. The consumer connects to a Kafka cluster via a secured SASL_SSL connection and listens for messages which are then printed to the console.\n\n## Dependencies\n- **Kafka Clients**: The class utilizes the Kafka client library for consuming messages.\n- **Confluent Serializers**: Used for deserializing JSON messages into Java objects (specifically the `Ride` class).\n- **Ride Class**: Represents the structure of the data being consumed. The properties of the `Ride` class must include at least `DOLocationID` as this value is printed in the consuming process.\n\n## Configuration Properties\n### Kafka Consumer Configuration\nThe constructor initializes a set of properties for the Kafka consumer:\n- **BOOTSTRAP_SERVERS_CONFIG**: Specifies the Kafka broker address to connect to.\n- **security.protocol**: Configures the security protocol used for the connection.\n- **sasl.jaas.config**: Contains the configuration for SASL authentication, including username and password.\n- **sasl.mechanism**: Defines the mechanism used for SASL authentication (in this case, \"PLAIN\").\n- **client.dns.lookup**: Sets DNS lookup behavior for the client.\n- **session.timeout.ms**: Configures the timeout for session expiry.\n- **KEY_DESERIALIZER_CLASS_CONFIG**: Specifies the deserializer used for the message key (in this case, a string).\n- **VALUE_DESERIALIZER_CLASS_CONFIG**: Specifies the deserializer for the message value, which uses a JSON deserializer to convert the incoming messages to the `Ride` class.\n- **GROUP_ID_CONFIG**: Defines the consumer group ID for managing offsets.\n- **AUTO_OFFSET_RESET_CONFIG**: Configures where to start consuming messages when no offset is committed (set to \"earliest\").\n- **JSON_VALUE_TYPE**: Indicates that the values being deserialized are of the type `Ride`.\n\n## Consumer Initialization\nOnce the properties are set in the constructor, a `KafkaConsumer<String, Ride>` instance is created. The consumer subscribes to the \"rides\" topic, preparing it to listen for messages from that topic.\n\n## Message Consumption\n### consumeFromKafka Method\nThe `consumeFromKafka` method is responsible for polling messages from the Kafka topic. \n\n1. **Start Message**: It prints a message to the console indicating that the consumption has started.\n2. **Polling Loop**: It polls the Kafka broker for messages with a timeout of 1 second. \n    - The loop continues until either the returned results are empty or it has completed 10 iterations.\n    - **Processing Results**: For each `ConsumerRecord` obtained in results, it extracts the `DOLocationID` from the `Ride` object and prints it. \n    - Following the processing of results, the method polls again, keeping track of how many times it has polled with a counter (`i`).\n\n## Main Method\nThe `main` method acts as the entry point of the application. It creates an instance of the `JsonConsumer` class and invokes the `consumeFromKafka` method to start consuming messages.\n\n## Summary\nThe `JsonConsumer` class is a simple Kafka consumer that connects to a secured Kafka cluster and consumes ride-related JSON messages. It is structured to poll the Kafka topic for messages continuously and prints specific data (like `DOLocationID`) to the console, allowing for real-time monitoring of ride information. The configuration is done dynamically in the constructor, making it versatile for various Kafka setups, while the consumption logic ensures it runs within controlled limits.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/JsonConsumer.java"
  },
  {
    "code": false,
    "content": "# JsonKStream Class Documentation\n\n## Overview\n\nThe `JsonKStream` class is an implementation of a Kafka Streams application that reads from a Kafka topic, processes data, and writes the results to another topic. Specifically, it counts the occurrences of records in the `rides` topic based on their keys and publishes these counts to the `rides-pulocation-count` topic. The application makes use of Apache Kafka for its pub-sub messaging capabilities and streams processing.\n\n## Configuration Properties\n\n### Kafka Streams Configuration\n\nIn the constructor of the `JsonKStream` class, various properties are defined to configure the Kafka Streams application. These properties include:\n\n- **Bootstrap Servers**: Specifies the Kafka broker addresses.\n- **Security Protocol**: Configured to use secure connections (SASL_SSL).\n- **JAAS Configuration**: Contains the credentials (username and password) for connecting to the Kafka cluster using the `PlainLoginModule`.\n- **Client DNS Lookup**: Set to use all DNS IPs for better resilience in service discovery.\n- **Session Timeout**: Configured to define the session expiration time.\n- **Application ID**: Identifies the Kafka Streams application.\n- **Auto Offset Reset**: Specifies behavior for consumers when there is no initial offset.\n- **Cache Size**: Set to zero, indicating that no caching is used for the stream processing.\n\nThis configuration is crucial for setting up the Kafka Streams environment and ensuring secure communication with the Kafka cluster.\n\n## Topology Creation\n\nThe `createTopology()` method is responsible for defining the processing logic for the Kafka Streams application. It does the following:\n\n1. **StreamsBuilder Initialization**: A `StreamsBuilder` instance is created to facilitate the definition of stream processing logic.\n2. **Stream Ingestion**: The application subscribes to the `rides` topic, specifying that it expects key-value pairs where the key is a `String` and the value is a custom serialized `Ride` object.\n3. **Key Grouping and Counting**: The incoming ride data is grouped by key, and the count of occurrences for each key is computed.\n4. **Stream Output**: The counted results are transformed into a stream and published to the `rides-pulocation-count` topic, where the key is a `String` and the value is a `Long` representing the count.\n\nThis method defines the core capabilities of the Kafka Streams application.\n\n## Counting PLocations\n\nThe primary method of the class is `countPLocation()`, which manages the lifecycle of the Kafka Streams application. This function carries out the following steps:\n\n1. **Topology Creation**: It calls the `createTopology()` method to retrieve the defined topology.\n2. **KafkaStreams Instance**: A `KafkaStreams` instance is created using the produced topology and the previously defined configuration properties.\n3. **Starting Streams**: The Kafka Streams application is started and will enter the RUNNING state.\n4. **State Monitoring**: A loop continuously checks and prints the current state of the Kafka Streams application until it is RUNNING, with a delay of one second between checks.\n5. **Shutdown Hook**: A shutdown hook is registered to ensure the Kafka Streams application closes gracefully when the application is terminated.\n\nThis method encompasses the operational mechanism of the application, allowing it to process incoming messages and track its state effectively.\n\n## Main Method\n\nThe `main` method serves as the entry point to the application. It performs the following:\n\n1. **Object Instantiation**: A new instance of `JsonKStream` is created.\n2. **Count Initialization**: It invokes the `countPLocation()` method, initiating the Kafka Streams processing.\n\nThe main method effectively kick-starts the application's execution in a straightforward way.\n\n## Dependencies\n\nThe class relies on several libraries and frameworks:\n\n- **Apache Kafka**: For streaming and handling pub-sub messaging.\n- **Custom Serializers/Deserializers**: It mentions a custom `CustomSerdes` class, which presumably handles the serialization/deserialization of the `Ride` objects.\n- **Data Class**: Uses a `Ride` class, which is expected to represent the data structure of the incoming messages.\n\nBy using these dependencies, the application leverages powerful data processing capabilities provided by the Kafka ecosystem.\n\n## Exception Handling\n\nThe `countPLocation` method is declared to throw `InterruptedException`, indicating that thread interruptions are anticipated during sleep or state checking. This also implies that the invoking method will need to handle such exceptions appropriately, ensuring stability in the thread management of this Kafka Streams application.\n\n## Conclusion\n\nThe `JsonKStream` class exemplifies a basic yet effective Kafka Streams application, managing the ingestion, processing, and output of streaming data. By leveraging Apache Kafka's robust stream processing capabilities, it performs key-based counting of ride events, making it suitable for scenarios like real-time analytics in transportation services. The structure and configuration of this class can be extended or modified to fit additional processing requirements as needed.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java"
  },
  {
    "code": false,
    "content": "# JsonKStreamJoins Documentation\n\n## Overview\n\nThe `JsonKStreamJoins` class is designed to process data streams from Kafka topics, specifically for joining ride and pickup location data. It leverages the Kafka Streams API to perform the joining operation and outputs the results to a designated Kafka topic. The implementation focuses on ensuring that the data is only combined under certain temporal conditions, using custom serializers and handling exceptions robustly.\n\n## Properties Configuration\n\n### Kafka Streams Configuration\n\nThe constructor of the `JsonKStreamJoins` class initializes a set of properties necessary for connecting to a Kafka cluster:\n\n- **Bootstrap Servers**: Points to the Kafka broker (`pkc-75m1o.europe-west3.gcp.confluent.cloud:9092`).\n- **Security Protocol**: Configured to use `SASL_SSL`.\n- **Authentication**: Uses JAAS for authentication, where the username and password are fetched from a `Secrets` class.\n- **Client Configuration**: Various settings, such as DNS lookup strategies and session timeout.\n- **Application ID**: Uniquely identifies the Kafka Streams application.\n- **Offset Reset Policy**: Configured to read the latest messages if no offsets are found.\n- **Buffering Configuration**: Set to disable the maximum bytes buffering.\n\n## Topology Creation\n\n### Stream Processing\n\nThe `createTopology` method is responsible for defining the data flow for the Kafka Streams application:\n\n1. **Stream Sources**: It creates two input streams:\n   - `rides`: Processes data from the `INPUT_RIDE_TOPIC`, deserialized into `Ride` objects using a custom serializer.\n   - `pickupLocations`: Processes data from the `INPUT_RIDE_LOCATION_TOPIC`, deserialized into `PickupLocation` objects.\n\n2. **Key Selection**: The `pickupLocations` stream is then re-keyed based on the `PULocationID`, which is intended for joining with rides.\n\n3. **Joining Streams**: \n   - A join operation between `rides` and `pickupLocationsKeyedOnPUId` is established. The `join` uses a `ValueJoiner` that checks if the difference between the drop-off and pickup times is within 10 minutes.\n   - If the condition is met, a new `VendorInfo` instance is created, containing the ride and location information.\n\n4. **Windowing**: The joining operation uses a time window of 20 minutes and a grace period of 5 minutes.\n\n5. **Filtering and Output**: After the join, the resulting stream is filtered to exclude any `Optional` values that are empty. The final results, containing `VendorInfo`, are sent to the `OUTPUT_TOPIC`.\n\n## Stream Execution\n\n### Kafka Streams Lifecycle Management\n\nThe `joinRidesPickupLocation` method executes and manages the Kafka Streams lifecycle:\n\n1. **Topology Building**: Calls the `createTopology` method to build the processing topology.\n2. **Stream Initialization**: Constructs a `KafkaStreams` instance with the defined topology and properties.\n3. **Exception Handling**: Sets an uncaught exception handler that will log any exceptions and shut down the application gracefully if errors occur.\n4. **Starting the Stream**: Initiates the stream and checks its state in a loop until it transitions to the `RUNNING` state.\n5. **Shutdown Hook**: Adds a shutdown hook to ensure the stream is closed properly on application termination.\n\n## Entry Point\n\n### Main Method\n\nThe `main` method serves as the entry point for running the `JsonKStreamJoins` application:\n\n1. An instance of `JsonKStreamJoins` is created.\n2. The `joinRidesPickupLocation` method is called, starting the stream processing.\n\nThis encapsulative flow is typical in Kafka Streams applications, where application logic is housed in specific classes, and the main method initializes the execution context.\n\n## Conclusion\n\nThe `JsonKStreamJoins` class exemplifies the use of Kafka Streams for real-time data processing by joining ride data with pickup location data. Utilizing proper configurations for security, custom serialization, and meticulous handling of stream state, this class serves as a robust framework for applications needing to perform on-the-fly joins of temporal datasets. The structured exception handling and seamless integration with Kafka topics make it a suitable solution for scalable data processing tasks.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java"
  },
  {
    "code": false,
    "content": "# Documentation for JsonKStreamWindow\n\n## Overview\nThe `JsonKStreamWindow` class is designed for processing streaming data from a Kafka topic using the Kafka Streams API. It specifically counts the occurrences of rides grouped by their pickup locations (PLocation) in a windowed manner. This Java application connects to a Kafka cluster, consumes ride data, performs aggregation over a defined time window, and then outputs the results to another Kafka topic.\n\n## Imports and Dependencies\nThe code imports various classes from the Kafka Streams library, including configurations, serializers/deserializers, and utilities for processing streams. It also imports custom serialization and data types specific to the application's backend setup.\n\n```java\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.TimeWindows;\nimport org.apache.kafka.streams.kstream.WindowedSerdes;\nimport org.example.customserdes.CustomSerdes;\nimport org.example.data.Ride;\n```\n\n## Class Definition\n### `JsonKStreamWindow`\nThe main class, `JsonKStreamWindow`, initializes a set of properties required for configuring the Kafka Streams application. It provides methods to create the processing topology and execute the stream processing.\n\n### Properties\nThe `props` variable is a `Properties` object that contains key-value pairs for configuring various aspects of the Kafka Streams application, such as bootstrap servers, security protocols, and session timeouts.\n\n## Constructor\n### `JsonKStreamWindow()`\nThe constructor initializes the properties for the Kafka Streams library. Key attributes include:\n\n- **Bootstrap Servers**: Specifies the Kafka server's address.\n- **Security Protocol and JAAS Config**: Sets up SASL_SSL security with credentials to connect to the Kafka cluster securely.\n- **Application ID**: A unique identifier for the application instance.\n- **Consumer Auto Offset Reset**: Dictates the behavior for offset management.\n- **Cache Configuration**: Sets the maximum bytes of buffering for caching in the stream process.\n\n```java\npublic JsonKStreamWindow() {\n    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"pkc-75m1o.europe-west3.gcp.confluent.cloud:9092\");\n    props.put(\"security.protocol\", \"SASL_SSL\");\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='\"+Secrets.KAFKA_CLUSTER_KEY+\"' password='\"+Secrets.KAFKA_CLUSTER_SECRET+\"';\");\n    props.put(\"sasl.mechanism\", \"PLAIN\");\n    props.put(\"client.dns.lookup\", \"use_all_dns_ips\");\n    props.put(\"session.timeout.ms\", \"45000\");\n    props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kafka_tutorial.kstream.count.plocation.v1\");\n    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"latest\");\n    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n}\n```\n\n## Topology Creation\n### `createTopology()`\nThis method constructs the processing topology. It uses a `StreamsBuilder` to define the flow of data processing:\n\n1. **Stream Definition**: It reads messages from the `rides` topic, using the custom serializer/deserializer for `Ride` objects.\n2. **Windowed Grouping and Counting**: The stream is grouped by their key and processed in time windows of 10 seconds, with a grace period of 5 seconds for late arriving data. The count of rides in each window is calculated.\n3. **Output Stream**: The result of the counting operation is sent to the `rides-pulocation-window-count` topic using a specified serializer for the window key and a long integer for the count.\n\n```java\npublic Topology createTopology() {\n    StreamsBuilder streamsBuilder = new StreamsBuilder();\n    var ridesStream = streamsBuilder.stream(\"rides\", Consumed.with(Serdes.String(), CustomSerdes.getSerde(Ride.class)));\n    var puLocationCount = ridesStream.groupByKey()\n            .windowedBy(TimeWindows.ofSizeAndGrace(Duration.ofSeconds(10), Duration.ofSeconds(5)))\n            .count().toStream();\n    var windowSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class, 10*1000);\n\n    puLocationCount.to(\"rides-pulocation-window-count\", Produced.with(windowSerde, Serdes.Long()));\n    return streamsBuilder.build();\n}\n```\n\n## Stream Processing\n### `countPLocationWindowed()`\nThis method initializes the Kafka Streams application:\n\n1. **Create Topology**: Invokes the `createTopology()` method to define the processing logic.\n2. **Start KafkaStreams Instance**: Creates a `KafkaStreams` object with the defined topology and properties, then calls `start()` to begin processing.\n3. **Shutdown Hook**: Registers a shutdown hook to cleanly close the Kafka Streams instance on application termination.\n\n```java\npublic void countPLocationWindowed() {\n    var topology = createTopology();\n    var kStreams = new KafkaStreams(topology, props);\n    kStreams.start();\n\n    Runtime.getRuntime().addShutdownHook(new Thread(kStreams::close));\n}\n```\n\n## Main Method\n### `main(String[] args)`\nThe entry point for the application. It creates an instance of `JsonKStreamWindow` and invokes the method to start processing the ride data stream.\n\n```java\npublic static void main(String[] args) {\n    var object = new JsonKStreamWindow();\n    object.countPLocationWindowed();\n}\n```\n\n## Summary\nThe `JsonKStreamWindow` class encapsulates the functionality of a Kafka Streams application dedicated to counting ride occurrences by pickup location. It effectively leverages windowing to manage time-based stream processing, ensuring that data aggregation is performed on sliding time windows. The application is designed to be robust, with safety measures for connection and shutdown, making it suitable for production-like environments where real-time data processing is critical.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java"
  },
  {
    "code": false,
    "content": "# JSON Producer Documentation\n\n## Overview\n\nThe `JsonProducer` class in the provided code is designed for reading ride data from a CSV file and publishing it to a Kafka topic using the Apache Kafka producer. The program leverages the OpenCSV library to manage CSV file operations and applies Kafka\u2019s functionality to stream the ride data in JSON format.\n\n## Dependencies\n\nThis class depends on several libraries and frameworks:\n- **OpenCSV**: Used for reading CSV files.\n- **Apache Kafka**: Used to create a producer that sends ride data to a Kafka topic.\n- **Confluent Kafka Serializers**: For serializing the `Ride` objects into JSON format.\n\n## Properties and Configuration\n\n### Kafka Producer Configuration\n\nUpon instantiation of the `JsonProducer` class, it initializes Kafka producer configurations as follows:\n- **Bootstrap Servers**: Specifies the Kafka broker address for connection.\n- **Security Protocol**: Uses SSL protocol for secure communication.\n- **SASL Authentication**: Configures the security settings which include the mechanisms and credentials for authentication.\n- **Other Settings**: Includes configurations like session timeout, acknowledgments, and serializers for the key and value of the Kafka records.\n\n```java\nprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"pkc-75m1o.europe-west3.gcp.confluent.cloud:9092\");\nprops.put(\"security.protocol\", \"SASL_SSL\");\nprops.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='\"+Secrets.KAFKA_CLUSTER_KEY+\"' password='\"+Secrets.KAFKA_CLUSTER_SECRET+\"';\");\nprops.put(\"sasl.mechanism\", \"PLAIN\");\nprops.put(\"client.dns.lookup\", \"use_all_dns_ips\");\nprops.put(\"session.timeout.ms\", \"45000\");\nprops.put(ProducerConfig.ACKS_CONFIG, \"all\");\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\");\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"io.confluent.kafka.serializers.KafkaJsonSerializer\");\n```\n\n## Method Descriptions\n\n### `getRides()`\n\nThe `getRides` method is responsible for reading data from a `rides.csv` file, parsing it, and transforming it into a list of `Ride` objects. This method performs the following tasks:\n1. Accesses the `rides.csv` file from the classpath.\n2. Uses the `CSVReader` class to read the file, skipping the header.\n3. Maps each row of data into a `Ride` object and collects them into a list.\n\n```java\npublic List<Ride> getRides() throws IOException, CsvException {\n    var ridesStream = this.getClass().getResource(\"/rides.csv\");\n    var reader = new CSVReader(new FileReader(ridesStream.getFile()));\n    reader.skip(1);\n    return reader.readAll().stream().map(arr -> new Ride(arr))\n            .collect(Collectors.toList());\n}\n```\n\n### `publishRides(List<Ride> rides)`\n\nThis method takes a list of `Ride` objects and sends each ride to the Kafka topic named \"rides\". Here's what it does:\n1. Initializes a `KafkaProducer` with the previously defined properties.\n2. Iterates through each `Ride` object in the list.\n3. Updates the pickup and drop-off timestamps to the current time.\n4. Sends the `Ride` object as a record to the Kafka topic using `ProducerRecord`.\n5. Implements a callback to handle exceptions during sending.\n6. Logs the offset of the published record and the `DOLocationID` of the ride.\n7. Waits for 500 milliseconds between each publishing to avoid flooding the Kafka topic.\n\n```java\npublic void publishRides(List<Ride> rides) throws ExecutionException, InterruptedException {\n    KafkaProducer<String, Ride> kafkaProducer = new KafkaProducer<String, Ride>(props);\n    for(Ride ride: rides) {\n        ride.tpep_pickup_datetime = LocalDateTime.now().minusMinutes(20);\n        ride.tpep_dropoff_datetime = LocalDateTime.now();\n        var record = kafkaProducer.send(new ProducerRecord<>(\"rides\", String.valueOf(ride.DOLocationID), ride), (metadata, exception) -> {\n            if(exception != null) {\n                System.out.println(exception.getMessage());\n            }\n        });\n        System.out.println(record.get().offset());\n        System.out.println(ride.DOLocationID);\n        Thread.sleep(500);\n    }\n}\n```\n\n### `main(String[] args)`\n\nThe `main` method serves as the entry point of the application and performs the following actions:\n1. Creates an instance of the `JsonProducer`.\n2. Calls the `getRides` method to read and parse the ride data.\n3. Passes the list of rides to the `publishRides` method to publish the data to the Kafka topic.\n\n```java\npublic static void main(String[] args) throws IOException, CsvException, ExecutionException, InterruptedException {\n    var producer = new JsonProducer();\n    var rides = producer.getRides();\n    producer.publishRides(rides);\n}\n```\n\n## Error Handling\n\nThe code incorporates basic error handling with exception propagation:\n- The methods `getRides` and `publishRides` throw `IOException` and `ExecutionException` or `InterruptedException`, allowing these exceptions to be handled at a higher level.\n- Additionally, errors encountered during the sending of records to Kafka are handled within the `publishRides` method using a callback.\n\n## Conclusion\n\nThe `JsonProducer` class effectively encapsulates the functionality to read ride data from a CSV file and publish it to a Kafka topic using a structured approach. By leveraging libraries like OpenCSV and Kafka, it implements a straightforward pipeline suitable for data streaming applications.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java"
  },
  {
    "code": false,
    "content": "# Documentation for `JsonProducerPickupLocation`\n\n## Overview\nThe `JsonProducerPickupLocation` class is a Java program that integrates with Apache Kafka to publish messages about pickup locations. The primary function of this code is to create a Kafka producer, configure its properties, and use it to send `PickupLocation` objects to a specified Kafka topic. The integration utilizes the OpenCSV library for handling potential CSV exceptions and requires Apache Kafka and Confluent libraries for serialization.\n\n## Dependencies\nThe following libraries are utilized in this program:\n- **Apache Kafka**: For Kafka client functionalities, allowing the production of messages to Kafka topics.\n- **Confluent Kafka**: For handling JSON serialization of message values.\n- **OpenCSV**: For handling CSV-related exceptions, although the specific CSV functionality is not explicitly utilized in this code.\n\n## Class Definition\n### `JsonProducerPickupLocation`\nThe class `JsonProducerPickupLocation` handles the configuration and creation of a Kafka producer to send `PickupLocation` data.\n\n### Member Variables\n- **Properties `props`**: A `Properties` object that holds the configuration settings necessary for connecting to the Kafka broker.\n\n## Constructor\n### `JsonProducerPickupLocation()`\n- **Purpose**: Initializes the Kafka producer configuration properties.\n- **Configuration Settings**:\n  - **Bootstrap Servers**: Specifies the Kafka broker address.\n  - **Security Protocol**: Configures SASL over SSL for authentication.\n  - **JAAS Configuration**: Uses a plain login module for SASL authentication, retrieving credentials from a `Secrets` class (assumed to contain sensitive information).\n  - **DNS Lookup**: Ensures all DNS IPs are utilized for client connections.\n  - **Session Timeout**: Sets the timeout for the session connection to 45 seconds.\n  - **Acknowledgments Configuration**: Requires all in-sync replicas to acknowledge messages.\n  - **Serialization**: Specifies the serializers for the key and value of the Kafka messages.\n\n## Method Definitions\n### `public void publish(PickupLocation pickupLocation)`\n- **Inputs**: Expects a `PickupLocation` object.\n- **Purpose**: Publishes the provided `PickupLocation` instance to a Kafka topic named `\"rides_location\"`.\n- **Functionality**:\n  - Creates an instance of `KafkaProducer` using the configured properties.\n  - Sends the `PickupLocation` as a `ProducerRecord` with its `PULocationID` as the key.\n  - Utilizes a callback to log any exceptions that occur during sending.\n  - Outputs the offset of the message once successfully sent.\n\n## Main Method\n### `public static void main(String[] args)`\n- **Purpose**: Entry point for the application.\n- **Functionality**:\n  - Creates an instance of `JsonProducerPickupLocation`.\n  - Calls the `publish` method with a new `PickupLocation` object, which includes a pre-defined location ID and the current timestamp.\n- **Error Handling**: The main method handles potential IO exceptions, CSV exceptions, and execution interruptions during the operation.\n\n## Usage\nThe class can be utilized directly by running the `main` method, which will produce a single `PickupLocation` message to the Kafka topic. This could be integrated into a larger application that continuously produces `PickupLocation` messages in a real-time environment, such as a ride-sharing platform.\n\n## Considerations\n- The use of sensitive information such as Kafka credentials indicates that this class is meant for secured environments.\n- Proper exception handling is essential to manage any potential failures during message publishing.\n- Scaling the functionality to handle multiple `PickupLocation` messages might involve implementing a loop or a listening mechanism for real-time updates.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java"
  },
  {
    "code": false,
    "content": "# Documentation for the `Secrets` Class\n\n## Overview\n\nThe `Secrets` class is part of the package `org.example`. It is designed to store sensitive information such as credentials that are crucial for connecting to external services like a Kafka cluster and a schema registry. This class provides a centralized place to define constants for these keys and secrets.\n\n## Class Structure\n\n### Constant Fields\n\nThe class contains four public static final fields, which are constants used to hold the key and secret for both Kafka and the schema registry. Each of these constants needs to be replaced with actual values before use. Here\u2019s a breakdown of the fields:\n\n1. **KAFKA_CLUSTER_KEY**\n   - Purpose: This constant is intended to store the key required to authenticate against a Kafka cluster.\n   - Default Value: `\"REPLACE_WITH_YOUR_KAFKA_CLUSTER_KEY\"` indicates that this placeholder should be replaced with the actual Kafka cluster key at the time of implementation.\n\n2. **KAFKA_CLUSTER_SECRET**\n   - Purpose: This constant is designed to hold the secret associated with the Kafka cluster key for authentication purposes.\n   - Default Value: `\"REPLACE_WITH_YOUR_KAFKA_CLUSTER_SECRET\"` also serves as a placeholder needing to be replaced with the actual Kafka cluster secret.\n\n3. **SCHEMA_REGISTRY_KEY**\n   - Purpose: This constant is used to store the key for accessing a schema registry, which is essential for managing schemas when using systems like Kafka.\n   - Default Value: `\"REPLACE_WITH_SCHEMA_REGISTRY_KEY\"` is a reminder to substitute this with the actual schema registry key.\n\n4. **SCHEMA_REGISTRY_SECRET**\n   - Purpose: Similar to the above constants, this field stores the secret corresponding to the schema registry key for secure authentication.\n   - Default Value: `\"REPLACE_WITH_SCHEMA_REGISTRY_SECRET\"` emphasizes that the user must input an actual secret value.\n\n## Usage\n\n### Security Considerations\n\n- **Hardcoding Secrets**: It is important to note that hardcoding sensitive information like keys and secrets directly in the source code is not a secure practice. This approach increases the risk of exposing sensitive credentials when the code is shared or deployed. \n- **Best Practices**: It is recommended to utilize secure methods for managing and accessing these credentials, such as environment variables, configuration management tools, or secure vaults (e.g., HashiCorp Vault, AWS Secrets Manager).\n\n### Integration\n\nDevelopers utilizing this class should replace the placeholder values with their actual credentials before deployment. The structure allows for easy integration with Kafka clients and schema management tools within the application that utilizes the `Secrets` class.\n\n### Consistency and Convention\n\nDefining the keys and secrets as `public static final` ensures they are constants that can be accessed without instantiating the class. This approach maintains consistency and clarity when referring to authentication credentials throughout the application, enhancing overall code readability.\n\n## Conclusion\n\nThe `Secrets` class serves as a straightforward utility for managing critical authentication secrets needed to connect to external services like Kafka and schema registries. While it provides an organized structure for these credentials, developers must ensure secure practices for managing sensitive information are followed.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/Secrets.java"
  },
  {
    "code": false,
    "content": "# Documentation for `Topics` Class\n\n## Overview\nThe `Topics` class is part of the `org.example` package and serves as a constants holder for topic names used within a broader application context, likely relating to data processing or messaging systems such as Apache Kafka. It defines three public static final string constants that represent different topics.\n\n## Purpose\nThe primary role of the `Topics` class is to centralize and manage topic names that are used in the application. By defining these topics as constants, the class helps in maintaining consistency throughout the codebase, ensuring that the same names are referenced uniformly, which minimizes errors related to string literals spread across the code.\n\n## Constants\n\n### 1. `INPUT_RIDE_TOPIC`\n```java\npublic static final String INPUT_RIDE_TOPIC = \"rides\";\n```\n- **Description**: This constant holds the name of the input topic for ride data. It suggests that the application is likely processing information related to transportation or ride-sharing services.\n- **Use Case**: Components of the application that require access to the ride data would refer to this constant to ensure they are subscribing or publishing to the correct topic without hardcoding the string \"rides\".\n\n### 2. `INPUT_RIDE_LOCATION_TOPIC`\n```java\npublic static final String INPUT_RIDE_LOCATION_TOPIC = \"rides_location\";\n```\n- **Description**: This constant contains the name of the input topic for ride location data. This indicates a focus on geographical aspects of the ride, such as starting points, destinations, or GPS coordinates.\n- **Use Case**: Similar to the previous constant, this one aids in consistently referring to the ride location data topic across the application, aiding data ingestion or processing components that rely on location-specific information.\n\n### 3. `OUTPUT_TOPIC`\n```java\npublic static final String OUTPUT_TOPIC = \"vendor_info\";\n```\n- **Description**: This constant defines the name of the output topic, seemingly for vendor information. This suggests that the application processes ride data to generate or utilize information pertinent to vendors, potentially involving analytics, reporting, or integration with third-party services.\n- **Use Case**: Components that need to publish processed results or aggregated vendor information would utilize this constant to guarantee that the data is sent to the correct output topic.\n\n## Section Summary\nThe design choice to encapsulate topic names in a dedicated class promotes code clarity and reduces the risk of runtime errors caused by typos or inconsistent topic naming. By using constants, developers can easily modify topic names in one place without needing to scour through the codebase for every instance where a topic name might appear. This enhances maintainability, readability, and structural integrity of the code.\n\n## Conclusion\nIn summary, the `Topics` class is a simple yet effective means for managing topic names within an application that likely deals with ride data and vendor information. Its constants enhance code safety, making the integration and messaging aspects of the application easier to manage and understand. This setup is particularly beneficial in distributed systems where communication between services relies heavily on standardized identifiers like topic names.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/Topics.java"
  },
  {
    "code": false,
    "content": "# Documentation for `CustomSerdes` Class\n\nThe `CustomSerdes` class is part of the `org.example.customserdes` package, designed to provide serialization and deserialization functionalities specifically for Apache Kafka using JSON and Avro formats. This utility class aims to create custom serializers and deserializers that can be easily utilized in Kafka-based applications.\n\n## Overview\n\nThe class contains two main static methods that facilitate creating SerDe (Serializer/Deserializer) for different data types. The methods cater to both JSON and Avro serialization, making it versatile for handling various data formats commonly used in Kafka applications.\n\n### Dependencies\n\nThe class relies on several libraries:\n- **Confluent Kafka Avro Serializer**: It provides functionality for integrating Avro serialization with Kafka.\n- **Kafka JSON Serializer**: Enables serialization and deserialization of JSON messages in Kafka.\n- **Apache Avro**: A data serialization system that integrates seamlessly with Kafka.\n- **Apache Kafka Common**: Used for standard serialization interfaces.\n\n## Method: `getSerde`\n\n### Purpose\n\nThe static method `getSerde(Class<T> classOf)` is defined to create a SerDe instance for a specified class type, facilitating serialization and deserialization for JSON-formatted data.\n\n### Parameters\n\n- **`Class<T> classOf`**: The class type for which the SerDe is created. This defines the structure of the JSON data being serialized and deserialized.\n\n### Functionality\n\n1. **Property Configuration**: \n   - A `HashMap` named `serdeProps` is created to hold configurations for the JSON serializer and deserializer.\n   - The property `\"json.value.type\"` is set to the passed class type.\n\n2. **Serializer and Deserializer Creation**:\n   - A new instance of `KafkaJsonSerializer` is created, which is configured using the previously set properties.\n   - Similarly, a new instance of `KafkaJsonDeserializer` is created and configured with the same properties.\n\n3. **SerDe Creation**:\n   - The method returns a SerDe instance constructed using the configured serializer and deserializer, which can be used to serialize and deserialize records of type `T`.\n\n## Method: `getAvroSerde`\n\n### Purpose\n\nThe second static method, `getAvroSerde(boolean isKey, String schemaRegistryUrl)`, facilitates the creation of a SerDe for Avro serialized data specifically for use with Kafka.\n\n### Parameters\n\n- **`boolean isKey`**: Indicates whether the created SerDe is intended for keys or values in Kafka messages.\n- **`String schemaRegistryUrl`**: The URL for the Schema Registry, which manages Avro schemas.\n\n### Functionality\n\n1. **Serde Initialization**:\n   - A `SpecificAvroSerde` instance is created, parameterized by the type `T`, which must be a subtype of `SpecificRecordBase`.\n\n2. **Property Configuration**: \n   - A `HashMap` named `serdeProps` is initialized to hold configuration properties specific to Avro serialization.\n   - The property `\"schema.registry.url\"` is set to the provided `schemaRegistryUrl`.\n\n3. **Serde Configuration**:\n   - The created SerDe is configured using the properties in `serdeProps` and the `isKey` parameter to determine the context of usage (key vs. value).\n\n4. **Return Value**: \n   - The method returns the configured `SpecificAvroSerde` instance, ready to handle serialization and deserialization of Avro data.\n\n## Usage Scenarios\n\nThe `CustomSerdes` class is intended to be utilized in scenarios where Kafka is employed for data streaming, and the application needs to serialize or deserialize message payloads.\n\n- **JSON Messages**: Using the `getSerde` method allows developers to handle JSON messages efficiently, which is common for applications that interact with HTTP or REST APIs.\n  \n- **Avro Messages**: The `getAvroSerde` method is particularly useful in systems that rely on Avro for schema evolution and backward compatibility, especially in large enterprise environments where structured data management is crucial.\n\n## Conclusion\n\nThe `CustomSerdes` class provides essential functionality for managing message serialization and deserialization in Kafka applications using JSON and Avro formats. It enables seamless creation of SerDe instances tailored for specific data types, thereby enhancing code reusability and maintaining cleaner implementation in Kafka-based architectures. By integrating with the Confluent libraries, it supports industry-standard practices for data serialization.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/customserdes/CustomSerdes.java"
  },
  {
    "code": false,
    "content": "# Documentation for `PickupLocation` Class\n\n## Overview\nThe `PickupLocation` class is part of the `org.example.data` package and represents a location from which a ride or transportation service is initiated. It encapsulates two crucial attributes: the unique identifier for the pickup location and the timestamp indicating when the pickup occurred. This class is essential for managing and manipulating data related to vehicle pickups in a transport application.\n\n## Class Definition\n```java\npublic class PickupLocation {\n```\nThe `PickupLocation` class is declared as a public class, making it accessible from other classes and packages. It serves as a blueprint for creating pickup location objects that contain specific attributes related to ride pickups.\n\n## Fields\n### PULocationID\n```java\npublic long PULocationID;\n```\n- **Type**: `long`\n- **Description**: This field stores a unique identifier for the pickup location. The use of `long` ensures that it can capture a significant range of IDs, which is essential for large datasets.\n\n### tpep_pickup_datetime\n```java\npublic LocalDateTime tpep_pickup_datetime;\n```\n- **Type**: `LocalDateTime`\n- **Description**: This field holds the date and time of the pickup in the `LocalDateTime` format. It provides information about when the pickup occurred, which can be critical for time-dependent data analysis and reporting.\n\n## Constructors\nThe `PickupLocation` class includes two constructors, allowing for different ways to instantiate a `PickupLocation` object.\n\n### 1. Parameterized Constructor\n```java\npublic PickupLocation(long PULocationID, LocalDateTime tpep_pickup_datetime) {\n    this.PULocationID = PULocationID;\n    this.tpep_pickup_datetime = tpep_pickup_datetime;\n}\n```\n- **Purpose**: This constructor allows for the creation of a `PickupLocation` object with specific values for both the `PULocationID` and the `tpep_pickup_datetime`. Utilizing this constructor ensures that both attributes can be set at the moment of object creation.\n\n### 2. Default Constructor\n```java\npublic PickupLocation() {\n}\n```\n- **Purpose**: The default constructor provides a way to create a `PickupLocation` object without requiring any initial values. This might be useful in scenarios where the attributes will be set later through other methods or setters.\n\n## Usage\nThe `PickupLocation` class can be utilized in various contexts within a larger application that manages ride-sharing or transportation services. When combined with other classes (perhaps representing drivers, passengers, or rides), it serves as a foundational element for tracking, managing, and analyzing transportation data.\n\n- **Example Instantiation**:\n    ```java\n    PickupLocation pickup = new PickupLocation(12345, LocalDateTime.now());\n    ```\n\nThis instance represents a pickup location with an ID of `12345` and the current date-time as the pickup time.\n\n## Advantages\nThe design of the `PickupLocation` class offers several advantages:\n\n1. **Encapsulation**: It encapsulates both the ID and timestamp, ensuring that they are handled together as a logical unit.\n2. **Flexibility**: The presence of both a parameterized and a default constructor provides flexibility in how objects are created.\n3. **Type Safety**: Using specific data types for the fields (like `long` and `LocalDateTime`) ensures that the data is handled correctly and reduces the risk of errors.\n\n## Potential Enhancements\nWhile this class serves its purpose well, potential enhancements could include:\n\n1. **Getter and Setter Methods**: To encapsulate the properties further and follow best practices in object-oriented design.\n2. **Validation**: Logic to ensure that `PULocationID` and `tpep_pickup_datetime` are always valid when set could improve the robustness of the class.\n3. **Override `toString()` Method**: Implementing a method to provide a string representation of a `PickupLocation` object would facilitate easier debugging and logging.\n\nOverall, the `PickupLocation` class is a straightforward yet essential component of a data model for transportation applications, effective for managing pickup location data.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/data/PickupLocation.java"
  },
  {
    "code": false,
    "content": "# Documentation for the `Ride` Class\n\n## Overview\n\nThe `Ride` class is a Java representation of a taxi ride, encapsulating various attributes associated with the ride and providing a constructor for instantiation using an array of strings. This class serves to model the data related to taxi rides, allowing for organized access and manipulation of this data in a structured format.\n\n## Class Definition\n\nThe `Ride` class is defined within the `org.example.data` package and contains multiple attributes that detail specific aspects of the ride. These attributes include pickup and drop-off times, passenger counts, distances, payment types, and various fees associated with the ride.\n\n### Attributes\n\n- **VendorID**: A string that represents the identifier for the taxi vendor.\n- **tpep_pickup_datetime**: A `LocalDateTime` object indicating the date and time when the ride was picked up.\n- **tpep_dropoff_datetime**: A `LocalDateTime` object indicating the date and time when the ride was dropped off.\n- **passenger_count**: An integer representing the number of passengers in the vehicle.\n- **trip_distance**: A double representing the distance of the trip in miles or kilometers.\n- **RatecodeID**: A long integer that signifies the rate code associated with the ride.\n- **store_and_fwd_flag**: A string indicator for whether the ride data was stored and forwarded.\n- **PULocationID**: A long integer representing the pickup location's identifier.\n- **DOLocationID**: A long integer indicating the drop-off location's identifier.\n- **payment_type**: A string that specifies the method of payment for the ride.\n- **fare_amount**: A double representing the fare amount charged for the ride.\n- **extra**: A double that indicates any extra charges incurred during the ride.\n- **mta_tax**: A double representing the Metropolitan Transportation Authority tax applied.\n- **tip_amount**: A double indicating the amount tipped to the driver.\n- **tolls_amount**: A double indicating the amount paid in tolls.\n- **improvement_surcharge**: A double that represents any improvement surcharges applied.\n- **total_amount**: A double which indicates the total amount charged for the ride.\n- **congestion_surcharge**: A double representing any congestion charges applied.\n\n## Constructors\n\n### Ride(String[] arr)\n\nThis constructor allows for the initialization of a `Ride` object based on an array of strings, `arr`. Each element of the array corresponds to specific attributes of the ride. \n\n- The following parsing operations are performed within this constructor:\n  - Strings are converted into `LocalDateTime` objects for the pickup and drop-off times using a predefined date-time format.\n  - Other strings are parsed into their respective numeric types (int, long, or double) for attributes like passenger count, trip distance, and financial amounts.\n\n### Ride()\n\nThis is a default constructor that initializes a `Ride` object without setting any attributes. This constructor enables the ability to create empty `Ride` objects, which can later be populated with data if required.\n\n## Data Handling\n\nThe `Ride` class encapsulates data for the taxi ride, making use of Java's built-in data types:\n\n- **LocalDateTime** is utilized for accurately managing date and time information, ensuring that the ride timing is easy to manipulate and compare.\n- Primitive data types (int, long, double) are employed for various ride associated figures to optimize performance and memory usage.\n\nThe use of these types ensures that the attributes hold values that are meaningful, allowing for calculations (e.g., total fare), comparisons (e.g., pickup vs. drop-off time), and logical operations required in ride management applications.\n\n## Usage\n\nThis class can be utilized in applications that require management and analysis of taxi rides. For example, ride-hailing services can utilize instances of this class to track ride data, compute earnings, manage driver and vehicle statistics, and generate analytics for business needs.\n\nIn summary, the `Ride` class serves as a foundational data structure that enables organized handling of detailed information regarding taxi rides, capturing essential data that can be processed or analyzed as per the application's requirements.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/data/Ride.java"
  },
  {
    "code": false,
    "content": "# VendorInfo Class Documentation\n\nThe `VendorInfo` class is part of the `org.example.data` package and serves as a data model representing information about a vendor, likely in a transportation or logistic context. Below is a detailed description of its components and functionality.\n\n## Overview\n\nThe `VendorInfo` class encapsulates several properties associated with a vendor, such as identification and time-related data. It provides defined constructors to create instances of the class, facilitating both parameterized and default object creation.\n\n## Properties\n\n### VendorID (String)\n- **Description**: Represents a unique identifier for the vendor. It is stored as a `String` to accommodate potential alphanumeric vendor IDs.\n  \n### PULocationID (long)\n- **Description**: Indicates the pickup location ID for the vendor, stored as a numerical value (long). This allows for the reference of specific locations tied to the vendor's service area.\n\n### pickupTime (LocalDateTime)\n- **Description**: Captures the time at which a vendor picks up a customer or an item. This property is of the `LocalDateTime` type, which provides date and time resolution without timezone information.\n\n### lastDropoffTime (LocalDateTime)\n- **Description**: Records the most recent time when the vendor made a drop-off. Like `pickupTime`, it also utilizes the `LocalDateTime` type.\n\n## Constructors\n\n### VendorInfo(String vendorID, long PULocationID, LocalDateTime pickupTime, LocalDateTime lastDropoffTime)\n- **Purpose**: A parameterized constructor that allows the creation of a `VendorInfo` object with specified values for `VendorID`, `PULocationID`, `pickupTime`, and `lastDropoffTime`.\n- **Parameters**:\n  - `vendorID`: A `String` representing the vendor's unique identifier.\n  - `PULocationID`: A `long` that denotes the pickup location.\n  - `pickupTime`: A `LocalDateTime` object indicating when the vendor picks up.\n  - `lastDropoffTime`: A `LocalDateTime` object representing the last drop-off time.\n\n### VendorInfo()\n- **Purpose**: A default constructor that initializes a blank `VendorInfo` object. This is useful in scenarios where the object will be populated later or as part of a data structure.\n\n## Usage Scenarios\n\nThe `VendorInfo` class is likely intended for use in applications where managing vendor-related data is essential, such as transportation services, delivery tracking systems, or logistics management platforms. \n\nBy instantiating the `VendorInfo` object, applications can efficiently capture and manipulate vendor-specific data, allowing for improved tracking and operational analysis. \n\nIn summary, this class provides a structured way to manage essential vendor information, enhancing data integrity and access within the application. The flexibility of having both a parameterized and a default constructor allows for diverse use cases in different contexts where vendor data is required.",
    "filename": "06-streaming/java/kafka_examples/src/main/java/org/example/data/VendorInfo.java"
  },
  {
    "code": false,
    "content": "# Documentation for `JsonKStreamJoinsTest.java`\n\n## Overview\nThe `JsonKStreamJoinsTest` class is designed to test the functionality of a Kafka Streams application that processes and joins ride data and pickup location data. It leverages the Kafka Streams library to create a topology that evaluates the interaction between different data streams. The class utilizes the JUnit framework for unit testing, particularly focusing on the joins involving ride and location data.\n\n## Dependencies and Class Structure\nThis class imports essential components from the Kafka Streams library, including `StreamsConfig`, `Topology`, and various serialization classes. It also imports custom serialization/deserialization (`CustomSerdes`) and data classes (`Ride`, `PickupLocation`, `VendorInfo`) used in the streaming operations. The test class is structured around the JUnit testing lifecycle, including setup and teardown functionalities.\n\n## Properties and Topology Setup\n### Properties Configuration\nThe `props` property object is defined to configure the Kafka Streams application. Specifically, it sets the application ID and bootstrap server properties. This configuration is crucial for correctly running the Streams application in a testing environment, allowing it to connect to the appropriate Kafka cluster.\n\n### Topology Creation\nAn instance of `Topology` is created by calling `createTopology()` from the `JsonKStreamJoins` class. This method presumably constructs and configures the necessary steps for processing incoming stream data from rides and pickup locations.\n\n## Test Initialization\n### Test Driver\nThe `TopologyTestDriver` is initialized in the `setup()` method. This driver simulates the processing of data through the created topology, allowing the test to send input data through the streams and to observe the output. \n\n### Topics Definition\nThree test topics are created:\n1. **Input Rides Topic**: A topic for incoming ride data.\n2. **Input Pickup Location Topic**: A topic for incoming pickup location data.\n3. **Output Topic**: A topic for results generated from joining the above data streams.\n\nCustom serialization/deserialization methods are used to ensure correct data handling as it flows through the system.\n\n## Test Execution\n### Join Functionality Test\nThe core test function, `testIfJoinWorksOnSameDropOffPickupLocationId`, is designed to evaluate if the joining of ride data and pickup location data works as intended. \n\n1. **Data Generation**: It generates instances of the `Ride` and `PickupLocation` classes using `DataGeneratorHelper`. A pickup location is created specifically with the drop-off location ID of the ride to simulate a real-world scenario accurately.\n  \n2. **Data Input**: The generated `Ride` and `PickupLocation` objects are then input through their respective topics.\n\n3. **Assertions**: The test checks if the output topic has one message, which asserts that the join operation has been successful. Further, it verifies the content of the output to ensure it aligns with expectations:\n   - Correct `VendorID`\n   - Correct pickup time associated with the generated ride.\n\n## Cleanup\n### Shutdown Procedure\nThe class contains a static method called `shutdown()` annotated with `@AfterAll`. This method is intended to close the `TopologyTestDriver` after all tests have been executed, ensuring proper resource management and cleanup.\n\n## Conclusion\nThe `JsonKStreamJoinsTest` is a well-structured unit test designed to validate the functionality of a Kafka Streams application dealing with ride-hailing data. It effectively sets up data streams, performs a join operation, and verifies the correctness of output results while adhering to testing best practices with clear setup and teardown mechanisms. The use of custom serializers and deserializers indicates a thoughtful approach to handling complex data types within the Kafka ecosystem.",
    "filename": "06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java"
  },
  {
    "code": false,
    "content": "# JsonKStreamTest Documentation\n\n## Overview\n\nThe `JsonKStreamTest` class is a test class designed to validate the functionality of a Kafka Streams application that processes ride data. Utilizing the Kafka Streams library, this class tests the correct counting of ride occurrences based on the drop-off location ID (`DOLocationID`). The class leverages JUnit for unit testing and custom serialization/deserialization for the ride data.\n\n## Class Structure\n\n### Imports\n\nThe class begins by importing the required libraries:\n\n- **Kafka Streams Libraries**: These are imported to facilitate the creation and management of Kafka Streams.\n- **JUnit Libraries**: These provide functionalities for writing tests.\n- **Custom Libraries**: This includes custom serializers and data generation helpers used for rides.\n\n### Fields Declaration\n\nThe class declares several fields:\n\n```java\nprivate Properties props;\nprivate static TopologyTestDriver testDriver;\nprivate TestInputTopic<String, Ride> inputTopic;\nprivate TestOutputTopic<String, Long> outputTopic;\nprivate Topology topology = new JsonKStream().createTopology();\n```\n\n- **props**: Holds configuration properties for the Kafka Streams application.\n- **testDriver**: A static instance of `TopologyTestDriver`, used for testing the stream topology.\n- **inputTopic** and **outputTopic**: Represent the input and output topics for testing the streaming process.\n- **topology**: The topology created by the `JsonKStream` class.\n\n## Setup and Teardown\n\n### Initialization\n\nThe `setup` method initializes the properties, creates the test driver, and sets up the input and output topics:\n\n```java\n@BeforeEach\npublic void setup() {\n    props = new Properties();\n    props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"testing_count_application\");\n    props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy:1234\");\n    if (testDriver != null) {\n        testDriver.close();\n    }\n    testDriver = new TopologyTestDriver(topology, props);\n    inputTopic = testDriver.createInputTopic(\"rides\", Serdes.String().serializer(), CustomSerdes.getSerde(Ride.class).serializer());\n    outputTopic = testDriver.createOutputTopic(\"rides-pulocation-count\", Serdes.String().deserializer(), Serdes.Long().deserializer());\n}\n```\n\n- **Properties Configuration**: Configures the application ID and bootstrap server settings for Kafka Streams.\n- **Topology Initialization**: Creates a new instance of `TopologyTestDriver` to build the stream processing topology.\n- **Topic Creation**: Sets up input and output topics with the necessary serializers and deserializers.\n\n### Cleanup\n\nThe `tearDown` method closes the test driver after all tests have completed:\n\n```java\n@AfterAll\npublic static void tearDown() {\n    testDriver.close();\n}\n```\n\n## Test Cases\n\n### Test Case 1: Single Message Input\n\n```java\n@Test\npublic void testIfOneMessageIsPassedToInputTopicWeGetCountOfOne() {\n    Ride ride = DataGeneratorHelper.generateRide();\n    inputTopic.pipeInput(String.valueOf(ride.DOLocationID), ride);\n\n    assertEquals(outputTopic.readKeyValue(), KeyValue.pair(String.valueOf(ride.DOLocationID), 1L));\n    assertTrue(outputTopic.isEmpty());\n}\n```\n\nThis test checks if a single ride message results in a count of one for the given drop-off location ID. It verifies that the output topic returns the expected key-value pair and confirms the output topic is empty after reading the value.\n\n### Test Case 2: Multiple Messages with Different Keys\n\n```java\n@Test\npublic void testIfTwoMessageArePassedWithDifferentKey() {\n    Ride ride1 = DataGeneratorHelper.generateRide();\n    ride1.DOLocationID = 100L;\n    inputTopic.pipeInput(String.valueOf(ride1.DOLocationID), ride1);\n\n    Ride ride2 = DataGeneratorHelper.generateRide();\n    ride2.DOLocationID = 200L;\n    inputTopic.pipeInput(String.valueOf(ride2.DOLocationID), ride2);\n\n    assertEquals(outputTopic.readKeyValue(), KeyValue.pair(String.valueOf(ride1.DOLocationID), 1L));\n    assertEquals(outputTopic.readKeyValue(), KeyValue.pair(String.valueOf(ride2.DOLocationID), 1L));\n    assertTrue(outputTopic.isEmpty());\n}\n```\n\nHere, the test case ensures that two distinct messages (with different `DOLocationID`s) are correctly counted as separate entities. The expected result is verified for both ride IDs, and it checks that the output topic is empty after reading the values.\n\n### Test Case 3: Multiple Messages with the Same Key\n\n```java\n@Test\npublic void testIfTwoMessageArePassedWithSameKey() {\n    Ride ride1 = DataGeneratorHelper.generateRide();\n    ride1.DOLocationID = 100L;\n    inputTopic.pipeInput(String.valueOf(ride1.DOLocationID), ride1);\n\n    Ride ride2 = DataGeneratorHelper.generateRide();\n    ride2.DOLocationID = 100L;\n    inputTopic.pipeInput(String.valueOf(ride2.DOLocationID), ride2);\n\n    assertEquals(outputTopic.readKeyValue(), KeyValue.pair(\"100\", 1L));\n    assertEquals(outputTopic.readKeyValue(), KeyValue.pair(\"100\", 2L));\n    assertTrue(outputTopic.isEmpty());\n}\n```\n\nThis test case validates the behavior when two ride messages share the same `DOLocationID`. The expected result is that the same key in the output should reflect an increment in the count, showcasing that the application correctly aggregates messages with the same key.\n\n## Conclusion\n\nThe `JsonKStreamTest` serves to verify that the Kafka Streams application for ride counting operates as expected. Each test method checks different scenarios related to input message handling based on drop-off location IDs, ensuring robust functionality and consistent behavior during stream processing.",
    "filename": "06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java"
  },
  {
    "code": false,
    "content": "# DataGeneratorHelper Class Documentation\n\nThe `DataGeneratorHelper` class is a utility class designed to facilitate the generation of ride and pickup location data within an application, presumably related to transportation services such as ride-hailing or logistics. This class includes static methods that create instances of the `Ride` and `PickupLocation` classes, which are part of the application's data model.\n\n## Class Overview\n\n- **Package**: `org.example.helper`\n- **Purpose**: Generate mock data for rides and pickup locations.\n- **Usage**: This class can be utilized wherever test data is required for rides and pickup locations, allowing for streamlined testing and development.\n\n## Method: `generateRide()`\n\n### Purpose\nThe `generateRide` method creates and returns a new instance of the `Ride` class with predefined values, simulating a ride record.\n\n### Functionality\n1. **Current Date and Time**: The method retrieves the current local date and time.\n2. **Departure Time**: It calculates a departure time that is 30 minutes before the current time.\n3. **Formatted Strings**: Both the arrival and departure times are formatted as strings in the \"yyyy-MM-dd HH:mm:ss\" format.\n4. **Ride Creation**: A new `Ride` object is instantiated with an array of string parameters. These parameters include identifying data and various attributes that characterize the ride (such as time, distance, fare, etc.).\n\n### Typical Output\nThe output will be a `Ride` object initialized with set values that are relevant for use in testing or data generation.\n\n## Method: `generatePickUpLocation(long pickupLocationId)`\n\n### Purpose\nThe `generatePickUpLocation` method is responsible for creating a `PickupLocation` object, which holds information about a pickup location.\n\n### Functionality\n1. **Pickup Location ID**: The method takes a `long` parameter representing the unique ID for the pickup location.\n2. **Current Timestamp**: It captures the current date and time to assign to the pickup location.\n3. **Pickup Location Creation**: The method returns a new `PickupLocation` object, instantiated with the provided ID and the current timestamp.\n\n### Typical Output\nThe output will be a `PickupLocation` object encapsulating the pickup location ID and the timestamp of its creation.\n\n## Usage Context\n\nThis `DataGeneratorHelper` class is likely used in testing scenarios where instances of `Ride` and `PickupLocation` are needed. It helps automate the generation of realistic data for development purposes, making it easier for developers and testers to simulate various situations without manually inputting each data point.\n\n### Integration\n- **Dependencies**: The class relies on the `Ride` and `PickupLocation` classes from the `org.example.data` package. Ensure that these classes are properly defined to utilize this helper effectively.\n- **Dynamic Data Simulation**: By providing current timestamps and configurable pickup IDs, the methods allow for dynamic test scenarios where varying conditions and data points can be tested effortlessly.\n\n## Conclusion\n\nThe `DataGeneratorHelper` class serves as an essential tool in the development and testing lifecycle of applications involving ride-sharing or similar services. With its two core methods, it simplifies the process of generating mock data for rides and pickup locations, contributing to more efficient verification of functionality within the application.",
    "filename": "06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java"
  },
  {
    "content": "## KSQL DB Examples\n### Create streams\n```sql\nCREATE STREAM ride_streams (\n    VendorId varchar, \n    trip_distance double,\n    payment_type varchar\n)  WITH (KAFKA_TOPIC='rides',\n        VALUE_FORMAT='JSON');\n```\n\n### Query stream\n```sql\nselect * from RIDE_STREAMS \nEMIT CHANGES;\n```\n\n### Query stream count\n```sql\nSELECT VENDORID, count(*) FROM RIDE_STREAMS \nGROUP BY VENDORID\nEMIT CHANGES;\n```\n\n### Query stream with filters\n```sql\nSELECT payment_type, count(*) FROM RIDE_STREAMS \nWHERE payment_type IN ('1', '2')\nGROUP BY payment_type\nEMIT CHANGES;\n```\n\n### Query stream with window functions\n```sql\nCREATE TABLE payment_type_sessions AS\n  SELECT payment_type,\n         count(*)\n  FROM  RIDE_STREAMS \n  WINDOW SESSION (60 SECONDS)\n  GROUP BY payment_type\n  EMIT CHANGES;\n```\n\n## KSQL documentation for details\n[KSQL DB Documentation](https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/quick-reference/)\n\n[KSQL DB Java client](https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-clients/java-client/)",
    "filename": "06-streaming/ksqldb/commands.md"
  },
  {
    "content": "# Apache Flink Training\nApache Flink Streaming Pipelines\n\n## :pushpin: Getting started \n\n### :whale: Installations\n\nTo run this repo, the following components will need to be installed:\n\n1. [Docker](https://docs.docker.com/get-docker/) (required)\n2. [Docker compose](https://docs.docker.com/compose/install/#installation-scenarios) (required)\n3. Make (recommended) -- see below\n    - On most Linux distributions and macOS, `make` is typically pre-installed by default. To check if `make` is installed on your system, you can run the `make --version` command in your terminal or command prompt. If it's installed, it will display the version information. \n    - Otherwise, you can try following the instructions below, or you can just copy+paste the commands from the `Makefile` into your terminal or command prompt and run manually.\n\n        ```bash\n        # On Ubuntu or Debian:\n        sudo apt-get update\n        sudo apt-get install build-essential\n\n        # On CentOS or Fedora:\n        sudo dnf install make\n\n        # On macOS:\n        xcode-select --install\n\n        # On windows:\n        choco install make # uses Chocolatey, https://chocolatey.org/install\n        ```\n\n### :computer: Local setup\n\nMake sure you're in the `pyflick` folder:\n\n```bash\ncd 06-streaming/pyflink\n```\n\n## :boom: Running the pipeline\n\n1. Build the Docker image and deploy the services in the `docker-compose.yml` file, including the PostgreSQL database and Flink cluster. This will (should) also create the sink table, `processed_events`, where Flink will write the Kafka messages to.\n\n    ```bash\n    make up\n\n    #// if you dont have make, you can run:\n    # docker compose up --build --remove-orphans  -d\n    ```\n\n    **:star: Wait until the Flink UI is running at [http://localhost:8081/](http://localhost:8081/) before proceeding to the next step.** _Note the first time you build the Docker image it can take anywhere from 5 to 30 minutes. Future builds should only take a few second, assuming you haven't deleted the image since._\n\n    :information_source: After the image is built, Docker will automatically start up the job manager and task manager services. This will take a minute or so. Check the container logs in Docker desktop and when you see the line below, you know you're good to move onto the next step.\n\n    ```\n    taskmanager Successful registration at resource manager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_* under registration id <id_number>\n    ```\n\n2. Now that the Flink cluster is up and running, it's time to finally run the PyFlink job! :smile:\n\n    ```bash\n    make job\n\n    #// if you dont have make, you can run:\n    # docker-compose exec jobmanager ./bin/flink run -py /opt/job/start_job.py -d\n    ```\n\n    After about a minute, you should see a prompt that the job's been submitted (e.g., `Job has been submitted with JobID <job_id_number>`). Now go back to the [Flink UI](http://localhost:8081/#/job/running) to see the job running! :tada:\n\n\n3. When you're done, you can stop and/or clean up the Docker resources by running the commands below.\n\n    ```bash\n    make stop # to stop running services in docker compose\n    make down # to stop and remove docker compose services\n    make clean # to remove the docker container and dangling images\n    ```\n\n    :grey_exclamation: Note the `/var/lib/postgresql/data` directory inside the PostgreSQL container is mounted to the `./postgres-data` directory on your local machine. This means the data will persist across container restarts or removals, so even if you stop/remove the container, you won't lose any data written within the container.\n\n------\n\n:information_source: To see all the make commands that're available and what they do, run:\n\n```bash\nmake help\n```\n\nAs of the time of writing this, the available commands are:\n\n```bash\n\nUsage:\n  make <target>\n\nTargets:\n  help                 Show help with `make help`\n  db-init              Builds and runs the PostgreSQL database service\n  build                Builds the Flink base image with pyFlink and connectors installed\n  up                   Builds the base Docker image and starts Flink cluster\n  down                 Shuts down the Flink cluster\n  job                  Submit the Flink job\n  stop                 Stops all services in Docker compose\n  start                Starts all services in Docker compose\n  clean                Stops and removes the Docker container as well as images with tag `<none>`\n  psql                 Runs psql to query containerized postgreSQL database in CLI\n  postgres-die-mac     Removes mounted postgres data dir on local machine (mac users) and in Docker\n  postgres-die-pc      Removes mounted postgres data dir on local machine (PC users) and in Docker\n```",
    "filename": "06-streaming/pyflink/README.md"
  },
  {
    "content": "# Homework\n\nFor this homework we will be using the Taxi data:\n- Green 2019-10 data from [here](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz)\n\n\n## Start Red Panda, Flink Job Manager, Flink Task Manager, and Postgres \n\nThere's a `docker-compose.yml` file in the homework folder (taken from [here](https://github.com/redpanda-data-blog/2023-python-gsg/blob/main/docker-compose.yml))\n\nCopy this file to your homework directory and run\n\n```bash\ndocker-compose up\n```\n\n(Add `-d` if you want to run in detached mode)\n\nVisit `localhost:8081` to see the Flink Job Manager\n\nConnect to Postgres with [DBeaver](https://dbeaver.io/).\n\nThe connection credentials are:\n- Username `postgres`\n- Password `postgres`\n- Database `postgres`\n- Host `localhost`\n- Port `5432`\n\n\nIn DBeaver, run this query to create the Postgres landing zone for the first events:\n```sql \nCREATE TABLE processed_events (\n    test_data INTEGER,\n    event_timestamp TIMESTAMP\n)\n```\n\n\n## Question 1. Connecting to the Kafka server\n\nWe need to make sure we can connect to the server, so\nlater we can send some data to its topics\n\nFirst, let's install the kafka connector (up to you if you\nwant to have a separate virtual environment for that)\n\n```bash\npip install kafka-python\n```\n\nYou can start a jupyter notebook in your solution folder or\ncreate a script\n\nLet's try to connect to our server:\n\n```python\nimport json\nimport time \n\nfrom kafka import KafkaProducer\n\ndef json_serializer(data):\n    return json.dumps(data).encode('utf-8')\n\nserver = 'localhost:9092'\n\nproducer = KafkaProducer(\n    bootstrap_servers=[server],\n    value_serializer=json_serializer\n)\n\nproducer.bootstrap_connected()\n```\n\n## Question 3: Sending the Trip Data\n\n* Read the green csv.gz file\n* We will only need these columns:\n  * `'lpep_pickup_datetime',`\n  * `'lpep_dropoff_datetime',`\n  * `'PULocationID',`\n  * `'DOLocationID',`\n  * `'passenger_count',`\n  * `'trip_distance',`\n  * `'tip_amount'`\n\n* Create a topic `green-trips` and send the data there with `load_taxi_data.py`\n* How much time in seconds did it take? (You can round it to a whole number)\n* Make sure you don't include sleeps in your code\n\n## Question 4: Build a Sessionization Window\n\n* Copy `aggregation_job.py` and rename it to `session_job.py`\n* Have it read from `green-trips` fixing the schema\n* Use a [session window](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/) with a gap of 5 minutes\n* Use `lpep_dropoff_datetime` time as your watermark with a 5 second tolerance\n* Which pickup and drop off locations have the longest unbroken streak of taxi trips?",
    "filename": "06-streaming/pyflink/homework.md"
  },
  {
    "code": false,
    "content": "# Overview of the PyFlink Streaming Application\n\nThis PyFlink application processes streaming data from a Kafka source, aggregates it, and writes the results to a PostgreSQL database. It sets up a streaming environment, defines source and sink tables, uses watermarks for event-time processing, and performs a time aggregation on the streaming data.\n\n## Importing Necessary Modules\n\nThe script begins by importing essential modules from the PyFlink library:\n\n```python\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import EnvironmentSettings, DataTypes, TableEnvironment, StreamTableEnvironment\nfrom pyflink.common.watermark_strategy import WatermarkStrategy\nfrom pyflink.common.time import Duration\n```\n\n- **StreamExecutionEnvironment**: Manages the execution context for streaming applications.\n- **TableEnvironment**: Provides the environment for executing SQL queries on streaming and batch data.\n- **WatermarkStrategy**: Manages the handling of event-time semantics and late events.\n- **Duration**: Represents time intervals, used for defining watermarks.\n\n## Function: `create_events_aggregated_sink`\n\nThis function sets up a sink table that will store the aggregated streaming data in PostgreSQL.\n\n```python\ndef create_events_aggregated_sink(t_env):\n    ...\n```\n\n- **Parameters**: Takes a `TableEnvironment` instance (`t_env`) as input.\n- **DDL**: Executes a SQL Data Definition Language (DDL) statement to create a table named `processed_events_aggregated` with columns for event hour, test data, and number of hits.\n- **Return Value**: Returns the name of the created table.\n\n## Function: `create_events_source_kafka`\n\nThis function defines a source table that reads events from a Kafka topic.\n\n```python\ndef create_events_source_kafka(t_env):\n    ...\n```\n\n- **Parameters**: Accepts a `TableEnvironment` instance (`t_env`).\n- **DDL**: Creates a table named `events` that defines the structure of incoming Kafka data, including an integer field for test data and a timestamp field for event timestamps.\n- **Watermark**: Generates a watermark to handle late events by adjusting for a one-second delay.\n- **Return Value**: Returns the name of the created table.\n\n## Function: `log_aggregation`\n\nThis function orchestrates the overall processing logic by setting up the execution and table environments, creating source and sink tables, and performing the aggregation.\n\n```python\ndef log_aggregation():\n    ...\n```\n\n1. **Execution Environment Setup**: Initializes the `StreamExecutionEnvironment` and sets checkpointing and parallelism settings.\n   - **Checkpointing**: Enables checkpointing every 10 seconds to enhance fault tolerance.\n   - **Parallelism**: Sets the parallelism level to 3 for processing scalability.\n\n2. **Table Environment Setup**: Creates a `StreamTableEnvironment` configured for streaming mode.\n\n3. **Watermark Strategy**: Defines a watermark strategy to manage late event arrival with a maximum out-of-orderness of 5 seconds.\n\n4. **Table Creation**: Calls the `create_events_source_kafka` and `create_events_aggregated_sink` functions to set up the source and sink tables.\n\n5. **Aggregation Logic**: Executes an SQL insert statement that:\n   - Performs a tumbling window aggregation on the incoming data from the Kafka source table over 1-minute intervals.\n   - Groups the results by event hour and test data while counting the number of hits.\n\n6. **Error Handling**: A try-except block captures and prints any errors that occur during the execution of the SQL commands.\n\n## Main Execution Block\n\nThe script concludes with a standard Python entry point:\n\n```python\nif __name__ == '__main__':\n    log_aggregation()\n```\n\n- It checks if the script is being run as the main module and calls the `log_aggregation` function to start the streaming workflow.\n\n## Overall Functionality Summary\n\nThis application provides a pipeline that effectively handles real-time data processing by:\n\n- Reading events from Kafka via a well-structured table definition.\n- Aggregating incoming events based on specified time intervals using Flink\u2019s SQL syntax.\n- Outputting the aggregated results to PostgreSQL for further analysis or reporting.\n\n## Conclusion\n\nThe code is structured efficiently to facilitate the processing of streaming data. By leveraging Flask's capabilities for distributed streaming computations and using watermarks for handling late data, it ensures that the application remains robust and reliable under varying data arrival patterns. This modular design makes it easy to expand or modify for different use cases or data sources.",
    "filename": "06-streaming/pyflink/src/job/aggregation_job.py"
  },
  {
    "code": false,
    "content": "# Documentation for Flink Streaming Application\n\nThis document outlines the functionality of a Python script that utilizes Apache Flink and PyFlink to process events from a Kafka source and write the processed results to a PostgreSQL sink. \n\n## Overview\n\nThe script sets up a streaming data pipeline that consumes JSON-formatted event data from a Kafka topic, processes it, and writes the results to a PostgreSQL database. The pipeline leverages table environments to operate on structured data processing efficiently using SQL-like syntax.\n\n## Key Components\n\n### Functions\n\n#### `create_processed_events_sink_postgres(t_env)`\n\n- **Purpose**: This function sets up a sink (output) table in a PostgreSQL database.\n- **Parameters**: \n  - `t_env`: The `TableEnvironment` object for managing SQL queries.\n- **Functionality**:\n  1. Defines a DDL (Data Definition Language) string to create a table named `processed_events`.\n  2. Specifies the schema of the table, including `test_data` (an integer) and `event_timestamp` (a timestamp).\n  3. Configures it to connect to a PostgreSQL database using specified connection properties (URL, table name, credentials).\n  4. Executes the DDL statement to create the table in the database.\n  5. Returns the name of the created table for further use.\n\n#### `create_events_source_kafka(t_env)`\n\n- **Purpose**: Sets up the source table that reads events from a Kafka topic.\n- **Parameters**:\n  - `t_env`: The `TableEnvironment` object.\n- **Functionality**:\n  1. Initiates a DDL string to create a table named `events`.\n  2. Defines the schema for the table, which includes `test_data` (an integer) and `event_timestamp` (a BIGINT).\n  3. Also creates a watermarked column to handle event time processing.\n  4. Sets up configurations to connect to the Kafka broker, specifying properties for connection, topic name, data format, and startup mode.\n  5. Executes the SQL statement to create the Kafka source table.\n  6. Returns the name of the created table.\n\n### `log_processing()`\n\n- **Purpose**: The main function of the script that orchestrates the data flow from Kafka to PostgreSQL.\n- **Functionality**:\n  1. Creates a streaming execution environment and enables checkpointing every 10 seconds to ensure fault tolerance.\n  2. It sets up a table environment configured for streaming mode.\n  3. Calls `create_events_source_kafka` to create the source table from which events will be read.\n  4. Calls `create_processed_events_sink_postgres` to create the sink table where processed data will be written.\n  5. Executes an SQL `INSERT` statement that selects data from the Kafka source table and writes it into the PostgreSQL sink, converting the timestamp format appropriately.\n  6. Implements error handling to capture exceptions during the data writing process.\n\n## Script Execution\n\nThe script is designed to be executed as a standalone program. When the script runs, the following occurs:\n\n1. The `log_processing` function is invoked when the script is executed directly.\n2. Within this function, the streaming environment and table environments are initialized, followed by the creation of both source and sink tables.\n3. The data flow is established, where events from the Kafka topic are processed and written to PostgreSQL.\n4. If any exceptions occur during this process, an error message is printed to the console, indicating the failure in writing records.\n\n## Conclusion\n\nThis script efficiently connects a Kafka source to a PostgreSQL sink using Apache Flink's capabilities. The functions defined provide modular components for setting up the data pipeline, with a focus on structured stream processing and fault tolerance. Through this setup, the application is capable of transforming real-time events into a structured database format suitable for further analysis or storage.",
    "filename": "06-streaming/pyflink/src/job/start_job.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis Python script utilizes Apache Flink (specifically the PyFlink API) to set up a data pipeline that reads taxi event data from a Kafka stream and writes it into a PostgreSQL database. The script defines functions for creating a source table (from Kafka) and a sink table (to PostgreSQL), followed by an execution function that manages the flow of data between these sources and sinks.\n\n## Setup and Configuration\n\nThe script starts by importing necessary modules from the `pyflink` library, which is required for implementing streaming data processing in Python. The major components being imported include:\n\n- `StreamExecutionEnvironment`: Responsible for configuring the execution environment.\n- `EnvironmentSettings`: Holds settings for the table environment.\n- `TableEnvironment`: Defines the context in which the Flink SQL runtime operates.\n- `StreamTableEnvironment`: A specialized table environment for streaming use cases.\n\n## Function: `create_taxi_events_sink_postgres`\n\nThe `create_taxi_events_sink_postgres` function is designed to create a sink table in a PostgreSQL database where the processed data will be stored. Within this function:\n\n- A DDL (Data Definition Language) SQL string is dynamically constructed using the table name `taxi_events`.\n- Various columns are defined along with their data types, including `VendorID`, `lpep_pickup_datetime`, `passenger_count`, and financial fields like `fare_amount` and `tip_amount`.\n- The function configures the JDBC connection parameters for PostgreSQL (URL, username, password, and JDBC driver).\n- The SQL DDL statement is executed using the provided `TableEnvironment`, and the function returns the name of the created table.\n\n## Function: `create_events_source_kafka`\n\nThe purpose of the `create_events_source_kafka` function is to establish a source table that reads from a Kafka topic. This is similarly done using a DDL SQL string. Key highlights include:\n\n- The function specifies a table name `taxi_events`, which matches the sink table created earlier.\n- Columns analogous to those in the sink table are defined, along with an additional `pickup_timestamp` column created from the `lpep_pickup_datetime`, enabling timestamp processing.\n- A watermark is defined to manage event time processing, allowing the system to handle late events up to 15 seconds.\n- Kafka connection settings are provided, including the bootstrap servers, topic name (`green-data`), and message format (JSON).\n- As with the previous function, this function also executes the DDL and returns the table name.\n\n## Function: `log_processing`\n\nThe `log_processing` function orchestrates the execution flow of the Flink application. The steps taken within this function are as follows:\n\n1. **Initialize Execution Environment**: The function creates a `StreamExecutionEnvironment` object and enables checkpointing every 10 seconds to ensure fault tolerance during streaming.\n   \n2. **Create Table Environment**: It sets up the streaming table environment required for Flink operations, specifying that streaming mode is enabled through `EnvironmentSettings`.\n\n3. **Create Tables**: The function calls `create_events_source_kafka` to set up the Kafka source table and `create_taxi_events_sink_postgres` to set up the PostgreSQL sink table.\n\n4. **Data Flow**: An SQL `INSERT INTO` statement is executed to transfer records from the Kafka source to the PostgreSQL sink. This data flow is crucial for the ETL (Extract, Transform, Load) process.\n\n5. **Exception Handling**: Any exceptions that occur during the data writing process to PostgreSQL are caught and logged, preventing the application from crashing unexpectedly.\n\n## Execution Entry Point\n\nLastly, within the `if __name__ == '__main__':` block, the `log_processing` function is invoked to start the streaming data pipeline. This section ensures that the processing begins only when the script is run as the main module, thereby allowing it to function correctly as an executable script.\n\n## Conclusion\n\nIn summary, this PyFlink script demonstrates a typical use case of streaming data applications: ingesting data from a Kafka topic, processing it, and writing the results into a PostgreSQL database. The defined functions modularize responsibilities, allowing for easy adjustments or enhancements in future implementations. This structure follows a clear pattern of creating source and sink, executing data flows, and managing the runtime environment in a robust manner, suitable for real-time data analytics.",
    "filename": "06-streaming/pyflink/src/job/taxi_job.py"
  },
  {
    "code": false,
    "content": "# Documentation of Kafka CSV Producer Script\n\n## Overview\nThis Python script is designed to read a CSV file containing green taxi trip data and send each row of data to a Kafka topic named \"green-data\". It utilizes the Kafka Python client library to create a Kafka producer, which facilitates sending data to Apache Kafka.\n\n## Libraries Used\n- **csv**: This library is used to read the contents of the CSV file as a dictionary, where each row corresponds to a key-value pair based on the CSV headers.\n- **json**: This library allows the script to serialize Python dictionaries into JSON format, which is the format used to send data to Kafka.\n- **kafka**: Specifically, the `KafkaProducer` class from the `kafka` library is employed to establish a connection to the Kafka brokers and perform message sending operations.\n\n## Function: `main()`\nThe `main` function encapsulates the primary functionality of the script. Below is a breakdown of its tasks:\n\n### Kafka Producer Initialization\n1. **Producer Creation**: The script initializes a Kafka producer with the following configurations:\n   - **`bootstrap_servers`**: Specifies the address of the Kafka broker (in this case, `localhost:9092`).\n   - **`value_serializer`**: A lambda function that converts Python objects to JSON format, followed by encoding the string in UTF-8. This ensures that the data format is acceptable for Kafka.\n\n### Reading CSV File\n2. **CSV File Path**: The variable `csv_file` is set to the path of the CSV file that contains the green taxi trip data. Users are prompted to modify this path if needed.\n\n3. **Open CSV File**: The script uses a `with` statement to open the specified CSV file. This ensures the file is properly managed and closed after its content has been read.\n\n4. **CSV Reader**: A `csv.DictReader` is employed to read the contents of the CSV file. This reader converts each row into a dictionary where keys are the CSV headers, allowing easy access to the data.\n\n### Sending Data to Kafka\n5. **Iterate Through CSV Rows**: The script iterates through each row of the CSV file. For each row:\n   - It sends the entire row (as a dictionary) to the Kafka topic \"green-data\" via the `producer.send()` method. This allows for all relevant data from the trip to be captured and processed.\n\n### Finalizing the Producer\n6. **Flush and Close**: After all rows have been processed and sent to Kafka, the script calls `producer.flush()` to ensure that any remaining messages in the queue are sent to Kafka. Finally, it closes the producer connection using `producer.close()`.\n\n## Execution Context\n- **Conditional Execution**: The script includes the standard Python idiom `if __name__ == \"__main__\":` to ensure that the `main` function is only invoked when the script is run directly, and not when imported as a module in another script.\n\n## Summary\nIn essence, this script automates the process of ingesting green taxi trip data from a CSV file into a Kafka topic. By structuring the code this way, it promotes easy modification for different datasets or Kafka configurations without requiring substantial changes to the core logic. This makes it suitable for use in data streaming applications where real-time data processing is essential.",
    "filename": "06-streaming/pyflink/src/producers/load_taxi_data.py"
  },
  {
    "code": false,
    "content": "# Kafka Producer Script Documentation\n\n## Overview\nThe provided script establishes a producer that sends messages to a Kafka topic named `test-topic`. It generates and sends a series of messages containing test data and a timestamp, which are serialized in JSON format. The script utilizes the Kafka Python client to interact with the Kafka broker, configured to run on `localhost:9092`.\n\n## Imports\n```python\nimport json\nimport time\nfrom kafka import KafkaProducer\n```\nThis section imports necessary libraries:\n- `json`: for serializing Python objects into JSON format.\n- `time`: to manage and track timing-related operations.\n- `kafka.KafkaProducer`: for creating a Kafka producer that can send messages to a Kafka topic.\n\n## JSON Serialization Function\n```python\ndef json_serializer(data):\n    return json.dumps(data).encode('utf-8')\n```\n### Purpose\nThe `json_serializer` function converts a Python dictionary or list into a JSON formatted string and encodes it to UTF-8 bytes. \n\n### Role\nThis function is crucial for ensuring that messages sent to Kafka are in a suitable format (byte-encoded JSON) that Kafka can handle effectively.\n\n## Kafka Producer Configuration\n```python\nserver = 'localhost:9092'\n\nproducer = KafkaProducer(\n    bootstrap_servers=[server],\n    value_serializer=json_serializer\n)\n```\n### Purpose\nThis section initializes a Kafka producer with specific configurations:\n- The `server` variable specifies the Kafka broker address.\n- The `KafkaProducer` object is created with the `bootstrap_servers` parameter pointing to the Kafka server and utilizes the `json_serializer` for message serialization.\n\n### Role\nBy configuring these settings, the producer is prepared to send messages to the Kafka cluster, ensuring they are properly formatted in JSON.\n\n## Message Sending Loop\n```python\nt0 = time.time()\ntopic_name = 'test-topic'\n\nfor i in range(10, 1000):\n    message = {'test_data': i, 'event_timestamp': time.time() * 1000}\n    producer.send(topic_name, value=message)\n    print(f\"Sent: {message}\")\n    time.sleep(0.05)\n```\n### Purpose\nThis section contains a loop that generates and sends messages to the Kafka topic.\n\n### Step-by-step Breakdown\n1. **Timing Start**: The script records the starting time for tracking performance.\n2. **Loop Initialization**: It initiates a loop ranging from 10 to 999 (inclusive).\n3. **Message Creation**: Within each iteration:\n   - A dictionary is created containing:\n     - `'test_data'`: the current loop index `i`.\n     - `'event_timestamp'`: the current time in milliseconds.\n4. **Message Sending**: The `producer.send()` method sends the message to the `test-topic`.\n5. **Logging**: Each message sent is printed to the console for tracking.\n6. **Pause**: The script pauses for 0.05 seconds before the next iteration, controlling the message sending rate.\n\n## Producer Flush and Timing Calculation\n```python\nproducer.flush()\n\nt1 = time.time()\nprint(f'took {(t1 - t0):.2f} seconds')\n```\n### Purpose\n- **Flush**: The `producer.flush()` method ensures all buffered messages are sent to the Kafka topic, preventing data loss.\n- **Timing End**: The script captures the end time and calculates the total duration for sending messages.\n\n### Role\nThis part of the script verifies that all messages have been successfully transmitted before concluding the process, providing feedback on how long the entire operation took.\n\n## Conclusion\nIn summary, this script is a simple yet effective tool for sending a series of JSON-encoded messages to a Kafka topic. It demonstrates the usage of Kafka in Python, encapsulating message serialization, sending, and performance tracking neatly within a loop, while managing timing to control message throughput.",
    "filename": "06-streaming/pyflink/src/producers/producer.py"
  },
  {
    "content": "### Stream-Processing with Python\n\nIn this document, you will be finding information about stream processing \nusing different Python libraries (`kafka-python`,`confluent-kafka`,`pyspark`, `faust`).\n\nThis Python module can be separated in following modules.\n\n####  1. Docker\nDocker module includes, Dockerfiles and docker-compose definitions \nto run Kafka and Spark in a docker container. Setting up required services is\nthe prerequsite step for running following modules.\n\n#### 2. Kafka Producer - Consumer Examples\n- [Json Producer-Consumer Example](json_example) using `kafka-python` library\n- [Avro Producer-Consumer Example](avro_example) using `confluent-kafka` library\n\nBoth of these examples require, up-and running Kafka services, therefore please ensure\nfollowing steps under [docker-README](docker/README.md)\n\nTo run the producer-consumer examples in the respective example folder, run following commands\n```bash\n# Start producer script\npython3 producer.py\n# Start consumer script\npython3 consumer.py\n```",
    "filename": "06-streaming/python/README.md"
  },
  {
    "code": false,
    "content": "# RideAvroConsumer Documentation\n\n## Overview\nThe provided script defines a class `RideAvroConsumer` that is designed to consume messages from a Kafka topic. The messages are expected to be in Avro format, utilizing schema information from a Schema Registry. The consumer processes both keys and values of the messages and prints them in a formatted manner. This is achieved through deserialization of the Avro messages, allowing structured access to the data within.\n\n## Dependencies\nThe script relies on several libraries:\n- **os**: For file path operations.\n- **typing**: For type hinting (used for `Dict` and `List`).\n- **confluent_kafka**: A Kafka client that supports Avro serialization and deserialization, including:\n  - `Consumer`: To consume messages from Kafka.\n  - `SchemaRegistryClient`: To interact with a Schema Registry.\n  - `AvroDeserializer`: To deserialize Avro messages.\n  - `SerializationContext`: To provide context during deserialization.\n  - `MessageField`: To distinguish keys and values during deserialization.\n  \nIt also imports helper functions from local modules to assist in transforming dictionary data formats: `dict_to_ride_record_key` and `dict_to_ride_record`. Configuration values are imported from a `settings` module.\n\n## Class: RideAvroConsumer\nThe `RideAvroConsumer` class is the main component of the script, encapsulating the logic required to consume messages from a Kafka topic.\n\n### `__init__` Method\nThe constructor accepts a dictionary of properties (`props`) that configure the consumer and deserialization process.\n\n1. **Schema Loading**: It loads schemas for keys and values using the `load_schema` method.\n2. **Schema Registry Configuration**: Initializes a `SchemaRegistryClient` using the provided URL.\n3. **Deserializer Initialization**: Sets up Avro deserializers for both message keys and values:\n   - The key serializer utilizes `dict_to_ride_record_key` for conversion from dictionary to a specific key format.\n   - The value serializer uses `dict_to_ride_record` for similar purposes with the message values.\n4. **Consumer Properties**: Configures a Kafka `Consumer` with bootstrap servers, group ID, and offset reset options.\n\n### `load_schema` Method\nThis static method is responsible for reading Avro schema files from the filesystem.\n\n- It constructs the complete path for the schema file, reads its contents, and returns the schema string. The file paths are derived from the same directory as the script.\n\n## Method: consume_from_kafka\nThe `consume_from_kafka` method is responsible for listening to a specified Kafka topic and processing messages in a loop:\n\n1. **Subscription**: Subscribes to the provided list of topics to receive messages.\n2. **Polling Loop**: Continuously polls for new messages:\n   - A timeout of 1 second is specified to avoid blocking indefinitely.\n   - If a message is retrieved, it proceeds to deserialize both the key and value using the previously defined Avro deserializers.\n3. **Message Handling**: If deserialization produces a non-None record, it prints the key and record in a formatted string.\n4. **Graceful Exit**: The loop can be exited using a KeyboardInterrupt (e.g., Ctrl+C), after which the consumer is properly closed.\n\n## Execution Block\nThe code includes a conditional block that executes when the script is run as the main program:\n\n1. **Configuration Dictionary**: A dictionary (`config`) is created with properties required by the `RideAvroConsumer`:\n   - Bootstrap server address.\n   - URL for the Schema Registry.\n   - Paths to the Avro schema files for the key and value.\n  \n2. **Consumer Initialization**: An instance of `RideAvroConsumer` is created using this configuration dictionary.\n\n3. **Kafka Consumption**: The consumer begins listening to messages from the specified Kafka topic, as defined in the `KAFKA_TOPIC` constant.\n\n## Conclusion\nThe `RideAvroConsumer` class provides a structured approach to consuming Avro-formatted messages from a Kafka topic. Through the use of deserializers and a clean setup/configuration interface, it successfully abstracts the complexities involved in message handling and schema management, making it easier for developers to implement data streaming applications using Kafka and Avro serialization.",
    "filename": "06-streaming/python/avro_example/consumer.py"
  },
  {
    "code": false,
    "content": "# Documentation for RideAvroProducer Script\n\n## Overview\n\nThis script is designed as a Kafka producer that sends ride record data to a specified Kafka topic using Avro serialization. It reads records from a CSV file, serializes them according to predefined schemas, and publishes them to a Kafka topic, providing a delivery report on the success or failure of each message sent.\n\n## Key Imports\n\n1. **OS and CSV Libraries**: Used for file handling and parsing CSV files.\n2. **Confluent Kafka Library**: This library provides the necessary tools for producing messages to Kafka and for Avro serialization.\n3. **Custom Modules**: \n   - `ride_record_key` and `ride_record`: These modules define the data structures for the ride records and their keys.\n   - `settings`: Configuration parameters for Kafka, schema paths, and input data paths.\n\n## Delivery Report Function\n\n```python\ndef delivery_report(err, msg):\n    ...\n```\n\nThis function is called after attempting to send a message to Kafka. It checks for errors during the delivery process:\n- If an error occurred, it outputs an error message indicating which record failed.\n- If successful, it logs the details of the successfully produced record, including the topic, partition, and offset.\n\n## RideAvroProducer Class\n\n### Initialization\n\n```python\nclass RideAvroProducer:\n    def __init__(self, props: Dict):\n        ...\n```\n\nUpon instantiation, the `RideAvroProducer` class synchronizes with the schema registry and initializes the necessary serializers. It does the following:\n1. Loads the key and value schemas from the provided schema paths.\n2. Configures the Schema Registry client.\n3. Initializes Avro serializers for both keys and values.\n4. Sets up a Kafka producer with the provided bootstrap servers.\n\n### Load Schema\n\n```python\n@staticmethod\ndef load_schema(schema_path: str):\n    ...\n```\n\nThis static method reads and returns the content of a schema file given its path. It ensures that the producer can utilize appropriate serialization/deserialization schemas.\n\n### Read Records\n\n```python\n@staticmethod\ndef read_records(resource_path: str):\n    ...\n```\n\nThis static method reads ride records from a CSV file:\n- It skips the header row and processes each subsequent row.\n- Constructs `RideRecord` and `RideRecordKey` instances based on specific columns of the CSV data.\n- Returns a zipped list of keys and values.\n\n### Publish Method\n\n```python\ndef publish(self, topic: str, records: [RideRecordKey, RideRecord]):\n    ...\n```\n\nThe `publish` method takes a Kafka topic and a series of ride records as inputs. It processes each record:\n1. Serializes the key and value using the Avro serializers.\n2. Sends each record to Kafka with an associated delivery report callback.\n3. It catches any exceptions and breaks on a keyboard interrupt.\n4. After sending all records, it flushes the producer, ensuring that all messages are delivered before closing, and introduces a brief sleep to manage the flow.\n\n## Main Execution Block\n\n```python\nif __name__ == \"__main__\":\n    ...\n```\n\nWhen the script is executed directly:\n1. It constructs a configuration dictionary with necessary Kafka and schema registry properties.\n2. Instantiates a `RideAvroProducer` object with this configuration.\n3. Calls the `read_records` method to fetch ride records from the specified CSV file.\n4. Publishes the read records to the specified Kafka topic using the `publish` method.\n\n## Summary\n\nThis script effectively integrates with Kafka and Avro serialization, showcasing how to read data from a CSV, serialize it, and send it to a Kafka topic. The `RideAvroProducer` class encapsulates all functionalities related to Kafka production, focusing on schema management, message publishing, and handling delivery reports. This composition supports a clean and extensible approach for producing ride record data in a real-time streaming application.",
    "filename": "06-streaming/python/avro_example/producer.py"
  },
  {
    "code": false,
    "content": "# Ride Record Module Documentation\n\nThis module defines a class for handling ride-sharing records, as well as functions to convert between dictionary representations and instances of the class. Below is a detailed description of the code's structure, functionalities, and purpose.\n\n## Class: RideRecord\n\nThe core component of the module is the `RideRecord` class, which encapsulates the data related to a ride.\n\n### Initialization Method (`__init__`)\n\n```python\ndef __init__(self, arr: List[str]):\n```\n\n- **Purpose:** Initializes a new instance of the `RideRecord` class using a list of string values.\n- **Parameters:** \n  - `arr`: A list of strings containing ride-related data in the following order: `vendor_id`, `passenger_count`, `trip_distance`, `payment_type`, and `total_amount`.\n- **Data Conversion:** Each element is converted to its appropriate type (integer for `vendor_id`, `passenger_count`, `payment_type`, and float for `trip_distance` and `total_amount`).\n\n### Class Method: `from_dict`\n\n```python\n@classmethod\ndef from_dict(cls, d: Dict):\n```\n\n- **Purpose:** Creates a new instance of `RideRecord` from a dictionary.\n- **Parameters:**\n  - `cls`: The class itself (used for instantiation).\n  - `d`: A dictionary containing ride data with keys `'vendor_id'`, `'passenger_count'`, `'trip_distance'`, `'payment_type'`, and `'total_amount'`.\n- **Return:** Returns a new `RideRecord` instance constructed from the provided dictionary.\n\n### Representation Method (`__repr__`)\n\n```python\ndef __repr__(self):\n```\n\n- **Purpose:** Returns a string representation of the `RideRecord` instance.\n- **Return:** A string that includes the class name and the object's dictionary representation, providing a convenient way to inspect the instance during debugging.\n\n## Functions for Conversion\n\nIn addition to the `RideRecord` class, there are two functions that facilitate the conversion between `RideRecord` instances and dictionaries, which is useful for serialization and deserialization processes.\n\n### Function: `dict_to_ride_record`\n\n```python\ndef dict_to_ride_record(obj, ctx):\n```\n\n- **Purpose:** Converts a dictionary representation of a ride record into a `RideRecord` instance.\n- **Parameters:**\n  - `obj`: The dictionary representing the ride record.\n  - `ctx`: Context parameter (not used in the function).\n- **Return:** Returns a `RideRecord` instance created using the `from_dict` class method. If `obj` is `None`, it simply returns `None`.\n\n### Function: `ride_record_to_dict`\n\n```python\ndef ride_record_to_dict(ride_record: RideRecord, ctx):\n```\n\n- **Purpose:** Converts a `RideRecord` instance into its dictionary representation.\n- **Parameters:**\n  - `ride_record`: The `RideRecord` instance to be converted.\n  - `ctx`: Context parameter (not used in the function).\n- **Return:** Returns the `__dict__` attribute of the `ride_record`, which is a dictionary of the instance's attributes and their values.\n\n## Summary\n\nThis module provides a structured approach to handle ride records in a ride-sharing context. The `RideRecord` class not only defines the structure for storing essential information about each ride but also provides methods for instantiation through either a list of strings or a dictionary. The accompanying functions allow for easy interchange between the internal representation of ride records and their dictionary forms, making it suitable for tasks such as data serialization, storage, or transmission over a network.",
    "filename": "06-streaming/python/avro_example/ride_record.py"
  },
  {
    "code": false,
    "content": "# Documentation for the RideRecordKey Code\n\n## Overview\nThe provided code defines a simple data structure for handling ride record keys, primarily focusing on the concept of a `vendor_id`. It consists of a class `RideRecordKey`, along with utility functions that facilitate the conversion between dictionary representations and instances of this class.\n\n## Class: RideRecordKey\n### Purpose\nThe `RideRecordKey` class models a key for ride records, encapsulating a single attribute: `vendor_id`. This class is designed to allow easy creation and representation of ride record keys.\n\n### Attributes\n- `vendor_id`: This attribute holds the identifier for the ride vendor. It is expected to be passed during the instantiation of the class.\n\n### Methods\n\n#### `__init__(self, vendor_id)`\n- **Description**: This constructor method initializes a new instance of `RideRecordKey` with the provided `vendor_id`. \n\n#### `from_dict(cls, d: Dict)`\n- **Description**: A class method that creates an instance of `RideRecordKey` from a dictionary. It extracts the vendor ID from the dictionary using the key `'vendor_id'`.\n- **Parameters**:\n  - `cls`: The class reference (used for creating new instances).\n  - `d`: A dictionary containing the ride record key data.\n- **Returns**: An instance of `RideRecordKey`.\n\n#### `__repr__(self)`\n- **Description**: A representation method providing a string representation of the instance. It outputs the class name and the instance's current state in a dictionary format.\n- **Returns**: A formatted string containing the class name and its attributes.\n\n## Functions\n\n### `dict_to_ride_record_key(obj, ctx)`\n- **Purpose**: This function transforms a dictionary representation of a ride record key back into a `RideRecordKey` instance.\n- **Parameters**:\n  - `obj`: The input dictionary which may be `None`.\n  - `ctx`: An additional context parameter, its purpose is not defined in the provided fragment.\n- **Returns**: An instance of `RideRecordKey` if `obj` is not `None`; otherwise returns `None`.\n\n### `ride_record_key_to_dict(ride_record_key: RideRecordKey, ctx)`\n- **Purpose**: Converts an instance of `RideRecordKey` back into its dictionary representation.\n- **Parameters**:\n  - `ride_record_key`: An instance of `RideRecordKey` that needs to be converted to a dictionary.\n  - `ctx`: Again, an additional context parameter, purpose unspecified.\n- **Returns**: A dictionary representation of the `RideRecordKey` instance, specifically the `__dict__` attribute containing its attributes.\n\n## Summary\nThe provided code is a structured implementation that allows the creation, representation, and conversion of `RideRecordKey` instances. It utilizes methods that facilitate the interchange between dictionary data and class instances, making it suitable for applications where ride record data management and storage is required. These functionalities ensure that ride record keys are easily handled in different contexts, particularly where data serialization might be needed. \n\nOverall, this code serves as a foundational component for applications that need to store or manipulate ride-related data efficiently, focusing primarily on the vendor identification aspect.",
    "filename": "06-streaming/python/avro_example/ride_record_key.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis code snippet is a configuration setup that defines file paths, schema registry details, and Kafka broker information for processing ride-sharing data using Apache Kafka and Avro schema format. It outlines the necessary components required to interact with a Kafka topic designed to handle data related to taxi rides.\n\n\n## File Paths\n\n### Input Data Path\n\n```python\nINPUT_DATA_PATH = '../resources/rides.csv'\n```\n\nThis variable specifies the location of a CSV file that contains ride-sharing data. The path is relative to the current working directory, directing to the `resources` directory where the `rides.csv` file is stored. This file likely contains structured data pertaining to individual taxi rides, such as timestamps, passenger counts, pickup and drop-off locations, and fare information.\n\n\n### Schema Paths\n\n```python\nRIDE_KEY_SCHEMA_PATH = '../resources/schemas/taxi_ride_key.avsc'\nRIDE_VALUE_SCHEMA_PATH = '../resources/schemas/taxi_ride_value.avsc'\n```\n\nThese variables define paths to Avro schema files used to enforce data structure and validation for the ride-sharing data. \n\n- **Key Schema** (`taxi_ride_key.avsc`): This schema is likely responsible for defining the structure of data keys used in Kafka, typically to identify unique records.\n  \n- **Value Schema** (`taxi_ride_value.avsc`): This schema describes the structure of the data values (payload) carried by the records relating to taxi rides, dictating what fields should be included and their types (e.g., strings, integers, dates).\n\n\n## Kafka Configuration\n\n### Schema Registry URL\n\n```python\nSCHEMA_REGISTRY_URL = 'http://localhost:8081'\n```\n\nThis variable indicates the URL of the Schema Registry, which is essential for managing and retrieving Avro schemas dynamically. The Schema Registry exists as a service that allows producers and consumers of Kafka messages to store and retrieve schema definitions. The URL points to a local instance, suggesting that the schema registry service is running on the same machine.\n\n\n### Bootstrap Servers\n\n```python\nBOOTSTRAP_SERVERS = 'localhost:9092'\n```\n\nThis variable specifies the address of the Kafka broker(s) that the application will connect to for producing and consuming messages. In this case, it points to a local Kafka server running on port 9092. This is the entry point for communication with the Kafka ecosystem.\n\n\n### Kafka Topic\n\n```python\nKAFKA_TOPIC = 'rides_avro'\n```\n\nThis variable defines the Kafka topic named `rides_avro`, where the ride-sharing data will be published and consumed. Topics in Kafka serve as categories or feeds to which records are sent, and they play a critical role in how data is organized and processed.\n\n\n## Summary\n\nIn summary, this code snippet serves as a fundamental configuration block for an application that aims to process taxi ride data using Kafka and Avro for serialization. The specified file paths, schema registry URL, bootstrap servers, and Kafka topic provide the necessary parameters for the program to effectively read ride data from a CSV file, validate it against defined schemas, and interact with Kafka's messaging system for data flow.",
    "filename": "06-streaming/python/avro_example/settings.py"
  },
  {
    "content": "# Running Spark and Kafka Clusters on Docker\n\n### 1. Build Required Images for running Spark\n\nThe details of how to spark-images are build in different layers can be created can be read through \nthe blog post written by Andr\u00e9 Perez on [Medium blog -Towards Data Science](https://towardsdatascience.com/apache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445)\n\n```bash\n# Build Spark Images\n./build.sh \n```\n\n### 2. Create Docker Network & Volume\n\n```bash\n# Create Network\ndocker network  create kafka-spark-network\n\n# Create Volume\ndocker volume create --name=hadoop-distributed-file-system\n```\n\n### 3. Run Services on Docker\n```bash\n# Start Docker-Compose (within for kafka and spark folders)\ndocker compose up -d\n```\nIn depth explanation of [Kafka Listeners](https://www.confluent.io/blog/kafka-listeners-explained/)\n\nExplanation of [Kafka Listeners](https://www.confluent.io/blog/kafka-listeners-explained/)\n\n### 4. Stop Services on Docker\n```bash\n# Stop Docker-Compose (within for kafka and spark folders)\ndocker compose down\n```\n\n### 5. Helpful Comands\n```bash\n# Delete all Containers\ndocker rm -f $(docker ps -a -q)\n\n# Delete all volumes\ndocker volume rm $(docker volume ls -q)\n```",
    "filename": "06-streaming/python/docker/README.md"
  },
  {
    "code": false,
    "content": "# Kafka JSON Consumer Documentation\n\nThis document provides a high-level overview of a Python script that implements a Kafka consumer for JSON messages. The script utilizes the `kafka-python` library to connect to a Kafka broker, consume messages from a specified topic, and deserialize the JSON messages into Python objects. \n\n## Overview\n\nThe script consists of a **JsonConsumer** class responsible for connecting to Kafka and consuming messages, and a stand-alone execution block that initializes and runs the consumer. The primary focus of this code is to read messages from a Kafka topic, print the keys and values of the messages, and deserialize the JSON data into `Ride` objects for further processing.\n\n## Key Components\n\n### Imports\n\nThe necessary modules and classes are imported at the beginning of the script:\n- `Dict` and `List` from the `typing` module for type hinting.\n- `loads` from the `json` module for JSON deserialization.\n- `KafkaConsumer` from the `kafka` package to facilitate interaction with the Kafka messaging system.\n- The `Ride` class from a local module that likely defines how to structure the ride data.\n- `BOOTSTRAP_SERVERS` and `KAFKA_TOPIC` constants, presumably defined in a `settings` module, which configure the Kafka connection and topic.\n\n### JsonConsumer Class\n\nThe **JsonConsumer** class encapsulates the logic for consuming messages from Kafka:\n\n- **`__init__` method**: \n  - The constructor initializes a Kafka consumer instance using the provided properties (props) dictionary, which includes configuration settings like server details and deserialization functions. \n\n- **`consume_from_kafka` method**: \n  - This method manages the workflow of subscribing to specified Kafka topics and consuming messages in a loop. \n  - The consumer subscribes to the given topics and starts a loop to poll for messages every second.\n  - If a keyboard interrupt (SIGINT) occurs, the loop exits gracefully, and the consumer closes the connection. \n  - The messages are printed out, specifically their keys and values, which helps in monitoring or debugging received messages.\n\n### Main Execution Block\n\nThis section of the code is executed when the script is run directly:\n\n- **Configuration Dictionary**:\n  - A dictionary named `config` is created with key-value pairs defining various Kafka consumer settings, such as:\n    - `bootstrap_servers`: The Kafka server(s) to connect to.\n    - `auto_offset_reset`: Defines the behavior for retrieving messages when there is no prior offset.\n    - `enable_auto_commit`: Specifies whether message offsets should be committed automatically.\n    - `key_deserializer`: A lambda function to deserialize message keys from byte format to integers.\n    - `value_deserializer`: A lambda function to deserialize message values from byte format into `Ride` objects using the `loads` function.\n    - `group_id`: Identifies the consumer group to which this consumer belongs.\n\n- **Instantiating the Consumer**:\n  - An instance of the **JsonConsumer** class is created using the `config` dictionary, allowing it to manage message consumption.\n\n- **Starting Consumption**:\n  - The `consume_from_kafka` method is called with the Kafka topic specified in `KAFKA_TOPIC`, starting the consumer loop to process incoming messages.\n\n## Conclusion\n\nThis script serves as a basic but efficient Kafka consumer tailored to handle JSON messages, deserialize them into Python objects, and print useful information to the console. The use of customization through deserializers and configuration settings allows for flexibility and adaptability to varying Kafka message structures and requirements. Its clean structure and separation of responsibilities make it straightforward for developers to extend or modify for specific use cases, such as logging, data processing, or error handling.",
    "filename": "06-streaming/python/json_example/consumer.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis script sets up a Kafka producer that reads ride data from a CSV file and publishes it to a specified Kafka topic after converting the data into JSON format. It utilizes the `kafka-python` library to interact with Kafka, and the `Ride` class is presumably defined in another module, modeling the ride data structure. This script emphasizes modularity through the use of a dedicated class for producing JSON data and encapsulating the related logic.\n\n## Imports and Dependencies\n\nThe script imports several modules:\n- **csv**: To handle CSV file operations, allowing the reading of ride data from a CSV file.\n- **json**: For converting objects into JSON format to be sent to Kafka.\n- **List** and **Dict**: Type hinting provided by the `typing` library for better code clarity.\n- **KafkaProducer** and **KafkaTimeoutError**: Classes from the `kafka` module for producing messages to Kafka topics and handling potential timeout errors.\n- **Ride**: A custom class presumably defined in another module, which represents the structure of a ride.\n- **settings**: A module that includes configuration settings like Kafka server addresses, CSV input path, and Kafka topic.\n\n## JsonProducer Class\n\n### Overview\n\nThe `JsonProducer` class inherits from `KafkaProducer`. It serves as a specialized producer that can handle JSON serialization of ride data.\n\n### Initialization\n\n```python\ndef __init__(self, props: Dict):\n    self.producer = KafkaProducer(**props)\n```\n\nThe constructor initializes the `JsonProducer` class by accepting a dictionary of properties (`props`) required for the `KafkaProducer`. This allows it to configure the Kafka connection according to the provided settings.\n\n### Read Records Method\n\n```python\n@staticmethod\ndef read_records(resource_path: str):\n    ...\n```\n\n- **Purpose**: This static method reads ride records from a specified CSV file.\n- **Functionality**:\n  - Opens the CSV file and uses a CSV reader to read its contents.\n  - Skips the header row.\n  - Converts each row into a `Ride` object (presumably mapping the columns to attributes of `Ride`) and accumulates these objects in a list to be returned.\n\n### Publish Rides Method\n\n```python\ndef publish_rides(self, topic: str, messages: List[Ride]):\n    ...\n```\n\n- **Purpose**: This method publishes messages to a specified Kafka topic.\n- **Functionality**:\n  - Iterates through a list of `Ride` objects.\n  - Sends each ride to the defined Kafka topic using the ride's `pu_location_id` as the key.\n  - Handles exceptions, specifically `KafkaTimeoutError`, and outputs error messages if sending fails.\n  - Prints a success message with the offset of the produced record for successful publish events.\n\n## Main Execution Block\n\n```python\nif __name__ == '__main__':\n    ...\n```\n\n### Configuration Setup\n\nA configuration dictionary is defined to match the expectations of the `KafkaProducer`:\n- **bootstrap_servers**: The Kafka server connection details are imported from a settings file.\n- **key_serializer**: A lambda function defining how to serialize keys (converts to bytes).\n- **value_serializer**: A lambda function that serializes `Ride` objects into JSON format, converting the object's dictionary representation to UTF-8 bytes.\n\n### Producer Initialization and Operation\n\n- An instance of `JsonProducer` is created with the configuration.\n- The ride data is read from the CSV file located at `INPUT_DATA_PATH`.\n- Finally, the read rides are published to the Kafka topic defined by `KAFKA_TOPIC`.\n\n## Conclusion\n\nThis script effectively demonstrates how to read structured data from CSV files, transform it into a proper format for messaging systems, and send it to Kafka. The use of classes and methods also enhances the modularity and reusability of the code, making it easier to maintain and extend in the future.",
    "filename": "06-streaming/python/json_example/producer.py"
  },
  {
    "code": false,
    "content": "# Ride Class Documentation\n\n## Overview\nThe provided code defines a `Ride` class designed to model and manage ride-hailing trip data. The class is primarily structured to parse data from a list or a dictionary, encapsulating various attributes that pertain to a ride. The attributes include details such as vendor identification, pickup and dropoff times, passenger count, trip distance, and various fare components.\n\n## Class Definition\n### `Ride` Class\nThe `Ride` class includes multiple methods and attributes that enable the representation and manipulation of ride data.\n\n### Attributes\nThe attributes of the `Ride` class are initialized through the constructor (`__init__`) and include the following:\n\n- `vendor_id`: A string representing the ID of the vendor providing the ride.\n- `tpep_pickup_datetime`: A `datetime` object representing the time the ride was picked up, parsed from a string input.\n- `tpep_dropoff_datetime`: A `datetime` object representing the time the ride was dropped off, also parsed from a string input.\n- `passenger_count`: An integer indicating the number of passengers in the ride.\n- `trip_distance`: A `Decimal` representing the distance of the trip.\n- `rate_code_id`: An integer representing the rate code associated with the ride.\n- `store_and_fwd_flag`: A string indicating whether the ride data was stored and forwarded.\n- `pu_location_id`: An integer that represents the pickup location ID.\n- `do_location_id`: An integer that represents the dropoff location ID.\n- `payment_type`: A string indicating the method of payment for the ride.\n- `fare_amount`: A `Decimal` representing the fare charged for the ride.\n- `extra`: A `Decimal` indicating any additional charges.\n- `mta_tax`: A `Decimal` representing the Metropolitan Transportation Authority tax.\n- `tip_amount`: A `Decimal` indicating the tip given for the ride.\n- `tolls_amount`: A `Decimal` representing the total tolls incurred during the ride.\n- `improvement_surcharge`: A `Decimal` indicating any improvement surcharges.\n- `total_amount`: A `Decimal` that sums up all the costs associated with the ride.\n- `congestion_surcharge`: A `Decimal` indicating any surcharges for congestion.\n\n## Initialization Method\n### `__init__(self, arr: List[str])`\nThis method takes a list of strings as input (presumably parsed from CSV or similar data), assigns the elements to the corresponding attributes, and performs necessary type conversions. The `datetime.strptime` function is utilized to convert pickup and dropoff datetime strings into `datetime` objects, while other attributes are cast to their respective types.\n\n## Class Method\n### `from_dict(cls, d: Dict)`\nThis class method provides an alternative way to create an instance of the `Ride` class from a dictionary. It extracts the required values from the dictionary, ensuring they are presented in the format expected by the constructor (`__init__`). The method uses the keys from the input dictionary to build a list, which it then passes to the constructor.\n\n## Representation Method\n### `__repr__(self)`\nThe `__repr__` method is defined to provide a clear textual representation of the `Ride` object, outputting the class name and its current state (i.e., the attribute dictionary). This is particularly useful for debugging, as it allows easy inspection of the object\u2019s attributes.\n\n## Conclusion\nThe `Ride` class is an efficient structure for managing ride-hailing trip data, encapsulating relevant attributes, and providing methods for instantiation from both lists and dictionaries. Its design facilitates easy onboarding of data into the system, ensuring robust type handling and clear representations for usage and debugging. Overall, this class serves as a critical component for applications that require ride data management, such as analytics, reporting, or real-time processing.",
    "filename": "06-streaming/python/json_example/ride.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis section of code defines constants relevant to a data processing pipeline, specifically for handling ride-sharing data. It establishes file paths for input data, as well as configuration settings for a Kafka messaging system.\n\n## Constants Defined\n\n### Input Data Path\n\n```python\nINPUT_DATA_PATH = '../resources/rides.csv'\n```\n\n- **Purpose**: This constant sets the file path to the CSV file containing ride data.\n- **Implication**: The relative path suggests that the CSV file is located in the `resources` directory, one level up from the current directory. This file presumably contains structured data from a ride-sharing application, which may include details like ride times, locations, user information, etc.\n\n### Kafka Configuration\n\n```python\nBOOTSTRAP_SERVERS = ['localhost:9092']\nKAFKA_TOPIC = 'rides_json'\n```\n\n- **BOOTSTRAP_SERVERS**: \n  - **Purpose**: This constant defines the address of the Kafka server (the message broker) that will be used in the data processing pipeline.\n  - **Implication**: The setting `localhost:9092` indicates that the Kafka server is hosted locally and is listening on the default port `9092`. This is commonly used during development or in environments where the Kafka service is temporarily deployed locally.\n\n- **KAFKA_TOPIC**:\n  - **Purpose**: This constant specifies the Kafka topic to which data will be published or from which it will be consumed.\n  - **Implication**: The topic named `rides_json` signifies that the messages exchanged within this topic relate to JSON representations of ride data. This is likely part of a system that enables various components or microservices to communicate ride-related updates or queries.\n\n## Summary of Roles\n\nOverall, the provided piece of code lays the foundation necessary for reading ride-sharing data and streaming it through a Kafka messaging system. The structured constants allow for flexibility, as they can easily be modified to reflect different data sources or configurations without altering the core logic of the application.\n\nThese constants will typically be used in the later stages of a script or application where data parsing, transformation, and sending to the Kafka topic occur. The use of a CSV file for input suggests a batch processing approach, which may be complemented by a streaming architecture leveraging Kafka for real-time updates or analytics.\n\n### Next Steps\n\nIn a complete implementation, you would expect to see additional code that handles:\n\n1. **Reading the CSV file**: Reading and parsing the data from `rides.csv`.\n2. **Data Transformation**: Converting this data into a suitable format, likely JSON, as denoted by the topic name.\n3. **Publishing to Kafka**: Sending the transformed data to the Kafka topic specified.\n\nThis setup hints at a typical data engineering or data pipeline use case, where ride data is collected, processed, and made available for real-time applications or analytics purposes.",
    "filename": "06-streaming/python/json_example/settings.py"
  },
  {
    "content": "# Basic PubSub example with Redpanda\n\nThe aim of this module is to have a good grasp on the foundation of these Kafka/Redpanda concepts, to be able to submit a capstone project using streaming:\n- clusters\n- brokers\n- topics\n- producers\n- consumers and consumer groups\n- data serialization and deserialization\n- replication and retention\n- offsets\n- consumer-groups\n- \n\n## 1. Pre-requisites\n\nIf you have been following the [module-06](./../../../06-streaming/README.md) videos, you might already have installed the `kafka-python` library, so you can move on to [Docker](#2-docker) section.\n\nIf you have not, this is the only package you need to install in your virtual environment for this Redpanda lesson. \n\n1. activate your environment\n2. `pip install kafka-python`\n\n## 2. Docker\n\nStart a Redpanda cluster. Redpanda is a single binary image, so it is very easy to start learning kafka concepts with Redpanda.\n\n```bash\ncd 06-streaming/python/redpanda_example/\ndocker-compose up -d\n```\n\n## 3. Set RPK alias\n\nRedpanda has a console command `rpk` which means `Redpanda keeper`, the CLI tool that ships with Redpanda and is already available in the Docker image. \n\nSet the following `rpk` alias so we can use it from our terminal, without having to open a Docker interactive terminal. We can use this `rpk` alias directly in our terminal. \n\n```bash\nalias rpk=\"docker exec -ti redpanda-1 rpk\"\nrpk version\n```\n\nAt this time, the verion is shown as `v23.2.26 (rev 328d83a06e)`. The important version munber is the major one `v23` following the versioning semantics `major.minor[.build[.revision]]`, to ensure that you get the same results as whatever is shared in this document.\n\n> [!TIP]\n> If you're reading this after Mar, 2024 and want to update the Docker file to use the latest Redpanda images, just visit [Docker hub](https://hub.docker.com/r/vectorized/redpanda/tags), and paste the new version number.\n\n\n## 4. Kafka Producer - Consumer Examples\n\nTo run the producer-consumer examples, open 2 shell terminals in 2 side-by-side tabs and run following commands. Be sure to activate your virtual environment in each terminal.\n\n```bash\n# Start consumer script, in 1st terminal tab\npython -m consumer.py\n# Start producer script, in 2nd terminal tab\npython -m producer.py\n```\n\nRun the `python -m producer.py` command again (and again) to observe that the `consumer` worker tab would automatically consume messages in real-time when new `events` occur\n\n## 5. Redpanda UI\n\nYou can also see the clusters, topics, etc from the Redpanda Console UI via your browser at [http://localhost:8080](http://localhost:8080)\n\n\n## 6. rpk commands glossary\n\nVisit [get-started-rpk blog post](https://redpanda.com/blog/get-started-rpk-manage-streaming-data-clusters) for more.\n\n```bash\n# set alias for rpk\nalias rpk=\"docker exec -ti redpanda-1 rpk\"\n\n# get info on cluster\nrpk cluster info\n\n# create topic_name with m partitions and n replication factor\nrpk topic create [topic_name] --partitions m --replicas n\n\n# get list of available topics, without extra details and with details\nrpk topic list\nrpk topic list --detailed\n\n# inspect topic config\nrpk topic describe [topic_name]\n\n# consume [topic_name]\nrpk topic consume [topic_name]\n\n# list the consumer groups in a Redpanda cluster\nrpk group list\n\n# get additional information about a consumer group, from above listed result\nrpk group describe my-group\n```\n\n## 7. Additional Resources\n\nRedpanda Univerity (needs a Redpanda account and it is free to enrol and do the course(s))\n- [RP101: Getting Started with Redpanda](https://university.redpanda.com/courses/hands-on-redpanda-getting-started)\n- [RP102: Stream Processing with Redpanda](https://university.redpanda.com/courses/take/hands-on-redpanda-stream-processing/lessons/37830192-intro)\n- [SF101: Streaming Fundamentals](https://university.redpanda.com/courses/streaming-fundamentals)\n- [SF102: Kafka building blocks](https://university.redpanda.com/courses/kafka-building-blocks)\n\nIf you feel that you already have a good foundational basis on Streaming and Kafka, feel free to skip these supplementary courses.",
    "filename": "06-streaming/python/redpanda_example/README.md"
  },
  {
    "code": false,
    "content": "# Kafka JSON Consumer Documentation\n\nThis document provides a high-level overview of a Kafka JSON consumer implemented using Python. The code is designed to consume messages from a Kafka topic, deserialize them into Python objects, and print key-value pairs to the console. Below is a structured breakdown of the code.\n\n## Imports and Dependencies\n\nThe script begins by importing essential libraries:\n\n```python\nimport os\nfrom typing import Dict, List\nfrom json import loads\nfrom kafka import KafkaConsumer\n\nfrom ride import Ride\nfrom settings import BOOTSTRAP_SERVERS, KAFKA_TOPIC\n```\n\n1. **Standard Libraries**: \n    - `os`: Standard library for interacting with the operating system, though not explicitly used in the code.\n    - `typing`: Provides type hints for better readability and type checking. `Dict` and `List` are imported for type annotations.\n    - `json`: The `loads` function is used to deserialize JSON strings into Python objects.\n  \n2. **Kafka Library**: `KafkaConsumer` is imported from the `kafka` library for consuming messages from Kafka topics.\n\n3. **Local Modules**: \n    - `Ride`: A custom class presumably defined in a separate module, used to deserialize message values.\n    - `settings`: Contains application configuration such as bootstrap servers and Kafka topic names.\n\n## Class Definition: `JsonConsumer`\n\n### Purpose\n\nThe `JsonConsumer` class is designed to encapsulate the functionality required to create a Kafka consumer, subscribe to specified topics, and consume messages.\n\n### Constructor\n\n```python\ndef __init__(self, props: Dict):\n    self.consumer = KafkaConsumer(**props)\n```\n\n- The constructor initializes the Kafka consumer with properties passed as a `Dict`. This allows for flexible configuration.\n\n### Method: `consume_from_kafka`\n\n```python\ndef consume_from_kafka(self, topics: List[str]):\n```\n\n- This method subscribes to one or more Kafka topics and enters an infinite loop to consume messages continuously.\n\n#### Subscription and Initialization\n\n- The consumer subscribes to the provided topics and prints the available subscription topics to the console.\n\n```python\nself.consumer.subscribe(topics)\nprint('Consuming from Kafka started')\nprint('Available topics to consume: ', self.consumer.subscription())\n```\n\n#### Message Processing Loop\n\n1. **Polling for Messages**: \n   - The consumer polls for messages with a timeout of 1 second. If no messages are available, it continues to poll.\n   \n2. **Handling Messages**:\n   - If messages are retrieved, it iterates through them and prints the keys and values.\n\n3. **Graceful Exit**:\n   - The loop can be exited using a `KeyboardInterrupt` (e.g., Ctrl+C), after which the consumer is closed.\n\n```python\nwhile True:\n    try:\n        message = self.consumer.poll(1.0)\n        if message is None or message == {}:\n            continue\n        for message_key, message_value in message.items():\n            for msg_val in message_value:\n                print(msg_val.key, msg_val.value)\n    except KeyboardInterrupt:\n        break\n\nself.consumer.close()\n```\n\n## Main Execution Block\n\n### Configuration Setup\n\nThe script's execution starts with the `if __name__ == '__main__':` block, which initializes the Kafka consumer.\n\n```python\nconfig = {\n    'bootstrap_servers': BOOTSTRAP_SERVERS,\n    'auto_offset_reset': 'earliest',\n    'enable_auto_commit': True,\n    'key_deserializer': lambda key: int(key.decode('utf-8')),\n    'value_deserializer': lambda x: loads(x.decode('utf-8'), object_hook=lambda d: Ride.from_dict(d)),\n    'group_id': 'consumer.group.id.json-example.1',\n}\n```\n\n- **Bootstrap Servers**: Defines the Kafka server address(es).\n- **Auto Offset Reset**: Ensures consumption starts from the earliest message if no previous offset exists.\n- **Enable Auto Commit**: Allows offsets to be automatically committed after messages are processed.\n- **Deserializers**:\n    - Key deserializer converts message keys from bytes to integers.\n    - Value deserializer converts message values from JSON strings to `Ride` objects using a classmethod `from_dict`.\n\n### Consumer Instantiation and Execution\n\nThe consumer is instantiated with the configuration and begins consuming from the specified Kafka topic:\n\n```python\njson_consumer = JsonConsumer(props=config)\njson_consumer.consume_from_kafka(topics=[KAFKA_TOPIC])\n```\n\n## Conclusion and Considerations\n\nThe implemented `JsonConsumer` provides a repeatable and effective way to consume JSON-formatted messages from a Kafka topic. \n\n### Flexibility with JSON Schema\n\nThe code comments emphasize that JSON does not enforce a schema, which allows the `Ride` class to still function even if there are structural changes in the incoming data. However, this flexibility raises concerns for downstream analytics as the absence of specific fields can compromise the integrity of data used for reporting and analysis.\n\nOverall, the consumer plays a crucial role in ensuring real-time data flow, balancing flexibility with the risks posed by unstructured data handling.",
    "filename": "06-streaming/python/redpanda_example/consumer.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThis script primarily serves to read ride data from a CSV file, transform it into a specific format using the `Ride` class, and then publish this data to a Kafka topic. It leverages the KafkaProducer from the Kafka library to facilitate message queueing and delivery. The provided settings such as bootstrap servers and input data path are fetched from external configuration files.\n\n## Imports\nThe script begins by importing necessary modules and classes:\n- `csv`: For reading data from CSV files.\n- `json`: For serializing ride data into JSON format.\n- `List`, `Dict`: For type hinting, improving clarity of function signatures.\n- `KafkaProducer`, `KafkaTimeoutError`: For creating a producer instance to send messages to a Kafka topic and handling potential timeout exceptions.\n- `Ride`: A custom class that is presumed to encapsulate the details of a ride record.\n- `settings`: External settings importing for configuration constants.\n\n## Class: JsonProducer\nThe `JsonProducer` class extends `KafkaProducer` and encapsulates functionality specific to producing JSON serialized data to Kafka:\n\n### Constructor: `__init__`\n```python\ndef __init__(self, props: Dict):\n    self.producer = KafkaProducer(**props)\n```\n- Initializes the Kafka producer with provided properties defined in a dictionary, allowing for flexible and adjustable configurations.\n\n### Static Method: `read_records`\n```python\n@staticmethod\ndef read_records(resource_path: str):\n```\n- Accepts a file path as input and reads ride records from a CSV file.\n- **Functionality**:\n  - Opens the specified CSV file for reading.\n  - Uses the `csv.reader` to parse the file, skipping the header.\n  - For each row, it creates an instance of `Ride` initialized with the row data and collects them into a list.\n  \n### Method: `publish_rides`\n```python\ndef publish_rides(self, topic: str, messages: List[Ride]):\n```\n- Responsible for sending ride messages to a specified Kafka topic.\n- **Functionality**:\n  - Iterates over a list of `Ride` objects, converting each one into a message format suitable for Kafka.\n  - Calls `self.producer.send()` to publish each ride.\n  - Handles `KafkaTimeoutError` to catch errors during sending, logging the issue to the console alongside the affected ride's location ID.\n\n## Main Execution Block\n```python\nif __name__ == '__main__':\n```\n- This block ensures that the following code only runs when the script is executed as a standalone program and not when imported as a module.\n\n### Configuration Initialization\n```python\nconfig = {\n    'bootstrap_servers': BOOTSTRAP_SERVERS,\n    'key_serializer': lambda key: str(key).encode(),\n    'value_serializer': lambda x: json.dumps(x.__dict__, default=str).encode('utf-8')\n}\n```\n- Constructs a configuration dictionary containing:\n  - `bootstrap_servers`: The address(es) of Kafka brokers, loaded from the settings.\n  - `key_serializer`: A lambda function that encodes keys as byte strings for Kafka compatibility.\n  - `value_serializer`: A lambda function that converts objects to JSON format before sending to Kafka.\n\n### Producer Instantiation and Record Processing\n```python\nproducer = JsonProducer(props=config)\n```\n- Creates an instance of `JsonProducer` using the defined configuration.\n\n### Reading Records\n```python\nrides = producer.read_records(resource_path=INPUT_DATA_PATH)\n```\n- Calls the `read_records` method to load ride data from the specified CSV file path (retrieved from settings).\n\n### Publishing Rides to Kafka\n```python\nproducer.publish_rides(topic=KAFKA_TOPIC, messages=rides)\n```\n- Finally, the script calls the `publish_rides` method to send all the ride records to the predefined Kafka topic.\n\n## Summary\nIn summary, this script establishes a Kafka producer, reads ride data from a specified CSV file converting each row into `Ride` objects, and publishes these rides as JSON-serialized messages to a specified Kafka topic. It handles potential errors during the publishing process, making it robust for production scenarios.",
    "filename": "06-streaming/python/redpanda_example/producer.py"
  },
  {
    "code": false,
    "content": "# Overview of the Ride Class\n\nThe provided code defines a `Ride` class that models taxi ride data. It encapsulates various attributes related to a taxi ride, such as the vendor ID, pickup and drop-off times, passenger count, trip distance, fare details, and more. This class can be used to create objects that represent individual rides, either from a list of attributes or from a dictionary.\n\n## Class Initialization\n\n### Constructor\n\nThe `__init__` method is the constructor for the `Ride` class. It takes a list of string attributes `arr` that contains all the necessary details for a ride. The attributes are parsed and assigned to instance variables, including:\n\n- **vendor_id**: A string indicative of the vendor providing the service.\n- **tpep_pickup_datetime**: The date and time when the ride was picked up, converted to a `datetime` object.\n- **tpep_dropoff_datetime**: The date and time of ride drop-off, also converted to a `datetime` object.\n- **passenger_count**: The number of passengers in the ride, converted to an integer.\n- **trip_distance**: The distance of the trip, represented as a `Decimal` for precision.\n- **rate_code_id**: An integer representing the rate code applied to the ride.\n- **store_and_fwd_flag**: A flag indicating whether the ride data was stored and forwarded.\n- **pu_location_id**: The pickup location ID, converted to an integer.\n- **do_location_id**: The drop-off location ID, converted to an integer.\n- **payment_type**: The method of payment used for the trip.\n- **fare_amount**: The base fare amount of the ride, represented as a `Decimal`.\n- **extra**: Any additional charges.\n- **mta_tax**: The Metropolitan Transportation Authority tax.\n- **tip_amount**: The tip amount provided by the rider.\n- **tolls_amount**: The amount charged for tolls during the trip.\n- **improvement_surcharge**: Charges related to improvements.\n- **total_amount**: The total amount charged for the ride.\n- **congestion_surcharge**: An additional charge related to congestion.\n\nEach attribute is parsed and stored with the right type for ease of use in calculations and data retrieval.\n\n## Class Method\n\n### from_dict Method\n\nThe `from_dict` class method provides an alternative way to create a `Ride` object. Instead of passing a list, it takes a dictionary `d` where keys correspond to the attributes of the `Ride` class. This method organizes the data from the dictionary in the same manner as the constructor, and it allows more flexible data handling, like working with JSON-like structures.\n\nThe method constructs and returns a new `Ride` object by extracting values from the dictionary corresponding to each attribute. Each value is chosen carefully to maintain the expected type for that attribute.\n\n## Representation Method\n\n### __repr__ Method\n\nThe `__repr__` method provides a string representation of the `Ride` object. This is helpful for debugging and logging purposes, as it outputs the class name and a dictionary of all the instance variables and their current values. This representation enables easy inspection of the object's state, making it clearer when printed or logged.\n\n## Data Handling and Types\n\nThe class uses various data types, particularly:\n\n- **Datetime**: For storing date and time values.\n- **Decimal**: For monetary values and distances, ensuring precision during calculations.\n- **Integer and String**: For attributes that are inherently numeric or categorical.\n\nThis careful selection ensures that the data model can handle typical operations without introducing issues related to floating point precision or string handling.\n\n## Use Cases\n\nThis class can serve various applications related to taxi services or fleet management systems. By creating instances of `Ride`, developers can manipulate ride data, analyze patterns (like average fare or distance), or prepare reports for ride statistics. This encapsulated data model simplifies the processing of ride data and integrates easily into larger systems.\n\n## Summary\n\nIn summary, the `Ride` class is a well-defined structure for managing taxi ride information. It provides mechanisms to create an object from both lists and dictionaries, allowing for flexibility in data ingestion. The representation method enhances usability for debugging and provides a clear view of the object's attributes at any given moment.\n\nDevelopers can build upon this class further by adding methods for processing ride data or implementing custom business logic, such as fare calculations or distance validations, which can extend its functionality effectively.",
    "filename": "06-streaming/python/redpanda_example/ride.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThe provided code snippet appears to be the beginning of a script that is likely intended for processing ride data from a CSV file and possibly sending it to a Kafka message broker for further processing or analysis. Below is an analysis of the defined constants and their implied use within a larger context.\n\n## Constants Definition\n\n### Input Data Path\n```python\nINPUT_DATA_PATH = '../resources/rides.csv'\n```\n- This line defines a constant `INPUT_DATA_PATH`, which specifies the relative path to a CSV file named `rides.csv`. This file is expected to contain data about rides, potentially including various fields such as ride IDs, timestamps, start and end locations, fare amounts, etc.\n- The specified path suggests that the code may be part of a larger project structure where the `resources` directory is organized to store such data files.\n\n### Kafka Configuration\n```python\nBOOTSTRAP_SERVERS = ['localhost:9092']\nKAFKA_TOPIC = 'rides_json'\n```\n- The `BOOTSTRAP_SERVERS` constant defines a list containing a single entry: the address of a Kafka broker running on `localhost` at port `9092`. Kafka is a distributed event streaming platform widely used to build real-time data pipelines and streaming applications.\n- `KAFKA_TOPIC` is defined as `'rides_json'`, indicating that this is the topic to which processed ride data will be published. In Kafka, a topic serves as a category or feed name to which messages are published.\n\n## Implied Workflow\n\nGiven the constants defined, we can infer a likely workflow that this script is a part of:\n\n### Data Loading\n- The first step in the code (though not shown) will likely involve reading the ride data from the CSV file specified in `INPUT_DATA_PATH`.\n- This will typically involve using a library like `pandas` or the built-in `csv` module to load the data into memory, possibly into a DataFrame or a similar structure for easier manipulation.\n\n### Data Transformation\n- Once the data is loaded, the script would likely include logic for transforming the ride data into JSON format. JSON (JavaScript Object Notation) is a lightweight data interchange format that is easy to read and write for humans and machines alike.\n- This transformation is essential when preparing data for ingestion into a message broker like Kafka.\n\n### Kafka Publishing\n- After the data has been transformed into the necessary format, the next step would be to establish a connection to the Kafka broker as specified by the `BOOTSTRAP_SERVERS`.\n- The script would then create a producer instance that prepares to send messages to the Kafka topic defined by `KAFKA_TOPIC`.\n\n### Sending Messages\n- The script would loop over each entry (or ride) in the processed data and send each serialized JSON message to the Kafka topic. This allows for real-time processing and can facilitate further analysis or storage in systems designed to handle streaming data.\n\n### Error Handling and Logging\n- While not covered in the snippet, good practice in such scripts typically involves including error handling to manage issues related to data loading, transformation, and communication with Kafka.\n- Additionally, logging the success or failure of each operation is important for monitoring and debugging purposes.\n\n## Conclusion\n\nOverall, this code sets up the initial configurations needed to read ride data from a CSV file and send it to a Kafka topic after transforming it into the appropriate format. The constants defined indicate a focus on local development/testing, with an eye towards real-time data processing. This is a foundational step in constructing a data pipeline that could be expanded upon with additional functionality such as data validation, more complex transformations, or integration with other systems and services.",
    "filename": "06-streaming/python/redpanda_example/settings.py"
  },
  {
    "code": false,
    "content": "# Documentation of a Faust Application for Taxi Ride Processing\n\n## Overview\n\nThis Python script defines a Faust application aimed at processing taxi ride data streamed from a Kafka topic. The application filters rides based on their total fare amounts and directs them to corresponding topics: high amount rides (greater than or equal to $40) and low amount rides (less than $40).\n\n## Import Statements\n\n```python\nimport faust\nfrom taxi_rides import TaxiRide\nfrom faust import current_event\n```\n\nIn this section, the script imports necessary libraries and modules. The `faust` library is essential for building the stream processing application. The `TaxiRide` class is imported from the `taxi_rides` module, presumably representing the structure of the taxi ride data. The `current_event` function from `faust` is used to handle the current event being processed within the application stream.\n\n## Application Setup\n\n```python\napp = faust.App('datatalksclub.stream.v3', broker='kafka://localhost:9092', consumer_auto_offset_reset=\"earliest\")\n```\n\nThis line initializes a Faust application named `datatalksclub.stream.v3`. It specifies Kafka as the message broker running at `localhost:9092`. The `consumer_auto_offset_reset=\"earliest\"` argument indicates that, if there are no previous offsets, the consumer should start reading from the earliest available message.\n\n## Topic Definitions\n\n```python\ntopic = app.topic('datatalkclub.yellow_taxi_ride.json', value_type=TaxiRide)\nhigh_amount_rides = app.topic('datatalks.yellow_taxi_rides.high_amount')\nlow_amount_rides = app.topic('datatalks.yellow_taxi_rides.low_amount')\n```\n\nHere, three topics are defined. The main topic, `datatalkclub.yellow_taxi_ride.json`, is where the taxi ride data is streamed in JSON format, and the value type is specified as `TaxiRide`. Two additional topics, `high_amount_rides` and `low_amount_rides`, are defined to segregate high fare and low fare rides respectively.\n\n## Event Processing Agent\n\n```python\n@app.agent(topic)\nasync def process(stream):\n    async for event in stream:\n        if event.total_amount >= 40.0:\n            await current_event().forward(high_amount_rides)\n        else:\n            await current_event().forward(low_amount_rides)\n```\n\nThe `process` function is defined as an asynchronous agent that listens for incoming events from the specified topic. \n\n### Event Handling\n\n- The agent iterates over incoming `stream` events, which represent individual taxi rides.\n- For each `event`, it checks if the `total_amount` attribute is greater than or equal to $40. If it is, the event is forwarded to the `high_amount_rides` topic; otherwise, it is forwarded to the `low_amount_rides` topic.\n\nThis categorization allows subsequent processing or analysis to easily distinguish between higher-value and lower-value rides.\n\n## Main Execution Block\n\n```python\nif __name__ == '__main__':\n    app.main()\n```\n\nThis conditional block checks if the script is being run as the main program. If true, it initiates the Faust application by calling `app.main()`. Thus, the application will begin processing messages from Kafka as defined in the preceding code sections.\n\n## Summary\n\nIn summary, this script is a basic yet efficient stream processing application using Faust, designed to classify taxi ride data into high and low fare categories based on the total fare amount. By leveraging Kafka for real-time data handling, it can efficiently direct rides to the appropriate topics for further analysis or processing.",
    "filename": "06-streaming/python/streams-example/faust/branch_price.py"
  },
  {
    "code": false,
    "content": "# Kafka CSV Producer\n\nThis script is designed to read ride data from a CSV file and send it to a Kafka topic using a Kafka producer. It leverages the `csv`, `json`, and `kafka` libraries in Python, executing a straightforward process to facilitate the transfer of ride information.\n\n## Libraries and Dependencies\n\nThe script begins with the following imports:\n\n```python\nimport csv\nfrom json import dumps\nfrom kafka import KafkaProducer\nfrom time import sleep\n```\n\n- **csv**: This module is used for reading and parsing CSV files. It allows for ease of access to data structured in rows and columns.\n  \n- **json.dumps**: This function converts Python objects into JSON strings, making it suitable for serializing the data before sending it over Kafka.\n\n- **KafkaProducer**: This class from the `kafka` library is used to send messages to a Kafka broker. The producer will be configured later with serializers for the keys and values.\n\n- **time.sleep**: This function pauses the execution of the script for a specified amount of time, allowing for the control of message flow to the Kafka topic.\n\n## Kafka Producer Initialization\n\nA Kafka producer is initialized with specific configurations:\n\n```python\nproducer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n                         key_serializer=lambda x: dumps(x).encode('utf-8'),\n                         value_serializer=lambda x: dumps(x).encode('utf-8'))\n```\n\n- **bootstrap_servers**: This parameter specifies the address of the Kafka broker. In this case, it points to a local server running on port 9092.\n\n- **key_serializer**: This lambda function serializes the key into a JSON string format and encodes it in UTF-8. The serialization allows the key to be sent as a uniform message over Kafka.\n\n- **value_serializer**: Similar to the key serializer, this function serializes the value to JSON format and encodes it to ensure compatibility with Kafka message formats.\n\n## CSV File Handling\n\nNext, the script opens the CSV file that contains the ride data:\n\n```python\nfile = open('../../resources/rides.csv')\n```\n\n- The relative path indicates that the file is structured within a directory called `resources`. It is assumed that the CSV file contains relevant ride details, potentially structured with various columns for vendor ID, passenger count, trip distance, payment type, and total amount.\n\n## Reading and Processing the CSV\n\nThe script utilizes the `csv.reader` to read the CSV file's content:\n\n```python\ncsvreader = csv.reader(file)\nheader = next(csvreader)\n```\n\n- **header**: The first row of the CSV is typically reserved for column names. Using `next(csvreader)` retrieves this header row and is not subsequently processed.\n\n- The script then iterates over the subsequent rows in the CSV file:\n\n```python\nfor row in csvreader:\n```\n\n## Constructing Message Keys and Values\n\nWithin the loop, each row from the CSV is processed to create a key and value for sending to Kafka:\n\n```python\nkey = {\"vendorId\": int(row[0])}\nvalue = {\"vendorId\": int(row[0]), \"passenger_count\": int(row[3]), \"trip_distance\": float(row[4]), \"payment_type\": int(row[9]), \"total_amount\": float(row[16])}\n```\n\n- **key**: Created as a dictionary with the vendor ID (assumed to be in the first column of the CSV).\n\n- **value**: Constructed as another dictionary containing multiple fields relevant to the taxi ride:\n  - *vendorId*: Vendor ID cast to an integer.\n  - *passenger_count*: Number of passengers, also an integer.\n  - *trip_distance*: Distance of the trip, converted to a float.\n  - *payment_type*: Payment method stored as an integer.\n  - *total_amount*: Total fare for the ride, represented as a float.\n\n## Sending Data to Kafka\n\nThe constructed key and value are sent to the Kafka topic:\n\n```python\nproducer.send('datatalkclub.yellow_taxi_ride.json', value=value, key=key)\n```\n\n- The topic specified is `datatalkclub.yellow_taxi_ride.json`. This indicates that the script is targeting a predefined topic for the storage and analysis of yellow taxi ride data.\n\n## Feedback and Flow Control\n\nAfter sending the message, the script prints a confirmation message and pauses for a second:\n\n```python\nprint(\"producing\")\nsleep(1)\n```\n\n- **Print Statement**: Outputs \"producing\" to the console, indicating successful message production.\n\n- **sleep(1)**: Introduces a one-second delay between messages, which helps manage the load on the Kafka broker and allows for controlled throughput of messages.\n\n## Conclusion\n\nIn summary, this script effectively captures essential ride-related data from a CSV file and streams it to a Kafka topic. The use of Kafka allows for scalable and reliable message handling, making it suitable for real-time data processing applications. The methodology of key and value construction provides a structured approach for organizing the taxi ride information efficiently.",
    "filename": "06-streaming/python/streams-example/faust/producer_taxi_json.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis code is a simple application built using the Faust library, which is designed for stream processing with Python. The application consumes messages from a specified Kafka topic, processes these messages, and prints the contents to the console.\n\n## Components of the Application\n\n### Import Statements\n\n```python\nimport faust\nfrom taxi_rides import TaxiRide\n```\n\nThe code begins by importing the `faust` library, which is utilized for building asynchronous stream processing applications. It also imports the `TaxiRide` class from a local module called `taxi_rides`. This class is likely a data model representing the structure of a taxi ride record.\n\n### Application Initialization\n\n```python\napp = faust.App('datatalksclub.stream.v2', broker='kafka://localhost:9092')\n```\n\nAn instance of a Faust application is created using `faust.App()`. The application is named `'datatalksclub.stream.v2'` and is configured to connect to a Kafka broker running locally on port 9092. The Kafka broker is essential for managing the streaming data that the application will process.\n\n### Topic Definition\n\n```python\ntopic = app.topic('datatalkclub.yellow_taxi_ride.json', value_type=TaxiRide)\n```\n\nA topic is defined, which refers to a specific stream of data that the application will consume. In this case, the topic is `'datatalkclub.yellow_taxi_ride.json'`, and the messages expected in this topic are of the type `TaxiRide`. The `value_type` parameter indicates the data structure that will be used for deserialization of the incoming records.\n\n### Message Processing Agent\n\n```python\n@app.agent(topic)\nasync def start_reading(records):\n    async for record in records:\n        print(record)\n```\n\nThis section defines an asynchronous agent function called `start_reading`, which takes the records from the specified topic. The `@app.agent(topic)` decorator indicates that this function will automatically receive records from the Kafka topic.\n\nWithin the function, an asynchronous loop iterates over the incoming records. For each record received, it simply prints the record to the console. This represents the core functionality of the application, where it listens for new data and outputs it.\n\n### Application Entry Point\n\n```python\nif __name__ == '__main__':\n    app.main()\n```\n\nFinally, the code includes a standard Python entry point check using `if __name__ == '__main__':`. If executed as the main program, the application calls `app.main()`, which starts the Faust app and begins processing the stream of data from Kafka.\n\n## Summary of Functionality\n\n- **Stream Processing**: The application is designed to perform stream processing using Faust by consuming messages from a Kafka topic.\n\n- **Kafka Integration**: It interacts with a local Kafka broker, making it suitable for real-time data applications.\n\n- **Message Deserialization**: The incoming messages are expected to conform to the `TaxiRide` data structure, facilitating organized processing of taxi ride data.\n\n- **Asynchronous Operations**: The use of async/await allows the application to handle multiple messages concurrently, improving efficiency in processing streams.\n\n- **Console Output**: The primary action taken on the incoming data is to print it to the console, which could be useful for debugging or monitoring purposes in a development context.\n\nOverall, this code serves as a foundational setup for a streaming application that can be further developed to include more sophisticated processing, error handling, or integration with other systems.",
    "filename": "06-streaming/python/streams-example/faust/stream.py"
  },
  {
    "code": false,
    "content": "# Faust Kafka Stream Processing Application\n\nThis document provides a detailed description of a Kafka stream processing application built using the Faust framework. The application consumes taxi ride data from a Kafka topic, processes it to keep track of rides per vendor, and utilizes a Faust table for efficient storage.\n\n## Overview\n\nThe primary function of this script is to count the number of taxi rides per vendor, using Kafka to manage and transport the incoming data. The application leverages Faust, an asynchronous stream processing library, which interacts seamlessly with Kafka. It consumes incoming messages about taxi rides from a specified topic, processes these messages in a grouped manner by vendor ID, and maintains a count of rides per vendor in a persistent table.\n\n## Library Imports\n\nThe script starts by importing necessary libraries:\n\n```python\nimport faust\nfrom taxi_rides import TaxiRide\n```\n\nHere, the `faust` library is imported to assist in stream processing, while `TaxiRide` is presumably a data model (defined in `taxi_rides` module) representing a taxi ride event's structure, such as its vendor ID and other relevant attributes.\n\n## Application Initialization\n\nThe Faust application is created and configured with the following code:\n\n```python\napp = faust.App('datatalksclub.stream.v2', broker='kafka://localhost:9092')\ntopic = app.topic('datatalkclub.yellow_taxi_ride.json', value_type=TaxiRide)\n```\n\nIn this segment:\n\n- A new Faust application instance named `datatalksclub.stream.v2` is initialized, indicating its use for data streaming tasks.\n- The Kafka broker is set to `localhost:9092`, designating where the application will connect for message processing.\n- A topic named `datatalkclub.yellow_taxi_ride.json` is registered, which the application will listen to, with messages expected to be of type `TaxiRide`.\n\n## Table for Ride Counting\n\nNext, a Faust table is defined:\n\n```python\nvendor_rides = app.Table('vendor_rides', default=int)\n```\n\nThis table, `vendor_rides`, is used to keep counts of rides per vendor. The `default=int` indicates that the default value for any new vendor ID in this table will be initialized to zero. This structure is critical for tracking and aggregating data efficiently across different vendors.\n\n## Stream Processing Agent\n\nThe heart of the script resides in the agent defined as follows:\n\n```python\n@app.agent(topic)\nasync def process(stream):\n    async for event in stream.group_by(TaxiRide.vendorId):\n        vendor_rides[event.vendorId] += 1\n```\n\n### Agent Decorator\n\nThe `@app.agent(topic)` decorator signifies that the `process` function is a Faust agent that will handle incoming messages from the specified Kafka topic. This function is a key component of the stream processing workflow.\n\n### Stream Processing Logic\n\nWithin the `process` function:\n\n- The function listens for events in the `stream` that are grouped by `vendorId`.\n- The `async for` loop processes each event asynchronously, which allows multiple events to be handled concurrently without blocking.\n- For each incoming event, the count of rides for the corresponding vendor ID is incremented by one in the `vendor_rides` table. This means that every time a taxi ride is consumed from the stream, the vendor's count is updated in real-time.\n\n## Application Execution\n\nFinally, the script contains a block to ensure that the application runs correctly:\n\n```python\nif __name__ == '__main__':\n    app.main()\n```\n\nThis conditional checks if the script is being executed directly (as opposed to being imported as a module), and calls `app.main()` to start the Faust application. This method initializes the agent, sets up the Kafka consumer, and begins the streaming process.\n\n## Summary\n\nIn summary, this Faust application efficiently tracks and aggregates the number of taxi rides per vendor from a streaming Kafka source. It utilizes an organized structure to process events asynchronously, enabling real-time data handling. The use of a persistent table ensures that vendor ride counts are maintained accurately and can be queried later, making the application suitable for analytics or reporting purposes on taxi ride data.",
    "filename": "06-streaming/python/streams-example/faust/stream_count_vendor_trips.py"
  },
  {
    "code": false,
    "content": "# Taxi Ride Processing with Faust\n\nThis code snippet leverages the Faust library to define a data model for processing taxi ride information. Faust is a stream processing library in Python, often used for building applications that consume and process data in real-time from message brokers like Kafka.\n\n## Class Definition\n\n### `TaxiRide`\n\nThe code defines a class named `TaxiRide`, which extends the `faust.Record` class from the Faust framework. The `TaxiRide` class is structured to represent data pertaining to individual taxi rides. This approach allows for easy validation and serialization of data objects in a streaming application.\n\n#### Attributes of `TaxiRide`\n\nThe `TaxiRide` class has several attributes that capture essential details of a taxi ride:\n\n- **vendorId (str)**: This attribute is intended to store the identifier for the vendor providing the taxi service. It helps in distinguishing between different taxi service providers.\n\n- **passenger_count (int)**: This integer attribute records the number of passengers included in the ride. It is useful for understanding ride capacity and usage statistics.\n\n- **trip_distance (float)**: This float attribute indicates the total distance traveled during the taxi ride, measured in miles or kilometers. It is crucial for calculating the fare and evaluating service efficiency.\n\n- **payment_type (int)**: This integer represents the method of payment utilized for the ride. It allows for categorization of payment methods (e.g., cash, credit card) which can be useful for financial analysis.\n\n- **total_amount (float)**: This float attribute stores the total fare charged for the ride. It aggregates all charges including base fare, additional fees, and taxes, and is essential for revenue analysis.\n\n## Purpose and Role of the Class\n\nThe `TaxiRide` class serves as a schema for taxi ride data being processed within a Faust application. By inheriting from `faust.Record`, it gains the ability to enforce data validation automatically, ensuring that the instances of `TaxiRide` hold valid data types for each of its attributes.\n\nThis structured approach is particularly useful in a stream processing context where data integrity and cleanliness are paramount. Each instance of `TaxiRide` can represent a single event (a taxi ride), ready for processing, storage, or transmission via a message queue.\n\nAnnotated attributes also facilitate documentation and understanding of the data structure across the development team, fostering transparency and collaboration.\n\n## Integration with Stream Processing\n\nIn a typical Faust application, instances of `TaxiRide` would be instantiated and published to a stream for further processing. These streamed instances can be consumed by different consumers, which perform operations like aggregating rides by vendor, calculating average distances, or generating reports on earnings based on payment type.\n\nThe `TaxiRide` class is designed in such a way that it can readily interface with other components within the Faust ecosystem, including topics, agents, and consumers, making it invaluable for real-time data processing tasks.\n\nIn summary, this code provides a robust foundation for working with taxi ride data, allowing developers to build complex stream processing workflows with ease and reliability. By encapsulating the ride information in a well-defined class, it enhances maintainability and scalability of the application, giving developers the tools they need to handle transportation data effectively.",
    "filename": "06-streaming/python/streams-example/faust/taxi_rides.py"
  },
  {
    "code": false,
    "content": "# Code Overview\n\nThis code snippet implements a Faust application that processes a stream of taxi ride data from Kafka. The application counts the number of rides for each taxi vendor over specified time intervals and maintains this count in a windowed table.\n\n## Libraries and Dependencies\n\n1. **`datetime.timedelta`**: This library is used to define time durations. In this code, it helps manage the time window for aggregating ride counts.\n   \n2. **`faust`**: A stream processing library that allows asynchronous processing of data streams. This is utilized for handling messages from a Kafka topic.\n\n3. **`taxi_rides.TaxiRide`**: This is likely a custom class representing a taxi ride's data structure. The code assumes that it has attributes pertinent to taxi rides, including a vendor identifier.\n\n## Application Setup\n\n- **Faust Application Initialization**:  \n  The application is instantiated with `faust.App`, identifying itself as `datatalksclub.stream.v2` and establishing a connection to a Kafka broker located at `localhost:9092`.\n\n  ```python\n  app = faust.App('datatalksclub.stream.v2', broker='kafka://localhost:9092')\n  ```\n\n- **Defining the Kafka Topic**:  \n  A Kafka topic named `datatalkclub.yellow_taxi_ride.json` is defined, with its messages expected to be of type `TaxiRide`. This allows the Faust application to correctly interpret the incoming data.\n\n  ```python\n  topic = app.topic('datatalkclub.yellow_taxi_ride.json', value_type=TaxiRide)\n  ```\n\n## Windowed Table for Ride Counting\n\n- **`vendor_rides` Table**:  \n  A table called `vendor_rides_windowed` is created to store the count of rides for each vendor. The key is the vendor ID, and the default value is an integer starting at zero.\n\n  ```python\n  vendor_rides = app.Table('vendor_rides_windowed', default=int).tumbling(\n      timedelta(minutes=1),\n      expires=timedelta(hours=1),\n  )\n  ```\n\n- **Time Window and Expiration**:  \n  The table updates counts in a tumbling window of 1 minute, meaning that every minute, counts are aggregated and reset. Additionally, the data in this table expires after one hour, helping to manage memory and data relevance.\n\n## Stream Processing Agent\n\n- **`@app.agent` Decorator**:  \n  The main functionality of the application is defined by the `process` function decorated with `@app.agent`. This designates it as an agent that listens to the specified Kafka topic.\n\n  ```python\n  @app.agent(topic)\n  async def process(stream):\n  ```\n\n- **Processing the Stream**:  \n  The `process` function continuously listens to the incoming stream of taxi ride messages grouped by `vendorId`. For each event, it increments the count in the `vendor_rides` table for the given `vendorId`.\n\n  ```python\n  async for event in stream.group_by(TaxiRide.vendorId):\n      vendor_rides[event.vendorId] += 1\n  ```\n\n## Main Application Execution\n\n- **Entry Point**:  \n  The standard Python entry point `if __name__ == '__main__':` block is present to ensure that `app.main()` is called when the script is executed. This runs the Faust application, initializing the stream processing.\n\n  ```python\n  if __name__ == '__main__':\n      app.main()\n  ```\n\n## Summary of Functionality\n\nIn summary, this Faust application is designed to process real-time data for yellow taxi rides, counts how many rides each vendor completes within a minute, and efficiently manages this count using a windowed table that expires old data after an hour. By utilizing Kafka as the messaging broker, the application can handle a stream of data robustly and asynchronously, providing valuable analytics on taxi ride patterns over time.",
    "filename": "06-streaming/python/streams-example/faust/windowing.py"
  },
  {
    "content": "# Running PySpark Streaming \n\n#### Prerequisite\n\nEnsure your Kafka and Spark services up and running by following the [docker setup readme](./../../docker/README.md). \nIt is important to create network and volume as described in the document. Therefore please ensure, your volume and network are created correctly\n\n```bash\ndocker volume ls # should list hadoop-distributed-file-system\ndocker network ls # should list kafka-spark-network \n```\n\n\n### Running Producer and Consumer\n```bash\n# Run producer\npython3 producer.py\n\n# Run consumer with default settings\npython3 consumer.py\n# Run consumer for specific topic\npython3 consumer.py --topic <topic-name>\n```\n\n### Running Streaming Script\n\nspark-submit script ensures installation of necessary jars before running the streaming.py\n\n```bash\n./spark-submit.sh streaming.py \n```\n\n### Additional Resources\n- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#structured-streaming-programming-guide)\n- [Structured Streaming + Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#structured-streaming-kafka-integration-guide-kafka-broker-versio)",
    "filename": "06-streaming/python/streams-example/pyspark/README.md"
  },
  {
    "code": false,
    "content": "# Kafka Consumer Documentation\n\n## Overview\nThis code implements a Kafka consumer that reads messages from a specified Kafka topic, particularly aimed at processing CSV data related to rides. It utilizes the `kafka-python` library to manage the communication with Kafka. The consumer will connect to a Kafka instance, subscribe to a topic, and continuously poll for messages, printing their keys and values until interrupted.\n\n## Dependencies\nThe script imports several modules:\n- `argparse`: Used for command-line argument parsing.\n- `Dict` and `List` from `typing`: Used for type hinting, enhancing code readability.\n- `KafkaConsumer` from `kafka`: Main class to interact with Kafka for consuming messages.\n- `settings`: Custom settings module that provides configuration variables for Kafka connection.\n\n## RideCSVConsumer Class\n### Initialization\n```python\nclass RideCSVConsumer:\n    def __init__(self, props: Dict):\n        self.consumer = KafkaConsumer(**props)\n```\nThis class is initialized with a dictionary of properties (`props`) necessary for the Kafka consumer. The `KafkaConsumer` instance is created using these properties, establishing the connection to the Kafka broker(s).\n\n### Consuming Messages\n```python\ndef consume_from_kafka(self, topics: List[str]):\n```\nThe `consume_from_kafka` method subscribes to the specified Kafka topics and enters a loop that polls for messages. \n\n- **Subscription**: It subscribes to the provided topics and confirms the subscription.\n- **Message Polling**: The method repeatedly polls for new messages using a timeout of 1 second.\n  - If no messages are returned, it continues polling.\n  - Upon receiving messages, it iterates through them, printing out each message's key and value along with their data types.\n  \n### Handling Interruptions\nThe method contains a try-except block to gracefully handle keyboard interrupts (SIGINT). When such an interrupt is detected, the loop is exited, and the consumer connection is closed.\n\n```python\nself.consumer.close()\n```\nThis line ensures that the Kafka consumer is properly closed when finished, releasing any resources.\n\n## Main Execution Block\nThe main part of the script handles command-line execution:\n```python\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Kafka Consumer')\n```\n- **Argument Parsing**: A command-line parser is created, allowing users to specify the Kafka topic they want to consume. If no topic is provided, it defaults to a value defined in the `settings` module (`CONSUME_TOPIC_RIDES_CSV`).\n\n### Configuration Setup\nThe script then prepares a configuration dictionary, `config`, for the Kafka consumer:\n- `bootstrap_servers`: Specifies the Kafka server(s) to connect to.\n- `auto_offset_reset`: Controls the behavior for resetting offsets; set to 'earliest' to read from the beginning of the topic if there are no previous offsets stored.\n- `enable_auto_commit`: Automatically commits offsets of messages consumed.\n- `key_deserializer` & `value_deserializer`: Functions for deserializing message keys and values; the keys are converted to integers, while values are decoded to strings.\n- `group_id`: Identifies the consumer group to which the consumer belongs.\n\n### Instantiating the Consumer\nFinally, a `RideCSVConsumer` instance is created using the prepared configuration and subscribes to the chosen topic for message consumption.\n\n```python\ncsv_consumer = RideCSVConsumer(props=config)\ncsv_consumer.consume_from_kafka(topics=[topic])\n```\n\n## Conclusion\nThis script serves as a straightforward Kafka consumer implementation. Following the principles of Object-Oriented Programming, it encapsulates functionality focused on consuming messages from Kafka into a class. The command-line interface makes it flexible to specify different topics at runtime, while built-in mechanisms handle message polling and graceful shutdown, ensuring that the consumer operates efficiently and reliably.",
    "filename": "06-streaming/python/streams-example/pyspark/consumer.py"
  },
  {
    "code": false,
    "content": "# RideCSVProducer Code Documentation\n\nThis document describes the purpose and functionality of the RideCSVProducer script. It outlines the key components, including classes and methods, and explains how they work together to produce records to a Kafka topic from a CSV file.\n\n## Overview\n\nThe script is designed to read ride data from a specified CSV file and produce messages to a Kafka topic. It uses the KafkaProducer from the Kafka library to send records containing ride information. The primary logic resides in the `RideCSVProducer` class, which encapsulates the functionality for reading CSV records and publishing them to a Kafka topic.\n\n## Imports and Dependencies\n\nThe script imports several modules:\n\n- **csv**: For reading the CSV file containing ride data.\n- **time**: Specifically the `sleep` function for pausing execution momentarily.\n- **typing**: Used for type annotations, specifically `Dict`.\n- **kafka**: The `KafkaProducer` class is imported to create a producer that can send messages to Kafka.\n- **settings**: Presumably contains configuration details such as Kafka server addresses, input file paths, and topic names.\n\n## Delivery Report Function\n\n```python\ndef delivery_report(err, msg):\n    ...\n```\n\n### Purpose\nThe `delivery_report` function serves as a callback to report the success or failure of a message sent to Kafka. It takes two parameters:\n\n- **err**: An exception object in case of failure, or `None` if the message was delivered successfully.\n- **msg**: The message object that was attempted to be sent, which contains metadata including its key, topic, partition, and offset.\n\n### Functionality\n- If `err` is not `None`, it indicates a failure in sending the message, and an error message is printed.\n- If the message is successfully produced, it logs the details of the message, including its key and location in the Kafka topic.\n\n## RideCSVProducer Class\n\n### Initialization\n```python\nclass RideCSVProducer:\n    def __init__(self, props: Dict):\n        ...\n```\nThe class is initialized with a dictionary `props` that contains configuration parameters for the Kafka producer, notably the `bootstrap_servers`, `key_serializer`, and `value_serializer`.\n\n### Method: read_records\n```python\n@staticmethod\ndef read_records(resource_path: str):\n    ...\n```\n\n#### Purpose\nThis static method reads records from a given CSV file and formats them into the required structure for Kafka. It specifically extracts and transforms relevant fields from each row of the CSV.\n\n#### Functionality\n- Opens the CSV file specified by `resource_path` and creates a CSV reader.\n- Skips the header row and iterates over the remaining rows.\n- For each row, it extracts specific columns (indices) that represent ride details, assembles these into a formatted string, and appends them to a list called `records`.\n- Additionally, it collects ride keys (the first column) in a separate list.\n- The method returns a zipped collection of ride keys and the formatted records.\n\n### Method: publish\n```python\ndef publish(self, topic: str, records: [str, str]):\n    ...\n```\n\n#### Purpose\nThe `publish` method sends the formatted records to a specified Kafka topic.\n\n#### Functionality\n- It loops over the records, extracting the key and value from each pair.\n- It attempts to send each message using the Kafka producer.\n- If a `KeyboardInterrupt` occurs, the loop breaks cleanly.\n- If an exception arises during sending, it catches the exception and prints an error message.\n- Once all records are sent, it flushes the producer, ensuring all pending messages are sent before pausing for a second.\n\n## Main Execution Block\n\n```python\nif __name__ == \"__main__\":\n    ...\n```\n\n### Purpose\nThis block is the entry point of the script and orchestrates the execution of the main logic.\n\n### Functionality\n- It constructs a `config` dictionary containing Kafka connection details, specifically the servers and serializers for keys and values.\n- It creates an instance of the `RideCSVProducer` class, initializing it with the configuration.\n- It reads records from the specified CSV file using the `read_records` method, capturing the returned ride records.\n- Finally, it calls the `publish` method to produce the ride records to the specified Kafka topic.\n\n## Conclusion\n\nThe RideCSVProducer script is a straightforward implementation for producing ride records to Kafka from a CSV file. It effectively encapsulates the Kafka interaction within the `RideCSVProducer` class, separating concerns between data reading and producing, making the code manageable and extendable. The script includes error handling for situations where message delivery may fail, ensuring robustness in message production.",
    "filename": "06-streaming/python/streams-example/pyspark/producer.py"
  },
  {
    "code": false,
    "content": "# Code Documentation: Rides Data Processing Configuration\n\nThis code snippet sets up the initial configuration and schema definitions for processing ride data using PySpark, likely in a streaming context. It primarily defines the schema for incoming ride data and establishes constants for paths and streaming configurations.\n\n## Constants and Configuration\n\n### Input Data Path\n```python\nINPUT_DATA_PATH = '../../resources/rides.csv'\n```\nThis constant defines the relative path to the CSV file containing ride data. It is assumed that this CSV file is structured in accordance with the schema defined later in the code.\n\n### Kafka Configuration\n```python\nBOOTSTRAP_SERVERS = 'localhost:9092'\nTOPIC_WINDOWED_VENDOR_ID_COUNT = 'vendor_counts_windowed'\nPRODUCE_TOPIC_RIDES_CSV = CONSUME_TOPIC_RIDES_CSV = 'rides_csv'\n```\nThis section sets up constants used for Kafka configuration:\n- **BOOTSTRAP_SERVERS**: This specifies the server address for connecting to Kafka. In this case, it points to a server running locally on port 9092.\n- **TOPIC_WINDOWED_VENDOR_ID_COUNT**: This constant is specified for storing windowed counts of vendor IDs. It appears to relate to a stream aggregation task.\n- **PRODUCE_TOPIC_RIDES_CSV and CONSUME_TOPIC_RIDES_CSV**: Both constants are set to 'rides_csv', indicating that rides data will both be produced to and consumed from this topic.\n\n## Data Schema Definition\n\n### Ride Schema\n```python\nRIDE_SCHEMA = T.StructType(\n    [T.StructField(\"vendor_id\", T.IntegerType()),\n     T.StructField('tpep_pickup_datetime', T.TimestampType()),\n     T.StructField('tpep_dropoff_datetime', T.TimestampType()),\n     T.StructField(\"passenger_count\", T.IntegerType()),\n     T.StructField(\"trip_distance\", T.FloatType()),\n     T.StructField(\"payment_type\", T.IntegerType()),\n     T.StructField(\"total_amount\", T.FloatType()),\n     ])\n```\nThis block defines the schema for the ride data using Spark's SQL types.\n\n#### Fields:\n- **vendor_id**: An integer representing the identifier for the ride vendor.\n- **tpep_pickup_datetime**: A timestamp for when the ride starts (pickup time).\n- **tpep_dropoff_datetime**: A timestamp for when the ride ends (drop-off time).\n- **passenger_count**: An integer representing the number of passengers on the ride.\n- **trip_distance**: A float representing the distance traveled during the ride.\n- **payment_type**: An integer indicating the type of payment (credit card, cash, etc.).\n- **total_amount**: A float representing the total fare amount for the ride.\n\n## Purpose of the Code\n\nThe code sets up the foundational aspects of a Spark application designed to handle rides data from a CSV file and stream it to Kafka for further processing. The schema ensures that the data can be correctly interpreted and utilized within Spark's DataFrame operations, enabling subsequent analytics or transformations as needed.\n\nGiven the integration with Kafka, the code suggests a framework for potentially real-time processing or analytics on ride data, including metrics such as the count of different vendors over a specified window of time.",
    "filename": "06-streaming/python/streams-example/pyspark/settings.py"
  },
  {
    "code": false,
    "content": "## 0. Spark Setup\n\nTo begin using Apache Spark with Kafka and Avro data formats, we need to configure the environment and establish a Spark session. This is essential for executing our data processing tasks.\n\n```python\nimport os\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell'\n```\n\nThis code sets the necessary loading parameters for PySpark, including packages for Kafka and Avro support required during runtime.\n\n```python\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.types as T\nimport pyspark.sql.functions as F\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Spark-Notebook\") \\\n    .getOrCreate()\n```\n\nHere, we create a `SparkSession`, which is the entry point for using the DataFrame and SQL API in PySpark. The application is named \"Spark-Notebook\" for easy identification in Spark's UI.\n\n## 1. Reading from Kafka Stream\n\nWe will now read data from a Kafka stream using PySpark's `readStream` method. This provides a way to continuously read streaming data into a DataFrame.\n\n### 1.1 Raw Kafka Stream\n\n```python\n# default for startingOffsets is \"latest\"\ndf_kafka_raw = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n    .option(\"subscribe\", \"rides_csv\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .option(\"checkpointLocation\", \"checkpoint\") \\\n    .load()\n```\n\nThis block reads from a Kafka topic named \"rides_csv\". The `startingOffsets` option is set to \"earliest\", meaning data will be read from the beginning of the topic, and a checkpoint location is set to enable fault tolerance.\n\n```python\ndf_kafka_raw.printSchema()\n```\n\nWe print the schema of the raw Kafka DataFrame for verification. This will show the structure of the data we are reading, including key-value pairs.\n\n### 1.2 Encoded Kafka Stream\n\n```python\ndf_kafka_encoded = df_kafka_raw.selectExpr(\"CAST(key AS STRING)\",\"CAST(value AS STRING)\")\n```\n\nIn this step, we convert the key and value columns of the Kafka messages to strings for easier manipulation down the line.\n\n```python\ndf_kafka_encoded.printSchema()\n```\n\nAs before, we print the schema of the encoded DataFrame to confirm the changes made in the previous step.\n\n### 1.3 Structure Streaming DataFrame\n\n```python\ndef parse_ride_from_kafka_message(df_raw, schema):\n    \"\"\" take a Spark Streaming df and parse value col based on <schema>, return streaming df cols in schema \"\"\"\n    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n\n    df = df_raw.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\n    # split attributes to nested array in one Column\n    col = F.split(df['value'], ', ')\n\n    # expand col to multiple top-level columns\n    for idx, field in enumerate(schema):\n        df = df.withColumn(field.name, col.getItem(idx).cast(field.dataType))\n    return df.select([field.name for field in schema])\n```\n\nThe `parse_ride_from_kafka_message` function takes a raw streaming DataFrame and a predefined schema to parse the incoming Kafka messages. It splits the 'value' column into multiple attributes and creates new columns based on the given schema.\n\n```python\nride_schema = T.StructType(\n    [T.StructField(\"vendor_id\", T.IntegerType()),\n     T.StructField('tpep_pickup_datetime', T.TimestampType()),\n     T.StructField('tpep_dropoff_datetime', T.TimestampType()),\n     T.StructField(\"passenger_count\", T.IntegerType()),\n     T.StructField(\"trip_distance\", T.FloatType()),\n     T.StructField(\"payment_type\", T.IntegerType()),\n     T.StructField(\"total_amount\", T.FloatType()),\n     ])\n```\n\nHere, we define a schema for ride data using PySpark's SQL types. This structure specifies the data types for each attribute of the incoming Kafka messages.\n\n```python\ndf_rides = parse_ride_from_kafka_message(df_raw=df_kafka_raw, schema=ride_schema)\n```\n\nWe call the parsing function to transform the raw DataFrame into a structured format according to the defined schema.\n\n```python\ndf_rides.printSchema()\n```\n\nFinally, we print the schema of the structured rides DataFrame to verify that the parsing was successful and the data conforms to the specified structure.\n\n## 2. Sink Operation & Streaming Query\n\nTo persist or analyze the data processed through our streaming DataFrame, we can utilize various sink operations. \n\n---\n### Output Sinks\n\nIn structured streaming, we have several output sinks available:\n- **File Sink**: Saves data to a specified directory.\n- **Kafka Sink**: Publishes output to Kafka topics.\n- **Foreach Sink**: Allows custom operations on each row.\n- **Console Sink**: Outputs content directly to the console, useful for debugging.\n- **Memory Sink**: Stores outputs in memory for quick access.\n\nFor further details, refer to the [Output Sinks documentation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks).\n\n---\n### Output Modes\n\nThere are three types of output modes that define how results are written to the sinks:\n- **Complete**: Outputs the entire result table after every trigger.\n- **Append**: Only new rows are added to the result table.\n- **Update**: Outputs only updated rows in the result.\n\nFor more information, see the [Output Modes documentation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes).\n\n---\n### Triggers\n\nThe following trigger settings define how often streaming queries are processed:\n- **default-micro-batch-mode**\n- **fixed-interval-micro-batch-mode**\n- **one-time-micro-batch-mode**\n- **available-now-micro-batch-mode**\n\nMore on triggers can be found in the [Triggers documentation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers).\n\n### Console and Memory Sink\n\n```python\ndef sink_console(df, output_mode: str = 'complete', processing_time: str = '5 seconds'):\n    write_query = df.writeStream \\\n        .outputMode(output_mode) \\\n        .trigger(processingTime=processing_time) \\\n        .format(\"console\") \\\n        .option(\"truncate\", False) \\\n        .start()\n    return write_query # pyspark.sql.streaming.StreamingQuery\n```\n\nThe `sink_console` function sets up a console sink to display the streaming DataFrame output. This function allows customization of the output mode and processing time.\n\n```python\nwrite_query = sink_console(df_rides, output_mode='append')\n```\n\nHere, the `write_query` object is created for the streaming DataFrame in append mode, meaning only new incoming ride data will be displayed at the console.\n\n```python\ndef sink_memory(df, query_name, query_template):\n    write_query = df \\\n        .writeStream \\\n        .queryName(query_name) \\\n        .format('memory') \\\n        .start()\n    query_str = query_template.format(table_name=query_name)\n    query_results = spark.sql(query_str)\n    return write_query, query_results\n```\n\nThis function creates a memory sink for quick access to the streaming DataFrame results. It allows defining a query name for later retrieval and runs a sample SQL query on the results.\n\n```python\nquery_name = 'vendor_id_counts'\nquery_template = 'select count(distinct(vendor_id)) from {table_name}'\nwrite_query, df_vendor_id_counts = sink_memory(df=df_rides, query_name=query_name, query_template=query_template)\n```\n\nWe set up a memory sink specifically to count distinct `vendor_id`s from the ride data stream. This output will be easily retrievable for analysis.\n\n```python\nprint(type(write_query)) # pyspark.sql.streaming.StreamingQuery\nwrite_query.status\n```\n\nThese statements output the type of the write query object and its status, providing insight into the ongoing streaming operation.\n\n```python\ndf_vendor_id_counts.show()\n```\n\nThis command retrieves and displays the counts of distinct vendor IDs from our memory sink in the Spark console.\n\n```python\nwrite_query.stop()\n```\n\nFinally, we stop the streaming query to free up resources when we're done processing the data.\n\n### Kafka Sink\n\nFor publishing stream results back to a Kafka topic, the DataFrame must include a column named `value`. \n\n```python\ndef prepare_dataframe_to_kafka_sink(df, value_columns, key_column=None):\n    columns = df.columns\n    df = df.withColumn(\"value\", F.concat_ws(', ', *value_columns))    \n    if key_column:\n        df = df.withColumnRenamed(key_column, \"key\")\n        df = df.withColumn(\"key\", df.key.cast('string'))\n    return df.select(['key', 'value'])\n    \ndef sink_kafka(df, topic, output_mode='append'):\n    write_query = df.writeStream \\\n        .format(\"kafka\") \\\n        .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n        .outputMode(output_mode) \\\n        .option(\"topic\", topic) \\\n        .option(\"checkpointLocation\", \"checkpoint\") \\\n        .start()\n    return write_query\n```\n\nThe `prepare_dataframe_to_kafka_sink` function constructs the appropriate DataFrame by concatenating specified value columns into a single 'value' column, and optionally renaming a key column. The `sink_kafka` function writes the DataFrame to a specified Kafka topic using the streaming write method.\n\n### Conclusion\n\nThis documentation provided a structured overview of setting up Spark with Kafka for processing ride data, as well as how to read from Kafka streams and write to different sinks. The choice of output sinks and modes can greatly influence how you manage and analyze your streaming data.",
    "filename": "06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb"
  },
  {
    "code": false,
    "content": "# Streaming Data Processing with PySpark\n\nThis document provides an overview of a PySpark script designed for streaming data processing from Kafka. The script includes functions for reading data from a Kafka topic, parsing it, and performing various aggregation operations before writing the processed data back to the console or Kafka.\n\n## Imports and Dependencies\n\n```python\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom settings import RIDE_SCHEMA, CONSUME_TOPIC_RIDES_CSV, TOPIC_WINDOWED_VENDOR_ID_COUNT\n```\n\nThe script begins by importing necessary libraries:\n- `SparkSession` is the entry point for using Spark SQL.\n- `pyspark.sql.functions` is imported as `F` for utilizing various built-in functions.\n- Configuration variables such as `RIDE_SCHEMA`, `CONSUME_TOPIC_RIDES_CSV`, and `TOPIC_WINDOWED_VENDOR_ID_COUNT` are imported from a `settings` module.\n\n## Kafka Data Reading\n\n```python\ndef read_from_kafka(consume_topic: str):\n    # Spark Streaming DataFrame, connect to Kafka topic served at host in bootrap.servers option\n    ...\n    return df_stream\n```\n\nThe `read_from_kafka` function establishes a connection to a specified Kafka topic. It uses the Spark Streaming API to create a streaming DataFrame, which allows real-time ingestion of data. The DataFrame's starting offsets are set to 'earliest', ensuring it captures all available messages. The function returns the streaming DataFrame for further processing.\n\n## Parsing Kafka Messages\n\n```python\ndef parse_ride_from_kafka_message(df, schema):\n    \"\"\" take a Spark Streaming df and parse value col based on <schema>, return streaming df cols in schema \"\"\"\n    ...\n    return df.select([field.name for field in schema])\n```\n\nThe `parse_ride_from_kafka_message` function takes in a streaming DataFrame and a schema. It converts the 'value' column from Kafka messages into multiple columns based on the provided schema. The function utilizes Spark SQL expressions to cast the relevant fields and returns a new DataFrame structured according to the schema.\n\n## Writing Data to Console\n\n```python\ndef sink_console(df, output_mode: str = 'complete', processing_time: str = '5 seconds'):\n    ...\n    return write_query  # pyspark.sql.streaming.StreamingQuery\n```\n\nThe `sink_console` function prints a DataFrame to the console for debugging purposes. It supports different output modes, such as 'complete' or 'append', and specifies the processing time interval for updates. This function is particularly useful for monitoring the data as it flows through the various transformations.\n\n## Memory and Kafka Sinks\n\n```python\ndef sink_memory(df, query_name, query_template):\n    ...\n    return query_results, query_df\n\ndef sink_kafka(df, topic):\n    ...\n    return write_query\n```\n\nTwo additional functions handle data storage:\n- `sink_memory` function stores the processed DataFrame in memory for query access. It executes a SQL query against this temporary table to retrieve results.\n- `sink_kafka` function writes DataFrames to a specified Kafka topic, ensuring that the data is serialized as Kafka messages. It requires specifying a checkpoint location to manage offsets and state.\n\n## Dataframe Preparation for Kafka Sink\n\n```python\ndef prepare_df_to_kafka_sink(df, value_columns, key_column=None):\n    ...\n    return df.select(['key', 'value'])\n```\n\nThis function prepares a DataFrame for emitting messages to Kafka. It concatenates specified columns into a single 'value' field and optionally renames and casts a key column. This final selection includes both 'key' and 'value', formatted for Kafka's expectations.\n\n## Aggregation Operations\n\n```python\ndef op_groupby(df, column_names):\n    ...\n    return df_aggregation\n\ndef op_windowed_groupby(df, window_duration, slide_duration):\n    ...\n    return df_windowed_aggregation\n```\n\nThe script includes aggregation functions:\n- `op_groupby` performs a basic grouping operation that counts occurrences of specified columns in the DataFrame.\n- `op_windowed_groupby` aggregates data based on a time window, allowing for more complex time-based analyses. It uses sliding windows to create aggregated counts of trips per vendor.\n\n## Main Execution Workflow\n\n```python\nif __name__ == \"__main__\":\n    spark = SparkSession.builder.appName('streaming-examples').getOrCreate()\n    spark.sparkContext.setLogLevel('WARN')\n\n    # read_streaming data\n    ...\n    sink_console(df_rides, output_mode='append')\n\n    df_trip_count_by_vendor_id = op_groupby(df_rides, ['vendor_id'])\n    df_trip_count_by_pickup_date_vendor_id = op_windowed_groupby(df_rides, window_duration=\"10 minutes\", slide_duration='5 minutes')\n\n    # write the output out to the console for debugging / testing\n    sink_console(df_trip_count_by_vendor_id)\n    # write the output to the kafka topic\n    df_trip_count_messages = prepare_df_to_kafka_sink(df=df_trip_count_by_pickup_date_vendor_id, value_columns=['count'], key_column='vendor_id')\n    kafka_sink_query = sink_kafka(df=df_trip_count_messages, topic=TOPIC_WINDOWED_VENDOR_ID_COUNT)\n\n    spark.streams.awaitAnyTermination()\n```\n\nThe `__main__` block initializes a Spark session, reads streaming data from Kafka, and processes it through defined functions. It performs several operations, including parsing, aggregation, and outputting results to both the console and Kafka. This ensures that the transform and actions of the DataFrame are continuously processed, with termination conditions managed by Spark's streaming capabilities.\n\n## Summary\n\nOverall, this script forms a complete pipeline for streaming data ingestion, transformation, and output using PySpark and Kafka. It captures real-time rides data, processes it with aggregation functions, and allows for output verification through console logs and further propagation to Kafka for downstream consumers. This architecture is commonly employed in scenarios requiring real-time analytics and operational monitoring.",
    "filename": "06-streaming/python/streams-example/pyspark/streaming.py"
  },
  {
    "content": "# Running PySpark Streaming with Redpanda\n\n### 1. Prerequisite\n\nIt is important to create network and volume as described in the document. Therefore please ensure, your volume and network are created correctly.\n\n```bash\ndocker volume ls # should list hadoop-distributed-file-system\ndocker network ls # should list kafka-spark-network \n```\n\n### 2. Create Docker Network & Volume\n\nIf you have not followed any other examples, and above `ls` steps shows no output, create them now.\n\n```bash\n# Create Network\ndocker network create kafka-spark-network\n\n# Create Volume\ndocker volume create --name=hadoop-distributed-file-system\n```\n\n### Running Producer and Consumer\n```bash\n# Run producer\npython producer.py\n\n# Run consumer with default settings\npython consumer.py\n# Run consumer for specific topic\npython consumer.py --topic <topic-name>\n```\n\n### Running Streaming Script\n\nspark-submit script ensures installation of necessary jars before running the streaming.py\n\n```bash\n./spark-submit.sh streaming.py \n```\n\n### Additional Resources\n- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#structured-streaming-programming-guide)\n- [Structured Streaming + Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#structured-streaming-kafka-integration-guide-kafka-broker-versio)",
    "filename": "06-streaming/python/streams-example/redpanda/README.md"
  },
  {
    "code": false,
    "content": "# RideCSVConsumer Kafka Consumer Documentation\n\n## Overview\nThe provided script implements a simple Kafka consumer that listens to messages on a specified topic. It is primarily designed to consume CSV-formatted ride data from a Kafka topic. The implementation utilizes the `KafkaConsumer` from the `kafka-python` library to manage connections to a Kafka cluster and process incoming messages.\n\n## Dependencies\nThe script imports several essential modules:\n- **argparse**: To handle command-line arguments for topic specification.\n- **typing**: Specifically, `Dict` and `List` types for clear type annotations.\n- **kafka.KafkaConsumer**: For consuming messages from Kafka topics.\n- **settings**: A custom module assumed to provide constants like `BOOTSTRAP_SERVERS` and `CONSUME_TOPIC_RIDES_CSV`.\n\n## Class Definition: RideCSVConsumer\nThe core functionality is encapsulated within the `RideCSVConsumer` class.\n\n### Constructor: `__init__`\n- **Parameters**: Takes a dictionary (`props`) containing consumer configuration parameters.\n- **Functionality**: Initializes a `KafkaConsumer` instance with the provided settings.\n\n### Method: `consume_from_kafka`\n- **Parameters**: Accepts a list of topics (`topics`) from which the consumer will listen.\n- **Functionality**:\n  - Subscribes to specified topics and starts consuming messages in an infinite loop.\n  - Polls the Kafka server every second for new messages.\n  - If messages are received, it iterates through them, printing the key and value of each message along with their types.\n  - Implements a graceful shutdown on keyboard interrupt (SIGINT), ensuring that resources are released properly by closing the consumer.\n\n## Command-Line Interface\nIn the `__main__` block, the script sets up an argument parser that allows for command-line execution:\n\n1. **Argument Parsing**: Uses `argparse` to provide a description of the script's functionality and to define a command-line argument for the Kafka topic. If no topic is provided, it defaults to `CONSUME_TOPIC_RIDES_CSV`.\n  \n2. **Configuration Setup**:\n   - Constructs a dictionary (`config`) which holds various configuration options for connecting to Kafka:\n     - `bootstrap_servers`: Host and port of the Kafka cluster.\n     - `auto_offset_reset`: Tells the consumer to start reading from the earliest message (if no offset is committed).\n     - `enable_auto_commit`: Automatically commits offsets of messages after they are polled.\n     - `key_deserializer` and `value_deserializer`: Functions to convert raw bytes into appropriate data types.\n     - `group_id`: Identifier for the consumer group.\n\n3. **Consumer Initialization**: Creates an instance of `RideCSVConsumer`, passing in the configuration settings.\n\n4. **Start Consuming**: Calls the `consume_from_kafka` method with the specified topic to begin message consumption.\n\n## Execution Flow\n1. The script starts execution in the `__main__` block.\n2. It parses command-line arguments, allowing the user to specify a Kafka topic.\n3. Configuration settings for the Kafka consumer are established, including deserializer functions for message keys and values.\n4. An instance of the `RideCSVConsumer` class is created with the consumer configuration.\n5. The consumer begins polling the Kafka topic for messages and will print details of any received messages until a keyboard interrupt occurs.\n\n## Conclusion\nThis script serves as an effective starting point for consuming messages, particularly in scenarios where CSV-formatted data is transmitted over Kafka. It efficiently manages message consumption and resource cleanup while providing useful runtime information via printed logs. The design can be extended for additional functionalities such as processing and storing the message data received.",
    "filename": "06-streaming/python/streams-example/redpanda/consumer.py"
  },
  {
    "code": false,
    "content": "# Ride CSV Producer Documentation\n\nThis document provides a high-level overview of the Ride CSV Producer code, detailing its purpose, structure, functions, and flow. The code is designed to read rides data from a CSV file and publish it to a Kafka topic.\n\n## Overview\n\nThe primary function of this script is to act as a producer that reads ride data from a specified CSV file and sends that data to a Kafka topic for consumption by other services. Utilizing the Kafka messaging system allows for efficient, scalable handling of streaming data in real-time applications.\n\n## Dependencies\n\nThe script relies on several external libraries:\n- **csv**: To handle reading from CSV files.\n- **time**: Specifically for adding pauses in execution (using `sleep`).\n- **kafka**: The `KafkaProducer` class is used to send data to a Kafka topic.\n- **settings**: This script imports several configuration constants (like the Kafka server address, input data path, and Kafka topic) from a separate settings module.\n\n## Functions\n\n### `delivery_report(err, msg)`\n\nThis function serves as a callback for reporting the status of messages sent to Kafka. It checks if there was an error (`err`), and if so, prints a failure message indicating the specific record and error information. If the message was successfully produced, it prints the details including the record key, topic, partition, and the offset at which it was stored.\n\n### `class RideCSVProducer`\n\nThe `RideCSVProducer` class encapsulates the functionality to produce ride records as follows:\n\n#### `__init__(self, props: Dict)`\n\nThe constructor initializes the `RideCSVProducer` instance. It creates a new `KafkaProducer` using properties passed through the `props` dictionary, which includes configurations like Kafka server addresses and serializers for keys and values.\n\n#### `read_records(resource_path: str)`\n\nThis static method reads ride records from the specified CSV file located at `resource_path`. The method performs the following steps:\n1. Opens the CSV file and initiates a reader.\n2. Skips the header row.\n3. Reads up to 5 rows from the CSV file, extracting specific fields (like vendor ID, passenger count, trip distance, payment type, total amount, and two other unspecified columns).\n4. Appends each processed record and its corresponding key (essentially the vendor ID) to two lists, which are then zipped together and returned.\n\n#### `publish(self, topic: str, records: [str, str])`\n\nThe publish method is responsible for sending records to the specified Kafka topic:\n1. It iterates over the records, treating each as a pair of key and value.\n2. For each record, it attempts to send it using the `self.producer.send()` method. If there's a keyboard interrupt, the process halts gracefully. If an exception is encountered, an error message is printed.\n3. Once all records are sent, the producer's buffer is flushed to ensure all messages are delivered, followed by a brief sleep to allow time for processing.\n\n## Main Execution Block\n\n### `if __name__ == \"__main__\":`\n\nIn the main execution block:\n1. A configuration dictionary `config` is prepared with `bootstrap_servers`, `key_serializer`, and `value_serializer`.\n2. An instance of `RideCSVProducer` is created using this configuration.\n3. Ride records are read from the CSV file specified by `INPUT_DATA_PATH` through the `read_records` method.\n4. It prints the retrieved ride records.\n5. Finally, it calls the `publish` method to send the records to the Kafka topic defined by `PRODUCE_TOPIC_RIDES_CSV`.\n\n## Summary\n\nThe Ride CSV Producer provides a complete pipeline to read ride data from CSV files and publish it to a Kafka topic. Through careful handling of Kafka configurations and record serialization, it ensures that the data is effectively sent and monitored. The class structure allows for encapsulation of the functionality, making it maintainable and adaptable for enhancements, like adding more configuration options or error handling features in the future.",
    "filename": "06-streaming/python/streams-example/redpanda/producer.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis script defines constants and a schema for processing ride data, presumably from a CSV file, using Apache Spark. The primary focus of the code is to establish a structured representation of ride data for further processing and potentially for output to a messaging system (like Kafka).\n\n## Constants\n\n### Input Data Path\n```python\nINPUT_DATA_PATH = '../../resources/rides.csv'\n```\nThis constant specifies the path to the CSV file containing ride data. The path is relative, indicating the file is located in a 'resources' directory two levels up from the current working directory.\n\n### Bootstrap Servers\n```python\nBOOTSTRAP_SERVERS = 'localhost:9092'\n```\nThis constant defines the address of the Kafka server that will be used for message production or consumption. Here, it points to a local server running on port 9092, which is the default port for Kafka.\n\n### Kafka Topics\n```python\nTOPIC_WINDOWED_VENDOR_ID_COUNT = 'vendor_counts_windowed'\nPRODUCE_TOPIC_RIDES_CSV = CONSUME_TOPIC_RIDES_CSV = 'rides_csv'\n```\n- `TOPIC_WINDOWED_VENDOR_ID_COUNT`: This constant designates a Kafka topic intended for publishing counts of vendor IDs over a time window, implying that some aggregation or windowing logic will be applied during processing.\n  \n- `PRODUCE_TOPIC_RIDES_CSV` and `CONSUME_TOPIC_RIDES_CSV`: Both constants point to the same Kafka topic named 'rides_csv'. This implies that the application both produces data to and consumes data from this topic.\n\n## Schema Definition\n\n### Ride Schema\n```python\nRIDE_SCHEMA = T.StructType(\n    [T.StructField(\"vendor_id\", T.IntegerType()),\n     T.StructField('tpep_pickup_datetime', T.TimestampType()),\n     T.StructField('tpep_dropoff_datetime', T.TimestampType()),\n     T.StructField(\"passenger_count\", T.IntegerType()),\n     T.StructField(\"trip_distance\", T.FloatType()),\n     T.StructField(\"payment_type\", T.IntegerType()),\n     T.StructField(\"total_amount\", T.FloatType()),\n     ])\n```\nThe `RIDE_SCHEMA` variable is defined using Spark's SQL types (`pyspark.sql.types`), creating a structured format for the ride data. This schema includes the following fields:\n\n- **vendor_id**: Represents the ID of the vendor (integer).\n- **tpep_pickup_datetime**: The timestamp for when the ride started (timestamp).\n- **tpep_dropoff_datetime**: The timestamp for when the ride ended (timestamp).\n- **passenger_count**: The number of passengers in the ride (integer).\n- **trip_distance**: The distance of the trip (float).\n- **payment_type**: Indicates the method of payment used (integer).\n- **total_amount**: The total fare for the ride (float).\n\nThis schema allows Spark to interpret the CSV file's structure correctly and facilitates processing tasks such as filtering, aggregating, and writing to other systems.\n\n## Summary\n\nIn summary, this script sets up the groundwork for processing ride data through Spark by defining the necessary input data paths, Kafka configurations, and a clear schema for ride records. By leveraging Apache Spark's capabilities, the script implies future utilization of data transformations and operations that will rely on the structured format defined by the `RIDE_SCHEMA`. The integration with Kafka suggests that the processed data will be streamed to consumers for real-time analytics or measurement purposes.",
    "filename": "06-streaming/python/streams-example/redpanda/settings.py"
  },
  {
    "code": false,
    "content": "## Spark Setup\n\nIn this section, we set up the necessary environment and create a Spark session. This involves installing required packages and initializing Spark to handle data streams.\n\n```python\nimport os\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell'\n```\n\nIn the code block above, we specify the necessary packages for Spark SQL and Kafka integration. The `PYSPARK_SUBMIT_ARGS` environment variable is set to include these packages when launching PySpark.\n\n```python\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.types as T\nimport pyspark.sql.functions as F\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Spark-Notebook\") \\\n    .getOrCreate()\n```\n\nHere, we import essential modules from PySpark, create a new Spark session, and name it \"Spark-Notebook.\" This session will be used to interact with our streaming data throughout the notebook.\n\n## Reading from Kafka Stream\n\nThis section describes how to read data streams from Kafka. The data is retrieved using the `readStream` function.\n\n### 1.1 Raw Kafka Stream\n\nThe initial step in acquiring our data stream is connecting to the Kafka topic. In this example, we utilize the \"rides_csv\" topic.\n\n```python\n# default for startingOffsets is \"latest\"\ndf_kafka_raw = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n    .option(\"subscribe\", \"rides_csv\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .option(\"checkpointLocation\", \"checkpoint\") \\\n    .load()\n```\n\nIn this code block, we set the Kafka bootstrap servers, specify the topic to subscribe to, and configure the starting offsets. The `checkpointLocation` is crucial for fault tolerance, ensuring the stream can be resumed in case of failures.\n\n```python\ndf_kafka_raw.printSchema()\n```\n\nThe `printSchema` method displays the structure of the DataFrame, allowing us to understand the nested structure of the incoming Kafka data.\n\n### 1.2 Encoded Kafka Stream\n\nNext, we need to cast the key and value columns to string types for easier handling.\n\n```python\ndf_kafka_encoded = df_kafka_raw.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n```\n\nThis transformation ensures that both the key and value from the Kafka messages are in string format, setting up the DataFrame for further processing.\n\n```python\ndf_kafka_encoded.printSchema()\n```\n\nWe again check the DataFrame's schema to verify that the data types have been correctly transformed.\n\n### 1.3 Structure Streaming DataFrame\n\nTo extract and structure the ride data, we define a helper function that parses the incoming messages based on a predefined schema.\n\n```python\ndef parse_ride_from_kafka_message(df_raw, schema):\n    \"\"\" take a Spark Streaming df and parse value col based on <schema>, return streaming df cols in schema \"\"\"\n    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n\n    df = df_raw.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\n    # split attributes to nested array in one Column\n    col = F.split(df['value'], ', ')\n\n    # expand col to multiple top-level columns\n    for idx, field in enumerate(schema):\n        df = df.withColumn(field.name, col.getItem(idx).cast(field.dataType))\n    return df.select([field.name for field in schema])\n```\n\nThis function checks if the DataFrame is streaming, casts the key and value columns as strings, splits the value into an array, and expands it into separate columns according to the provided schema.\n\n```python\nride_schema = T.StructType(\n    [T.StructField(\"vendor_id\", T.IntegerType()),\n     T.StructField('tpep_pickup_datetime', T.TimestampType()),\n     T.StructField('tpep_dropoff_datetime', T.TimestampType()),\n     T.StructField(\"passenger_count\", T.IntegerType()),\n     T.StructField(\"trip_distance\", T.FloatType()),\n     T.StructField(\"payment_type\", T.IntegerType()),\n     T.StructField(\"total_amount\", T.FloatType()),\n     ])\n```\n\nWe define a schema for ride data using `StructType`. This schema specifies the expected fields and their types, which will later be used to transform the incoming data.\n\n```python\ndf_rides = parse_ride_from_kafka_message(df_raw=df_kafka_raw, schema=ride_schema)\n```\n\nWith the function and schema defined, we parse the raw Kafka DataFrame to create a structured DataFrame containing the specific ride data.\n\n```python\ndf_rides.printSchema()\n```\n\nAs before, we check the structure of the ride DataFrame to ensure that it matches our expected schema.\n\n```python\ndf_rides.show()\n```\n\nFinally, we display the contents of the DataFrame, which shows the processed ride data.\n\n## Sink Operation & Streaming Query\n\nOnce the data is read and structured, the next step is to write the streaming results to a sink. This section provides details on how to implement different sink operations.\n\n---\n**Output Sinks**  \nThe key types of sinks in Spark Structured Streaming include:\n\n- **File Sink**: Writes output data to files.\n- **Kafka Sink**: Publishes data to Kafka topics.\n- **Foreach Sink**: Allows custom handling of each row.\n- **Console Sink**: For debugging, outputs data to the console.\n\nFor more details, refer to the [Output Sinks documentation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks).\n\n---\nThere are three types of **Output Modes** for streaming queries:\n\n- **Complete**: Outputs the entire Result Table after every trigger.\n- **Append** (default): Only new rows are added.\n- **Update**: Only updated rows are outputted.\n\nThe choice of output mode depends on transformations applied to the streaming data. Further information on output modes can be found [here](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes).\n\n---\n**Triggers**  \nTriggers in Spark Streaming define how often the streaming data is processed. Options include:\n\n- Default micro-batch mode.\n- Fixed interval micro-batch mode.\n- One-time micro-batch mode.\n- Available-now micro-batch mode.\n\nRefer to the [Trigger settings documentation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers) for more information.\n\n### Console and Memory Sink\n\nTo visualize or remember the streaming data, we can set up console and memory sinks.\n\n```python\ndef sink_console(df, output_mode: str = 'complete', processing_time: str = '5 seconds'):\n    write_query = df.writeStream \\\n        .outputMode(output_mode) \\\n        .trigger(processingTime=processing_time) \\\n        .format(\"console\") \\\n        .option(\"truncate\", False) \\\n        .start()\n    return write_query # pyspark.sql.streaming.StreamingQuery\n```\n\nThis function sets up a console sink that outputs the streaming DataFrame. It allows defining an output mode and the processing time for how often to update the console.\n\n```python\nwrite_query = sink_console(df_rides, output_mode='append')\n```\n\nWe initialize the console sink using the `df_rides` DataFrame, specifying the output mode as 'append'.\n\n```python\ndef sink_memory(df, query_name, query_template):\n    write_query = df \\\n        .writeStream \\\n        .queryName(query_name) \\\n        .format('memory') \\\n        .start()\n    query_str = query_template.format(table_name=query_name)\n    query_results = spark.sql(query_str)\n    return write_query, query_results\n```\n\nThis function sets up a memory sink, allowing us to run SQL queries against the results stored in memory. It captures the streaming DataFrame under a specific query name.\n\n```python\nquery_name = 'vendor_id_counts'\nquery_template = 'select count(distinct(vendor_id)) from {table_name}'\nwrite_query, df_vendor_id_counts = sink_memory(df=df_rides, query_name=query_name, query_template=query_template)\n```\n\nIn this code block, we create a query to count distinct vendor IDs from the `df_rides` DataFrame, storing the results in memory for easy access.\n\n```python\nprint(type(write_query)) # pyspark.sql.streaming.StreamingQuery\nwrite_query.status\n```\n\nWe check the type and status of the write query to ensure that it is active and streaming correctly.\n\n```python\ndf_vendor_id_counts.show()\n```\n\nWe display the results of our SQL query, which gives us the count of distinct vendor IDs from the ride data.\n\n```python\nwrite_query.stop()\n```\n\nFinally, we stop the streaming query to end the stream when it is no longer needed.\n\n### Kafka Sink\n\nTo send streaming results back into a Kafka topic, we prepare the DataFrame to meet Kafka's expected structure.\n\n```python\ndef prepare_dataframe_to_kafka_sink(df, value_columns, key_column=None):\n    columns = df.columns\n    df = df.withColumn(\"value\", F.concat_ws(', ', *value_columns))    \n    if key_column:\n        df = df.withColumnRenamed(key_column, \"key\")\n        df = df.withColumn(\"key\", df.key.cast('string'))\n    return df.select(['key', 'value'])\n\ndef sink_kafka(df, topic, output_mode='append'):\n    write_query = df.writeStream \\\n        .format(\"kafka\") \\\n        .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n        .outputMode(output_mode) \\\n        .option(\"topic\", topic) \\\n        .option(\"checkpointLocation\", \"checkpoint\") \\\n        .start()\n    return write_query\n```\n\nIn these functions, we define how to prepare the DataFrame for Kafka by concatenating value columns into a single \"value\" column and optionally setting a \"key\" column. The `sink_kafka` function sends the structured DataFrame to a specified Kafka topic.\n\n---\nThis concludes the setup for reading from Kafka, structuring the data, and deploying it to various sinks, including console and Kafka. Each step is geared toward ensuring efficient handling of streaming data within a Spark application.",
    "filename": "06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb"
  },
  {
    "code": false,
    "content": "# Overview of Spark Streaming Application\n\nThis code defines a Spark Streaming application that processes real-time data from a Kafka topic containing ride information. It provides functionality for reading, parsing, aggregating, and outputting this data to various sinks, such as console and Kafka. \n\n## Key Components\n\n### 1. **Imports and Configurations**\n- The code imports necessary libraries from `pyspark.sql` for building Spark sessions and working with DataFrames.\n- It imports functions as `F` from PySpark to facilitate various DataFrame operations.\n- Configuration constants like `RIDE_SCHEMA`, `CONSUME_TOPIC_RIDES_CSV`, and `TOPIC_WINDOWED_VENDOR_ID_COUNT` are imported from a settings module.\n\n### 2. **Function: `read_from_kafka`**\nThis function establishes a connection to the specified Kafka topic and reads streaming data.\n\n- **Parameters**: \n  - `consume_topic`: The Kafka topic to subscribe to.\n- **Returns**: A streaming DataFrame that represents the data read from Kafka.\n- It specifies bootstrap servers, offsets, checkpoint locations, and formats for reading the data.\n\n### 3. **Function: `parse_ride_from_kafka_message`**\nOnce the data is read from Kafka, this function parses the incoming messages according to a predefined schema.\n\n- **Parameters**:\n  - `df`: The DataFrame containing streaming data.\n  - `schema`: The schema to which the incoming data should conform.\n- **Returns**: A DataFrame with columns defined in the schema.\n- It casts keys and values to strings, splits the `value` column into separate fields, and expands them into individual columns in the DataFrame.\n\n### 4. **Sink Functions**\nThe code defines multiple output functions to store the streaming data:\n\n#### 4.1. `sink_console`\n- **Parameters**:\n  - `df`: The DataFrame to be output.\n  - `output_mode`: Determines how output is displayed.\n  - `processing_time`: The time interval for processing.\n- **Returns**: A streaming query that writes data to the console.\n\n#### 4.2. `sink_memory`\n- **Parameters**:\n  - `df`: The DataFrame to write to memory.\n  - `query_name`: A name for the query.\n  - `query_template`: Template for retrieving query results.\n- **Returns**: Query results and the streaming query that writes to memory.\n\n#### 4.3. `sink_kafka`\n- **Parameters**:\n  - `df`: The DataFrame to write to Kafka.\n  - `topic`: The Kafka topic where data will be sent.\n- **Returns**: A streaming query that writes data to the specified Kafka topic.\n\n### 5. **Function: `prepare_df_to_kafka_sink`**\nThis function formats the DataFrame to prepare it for output to Kafka.\n\n- **Parameters**:\n  - `df`: Input DataFrame.\n  - `value_columns`: Columns to be included in the 'value' for Kafka messages.\n  - `key_column`: Optional key column for Kafka.\n- **Returns**: A DataFrame with 'key' and 'value' columns ready for Kafka.\n\n### 6. **Aggregation Functions**\nThe code provides two aggregation functions for summarizing the ride data.\n\n#### 6.1. `op_groupby`\n- **Parameters**:\n  - `df`: The DataFrame to be aggregated.\n  - `column_names`: List of column names for grouping.\n- **Returns**: A DataFrame with the counts of occurrences for each group.\n\n#### 6.2. `op_windowed_groupby`\n- **Parameters**:\n  - `df`: The input DataFrame.\n  - `window_duration`: Duration for each time window.\n  - `slide_duration`: Frequency of sliding the window.\n- **Returns**: A DataFrame that includes aggregated counts based on time windows.\n\n### 7. **Execution Flow**\nThe execution flow of the script is as follows:\n\n1. **Initialization**:\n   - Creates a Spark session and sets a logging level.\n\n2. **Data Reading**:\n   - Calls `read_from_kafka` to create a streaming DataFrame from a specified Kafka topic.\n\n3. **Data Parsing**:\n   - Invokes `parse_ride_from_kafka_message` to parse the ride data according to a schema.\n\n4. **Data Sink**:\n   - Outputs the parsed data to console for debugging using `sink_console`.\n\n5. **Aggregation**:\n   - Uses `op_groupby` to count rides by vendor IDs.\n   - Uses `op_windowed_groupby` to count rides within time windows.\n\n6. **Sinks for Aggregated Data**:\n   - Counts are output to the console for debugging.\n   - Prepares the aggregated data to be sent to Kafka using `prepare_df_to_kafka_sink`.\n   - Sends the prepared data to a Kafka topic using `sink_kafka`.\n\n7. **Waiting for Termination**:\n   - Keeps the streaming context active, awaiting termination signals.\n\nThis application is a robust method for real-time analytics of ride data, efficiently handling streaming data from Kafka, processing it, and outputting results to multiple destinations.",
    "filename": "06-streaming/python/streams-example/redpanda/streaming.py"
  },
  {
    "content": "<p align=\"center\">\n  <img width=\"100%\" src=\"images/architecture/arch_v4_workshops.jpg\" alt=\"Data Engineering Zoomcamp Overview\">\n</p>\n\n<h1 align=\"center\">\n    <strong>Data Engineering Zoomcamp: A Free 9-Week Course on Data Engineering Fundamentals</strong>\n</h1>\n\n<p align=\"center\">\nMaster the fundamentals of data engineering by building an end-to-end data pipeline from scratch. Gain hands-on experience with industry-standard tools and best practices.\n</p>\n\n<p align=\"center\">\n<a href=\"https://airtable.com/shr6oVXeQvSI5HuWD\"><img src=\"https://user-images.githubusercontent.com/875246/185755203-17945fd1-6b64-46f2-8377-1011dcb1a444.png\" height=\"50\" /></a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://datatalks.club/slack.html\">Join Slack</a> \u2022\n<a href=\"https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG\">#course-data-engineering Channel</a> \u2022\n<a href=\"https://t.me/dezoomcamp\">Telegram Announcements</a> \u2022\n<a href=\"https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb\">Course Playlist</a> \u2022\n<a href=\"https://datatalks.club/faq/data-engineering-zoomcamp.html\">FAQ</a>\n</p>\n\n## How to Enroll\n\n### 2026 Cohort\n- **Start Date**: January 2026\n- **Register Here**: [Sign up](https://airtable.com/shr6oVXeQvSI5HuWD)\n\n### Self-Paced Learning\nAll course materials are freely available for independent study. Follow these steps:\n1. Watch the course videos.\n2. Join the [Slack community](https://datatalks.club/slack.html).\n3. Refer to the [FAQ document](https://datatalks.club/faq/data-engineering-zoomcamp.html) for guidance.\n\n## Syllabus Overview\nThe course consists of structured modules, hands-on workshops, and a final project to reinforce your learning.\n\n### **Prerequisites**\nTo get the most out of this course, you should have:\n- Basic coding experience\n- Familiarity with SQL\n- Experience with Python (helpful but not required)\n\nNo prior data engineering experience is necessary.\n\n### **Modules**\n\n#### [Module 1: Containerization and Infrastructure as Code](01-docker-terraform/)\n- Introduction to GCP\n- Docker and Docker Compose\n- Running PostgreSQL with Docker\n- Infrastructure setup with Terraform\n- Homework\n\n#### [Module 2: Workflow Orchestration](02-workflow-orchestration/)\n- Data Lakes and Workflow Orchestration\n- Workflow orchestration with Kestra\n- Homework\n\n#### [Workshop 1: Data Ingestion](cohorts/2025/workshops/dlt/README.md)\n- API reading and pipeline scalability\n- Data normalization and incremental loading\n- Homework\n\n#### [Module 3: Data Warehousing](03-data-warehouse/)\n- Introduction to BigQuery\n- Partitioning, clustering, and best practices\n- Machine learning in BigQuery\n\n#### [Module 4: Analytics Engineering](04-analytics-engineering/)\n- dbt (data build tool) with PostgreSQL & BigQuery\n- Testing, documentation, and deployment\n- Data visualization with Metabase\n\n#### [Module 5: Batch Processing](05-batch/)\n- Introduction to Apache Spark\n- DataFrames and SQL\n- Internals of GroupBy and Joins\n\n#### [Module 6: Streaming](06-streaming/)\n- Introduction to Kafka\n- Kafka Streams and KSQL\n- Schema management with Avro\n\n#### [Final Project](projects/)\n- Apply all concepts learned in a real-world scenario\n- Peer review and feedback process\n\n## Community & Support\n\n### **Getting Help on Slack**\nJoin the [`#course-data-engineering`](https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG) channel on [DataTalks.Club Slack](https://datatalks.club/slack.html) for discussions, troubleshooting, and networking.\n\nTo keep discussions organized:\n- Follow [our guidelines](asking-questions.md) when posting questions.\n- Review the [community guidelines](https://datatalks.club/slack/guidelines.html).\n\n## Meet the Instructors\n- [Victoria Perez Mola](https://www.linkedin.com/in/victoriaperezmola/)\n- [Alexey Grigorev](https://linkedin.com/in/agrigorev)\n- [Michael Shoemaker](https://www.linkedin.com/in/michaelshoemaker1/)\n- [Zach Wilson](https://www.linkedin.com/in/eczachly)\n- [Will Russell](https://www.linkedin.com/in/wrussell1999/)\n- [Anna Geller](https://www.linkedin.com/in/anna-geller-12a86811a/)\n\nPast instructors:\n- [Ankush Khanna](https://linkedin.com/in/ankushkhanna2)\n- [Sejal Vaidya](https://www.linkedin.com/in/vaidyasejal/)\n- [Irem Erturk](https://www.linkedin.com/in/iremerturk/)\n- [Luis Oliveira](https://www.linkedin.com/in/lgsoliveira/)\n\n## Sponsors & Supporters\nA special thanks to our course sponsors for making this initiative possible!\n\n<p align=\"center\">\n  <a href=\"https://kestra.io/\">\n    <img height=\"120\" src=\"images/kestra.svg\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://dlthub.com/\">\n    <img height=\"90\" src=\"images/dlthub.png\">\n  </a>\n</p>\n\nInterested in supporting our community? Reach out to [alexey@datatalks.club](mailto:alexey@datatalks.club).\n\n## About DataTalks.Club\n\n<p align=\"center\">\n  <img width=\"40%\" src=\"https://github.com/user-attachments/assets/1243a44a-84c8-458d-9439-aaf6f3a32d89\" alt=\"DataTalks.Club\">\n</p>\n\n<p align=\"center\">\n<a href=\"https://datatalks.club/\">DataTalks.Club</a> is a global online community of data enthusiasts. It's a place to discuss data, learn, share knowledge, ask and answer questions, and support each other.\n</p>\n\n<p align=\"center\">\n<a href=\"https://datatalks.club/\">Website</a> \u2022\n<a href=\"https://datatalks.club/slack.html\">Join Slack Community</a> \u2022\n<a href=\"https://us19.campaign-archive.com/home/?u=0d7822ab98152f5afc118c176&id=97178021aa\">Newsletter</a> \u2022\n<a href=\"http://lu.ma/dtc-events\">Upcoming Events</a> \u2022\n<a href=\"https://calendar.google.com/calendar/?cid=ZjhxaWRqbnEwamhzY3A4ODA5azFlZ2hzNjBAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ\">Google Calendar</a> \u2022\n<a href=\"https://www.youtube.com/@DataTalksClub/featured\">YouTube</a> \u2022\n<a href=\"https://github.com/DataTalksClub\">GitHub</a> \u2022\n<a href=\"https://www.linkedin.com/company/datatalks-club/\">LinkedIn</a> \u2022\n<a href=\"https://twitter.com/DataTalksClub\">Twitter</a>\n</p>\n\nAll the activity at DataTalks.Club mainly happens on [Slack](https://datatalks.club/slack.html). We post updates there and discuss different aspects of data, career questions, and more.\n\nAt DataTalksClub, we organize online events, community activities, and free courses. You can learn more about what we do at [DataTalksClub Community Navigation](https://www.notion.so/DataTalksClub-Community-Navigation-bf070ad27ba44bf6bbc9222082f0e5a8?pvs=21).",
    "filename": "README.md"
  },
  {
    "content": "## Thank you!\n\nThanks for signing up for the course.\n\nThe process of adding you to the mailing list is not automated yet, \nbut you will hear from us closer to the course start. \n\nTo make sure you don't miss any announcements\n\n- Register in [DataTalks.Club's Slack](https://datatalks.club/slack.html) and\n  join the [`#course-data-engineering`](https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG) channel\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp)\n- Subscribe to [DataTalks.Club's YouTube channel](https://www.youtube.com/c/DataTalksClub) and check \n  [the course playlist](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n- Subscribe to our [public Google Calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ) (it works from Desktop only)\n\nSee you in January!",
    "filename": "after-sign-up.md"
  },
  {
    "content": "## Asking questions\n\nIf you have any questions, ask them \nin the [`#course-data-engineering`](https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG) channel in [DataTalks.Club](https://datatalks.club) slack.\n\nTo keep our discussion in Slack more organized, we ask you to follow these suggestions:\n\n* First, review How to troubleshoot issues listed below.\n* Before asking a question, check the [FAQ](https://datatalks.club/faq/data-engineering-zoomcamp.html).\n* Before asking a question review the [Slack Guidelines](#Ask-in-Slack).\n* If somebody helped you with your problem and it's not in [FAQ](https://datatalks.club/faq/data-engineering-zoomcamp.html), please add it there.\n  It'll help other students.\n* Zed Shaw (of the Learn the Hard Way series) has [a great post on how to help others help you](https://learncodethehardway.com/blog/03-how-to-ask-for-help/)\n* Check [Stackoverflow guide on asking](https://stackoverflow.com/help/how-to-ask)\n  \n### How to troubleshoot issues\n\nThe first step is to try to solve the issue on you own; get used to solving problems. This will be a real life skill you need when employed.\n\n1. What does the error say? There will often be a description of the error or instructions on what is needed, I have even seen a link to the solution. Does it reference a specific line of your code?\n2. Restart the application or server/pc. \n3. Google it. It is going to be rare that you are the first to have the problem, someone out there has posted the issue and likely the solution. Search using: **technology** **problem statement**. Example: `pgcli error column c.relhasoids does not exist`. \n    * There are often different solutions for the same problem due to variation in environments. \n4. Check the tech\u2019s documentation. Use its search if available or use the browser's search function. \n5. Try uninstall (this may remove the bad actor) and reinstall of application or re-implementation of action. Don\u2019t forget to restart the server/pc for reinstalls.\n    * Sometimes reinstalling fails to resolve the issue but works if you uninstall first.\n6. Ask in Slack\n7. Take a break and come back to it later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day. \n8. Remember technology issues in real life sometimes take days or even weeks to resolve\n\n### Asking in Slack\n\n* Before asking a question, check the [FAQ](https://datatalks.club/faq/data-engineering-zoomcamp.html).\n* DO NOT use screenshots, especially don\u2019t take pictures from a phone.\n* DO NOT tag instructors, it may discourage others from helping you.\n* Copy and paste errors; if it\u2019s long, just post it in a reply to your thread. \n* Use ``` for formatting your code.\n* Use the same thread for the conversation (that means replying to your own thread). \n* DO NOT create multiple posts to discuss the issue.\n* You may create a new post if the issue reemerges down the road. Be sure to describe what has changed in the environment.\n* Provide additional information in the same thread of the steps you have taken for resolution.",
    "filename": "asking-questions.md"
  },
  {
    "content": "Have you found any cool resources about data engineering? Put them here\n\n## Learning Data Engineering\n\n### Courses\n\n* [Data Engineering Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp) by DataTalks.Club (free)\n* [Big Data Platforms, Autumn 2022: Introduction to Big Data Processing Frameworks](https://big-data-platforms-22.mooc.fi/) by the University of Helsinki (free)   \n* [Awesome Data Engineering Learning Path](https://awesomedataengineering.com/)\n\n\n### Books\n\n* [Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems by Martin Kleppmann](https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321)\n* [Big Data: Principles and Best Practices of Scalable Realtime Data Systems by Nathan Marz, James Warren](https://www.amazon.com/Big-Data-Principles-practices-scalable/dp/1617290343)\n* [Practical DataOps: Delivering Agile Data Science at Scale by Harvinder Atwal](https://www.amazon.com/Practical-DataOps-Delivering-Agile-Science/dp/1484251032)\n* [Data Pipelines Pocket Reference: Moving and Processing Data for Analytics by James Densmore](https://www.amazon.com/Data-Pipelines-Pocket-Reference-Processing/dp/1492087831)\n* [Best books for data engineering](https://awesomedataengineering.com/data_engineering_best_books)\n* [Fundamentals of Data Engineering: Plan and Build Robust Data Systems by Joe Reis, Matt Housley](https://www.amazon.com/Fundamentals-Data-Engineering-Robust-Systems/dp/1098108302)\n\n\n### Introduction to Data Engineering Terms\n\n* [https://datatalks.club/podcast/s05e02-data-engineering-acronyms.html](https://datatalks.club/podcast/s05e02-data-engineering-acronyms.html) \n\n\n### Data engineering in practice\n\nConference talks from companies, blog posts, etc\n\n* [Uber Data Archives](https://eng.uber.com/category/articles/uberdata/) (Uber engineering blog)\n* [Data Engineering Weekly (DE-focused substack)](https://www.dataengineeringweekly.com/)\n* [Seattle Data Guy (DE-focused substack)](https://seattledataguy.substack.com/) \n\n\n## Doing Data Engineering\n\n### Coding & Python\n\n* [CS50's Introduction to Computer Science | edX](https://www.edx.org/course/introduction-computer-science-harvardx-cs50x) (course)\n* [Python for Everybody Specialization](https://www.coursera.org/specializations/python) (course)\n* [Practical Python programming](https://github.com/dabeaz-course/practical-python/blob/master/Notes/Contents.md)\n\n\n### SQL\n\n* [Intro to SQL: Querying and managing data | Khan Academy](https://www.khanacademy.org/computing/computer-programming/sql) \n* [Mode SQL Tutorial](https://mode.com/sql-tutorial/)\n* [Use The Index, Luke](https://use-the-index-luke.com/) (SQL Indexing a        nd Tuning e-Book)nfreffx \n* [SQL Performance Explained](https://sql-performance-explained.com/) (book)  e\n\n\n### Workflow orchestration\n\n* [What is DAG?](https://youtu.be/1Yh5S-S6wsI) (video) \n* [Airflow, Prefect, and Dagster: An Inside Look](https://towardsdatascience.com/airflow-prefect-and-dagster-an-inside-look-6074781c9b77) (blog post) \n* [Open-Source Spotlight - Prefect - Kevin Kho](https://www.youtube.com/watch?v=ISLV9JyqF1w) (video) \n* [Prefect as a Data Engineering Project Workflow Tool, with Mary Clair Thompson (Duke) - 11/6/2020](https://youtu.be/HuwA4wLQtCM) (video) \n\n\n### ETL and ELT\n\n* [ETL vs. ELT: What\u2019s the Difference?](https://rivery.io/blog/etl-vs-elt/) (blog post) (print version)\n\n### Data lakes\n\n* [An Introduction to Modern Data Lake Storage Layers (Hodi, Iceberg, Delta Lake)](https://dacort.dev/posts/modern-data-lake-storage-layers/) (blog post) \n* [Lake House Architecture @ Halodoc: Data Platform 2.0](https://blogs.halodoc.io/lake-house-architecture-halodoc-data-platform-2-0/amp/) (blzog post) \n\n\n### Data warehousing\n\n\n* [Guide to Data Warehousing. Short and comprehensive information\u2026 | by Tomas Peluritis](https://medium.com/towards-data-science/guide-to-data-warehousing-6fdcf30b6fbe) (blog post) \n* [Snowflake, Redshift, BigQuery, and Others: Cloud Data Warehouse Tools Compared](https://www.altexsoft.com/blog/snowflake-redshift-bigquery-data-warehouse-tools/) (blog post)\n\n\n### Streaming\n\n\n*   Building Streaming Analytics: The Journey and Learnings - Maxim Lukichev\n\n### DataOps\n\n* [DataOps 101 with Lars Albertsson \u2013 DataTalks.Club](https://datatalks.club/podcast/s02e11-dataops.html) (podcast)\n*  \n\n\n### Monitoring and observability \n\n* [Data Observability: The Next Frontier of Data Engineering with Barr Moses](https://datatalks.club/podcast/s03e03-data-observability.html) (podcast)\n\n\n### Analytics engineering\n\n* [Analytics Engineer: New Role in a Data Team with Victoria Perez Mola](https://datatalks.club/podcast/s03e11-analytics-engineer.html) (podcast)\n* [Modern Data Stack for Analytics Engineering - Kyle Shannon](https://www.youtube.com/watch?v=UmIZIkeOfi0) (video) \n* [Analytics Engineering vs Data Engineering | RudderStack Blog](https://www.rudderstack.com/blog/analytics-engineering-vs-data-engineering) (blog post)\n* [Learn the Fundamentals of Analytics Engineering with dbt](https://courses.getdbt.com/courses/fundamentals) (course)\n\n\n### Data mesh\n\n* [Data Mesh in Practice - Max Schultze](https://www.youtube.com/watch?v=ekEc8D_D3zY) (video)\n\n### Cloud\n\n* [https://acceldataio.medium.com/data-engineering-best-practices-how-netflix-keeps-its-data-infrastructure-cost-effective-dee310bcc910](https://acceldataio.medium.com/data-engineering-best-practices-how-netflix-keeps-its-data-infrastructure-cost-effective-dee310bcc910) \n\n\n### Reverse ETL\n\n* TODO: What is reverse ETL?\n* [https://datatalks.club/podcast/s05e02-data-engineering-acronyms.html](https://datatalks.club/podcast/s05e02-data-engineering-acronyms.html) \n* [Open-Source Spotlight - Grouparoo - Brian Leonard](https://www.youtube.com/watch?v=hswlcgQZYuw) (video) \n* [Open-Source Spotlight - Castled.io (Reverse ETL) - Arun Thulasidharan](https://www.youtube.com/watch?v=iW0XhltAUJ8) (video) \n\n## Career in Data Engineering\n\n* [From Data Science to Data Engineering with Ellen K\u00f6nig \u2013 DataTalks.Club](https://datatalks.club/podcast/s07e08-from-data-science-to-data-engineering.html) (podcast)\n* [Big Data Engineer vs Data Scientist with Roksolana Diachuk \u2013 DataTalks.Club](https://datatalks.club/podcast/s04e03-big-data-engineer-vs-data-scientist.html) (podcast)\n* [What Skills Do You Need to Become a Data Engineer](https://www.linkedin.com/pulse/what-skills-do-you-need-become-data-engineer-peng-wang/) (blog post) \n* [The future history of Data Engineering](https://groupby1.substack.com/p/data-engineering?s=r) (blog post) \n* [What Skills Do Data Engineers Need](https://www.theseattledataguy.com/what-skills-do-data-engineers-need/) (blog post)\n\n### Data Engineering Management \n\n* [Becoming a Data Engineering Manager with Rahul Jain \u2013 DataTalks.Club](https://datatalks.club/podcast/s07e07-becoming-a-data-engineering-manager.html) (podcast)\n\n## Data engineering projects\n\n* [How To Start A Data Engineering Project - With Data Engineering Project Ideas](https://www.youtube.com/watch?v=WpN47Jddo7I) (video)\n* [Data Engineering Project for Beginners - Batch edition](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/) (blog post)\n* [Building a Data Engineering Project in 20 Minutes](https://www.sspaeti.com/blog/data-engineering-project-in-twenty-minutes/) (blog post)\n* [Automating Nike Run Club Data Analysis with Python, Airflow and Google Data Studio | by Rich Martin | Medium](https://medium.com/@rich_23525/automating-nike-run-club-data-analysis-with-python-airflow-and-google-data-studio-3c9556478926) (blog post)\n\n\n## Data Engineering Resources \n\n### Blogs\n\n* [Start Data Engineering](https://www.startdataengineering.com/)\n\n### Podcasts\n\n* [The Data Engineering Podcast](https://www.dataengineeringpodcast.com/)\n* [DataTalks.Club Podcast](https://datatalks.club/podcast.html) (only some episodes are about data engineering) \n* \n\n### Communities\n\n* [DataTalks.Club](https://datatalks.club/)\n* [/r/dataengineering](https://www.reddit.com/r/dataengineering) \n\n\n### Meetups\n\n* [Sydney Data Engineers](https://sydneydataengineers.github.io/) \n\n### People to follow on Twitter and LinkedIn\n\n* TODO\n\n### YouTube channels\n\n* [Karolina Sowinska - YouTube](https://www.youtube.com/channel/UCAxnMry1lETl47xQWABvH7g) x`\n* [Seattle Data Guy - YouTube](https://www.youtube.com/c/SeattleDataGuy) \n* [Andreas Kretz - YouTube](https://www.youtube.com/c/andreaskayy) \n* [DataTalksClub - YouTube](https://youtube.com/c/datatalksclub) (only some videos are about data engineering) \n\n### Resource aggregators\n\n* [Reading List](https://www.scling.com/reading-list/) by Lars Albertsson\n* [GitHub - igorbarinov/awesome-data-engineering](https://github.com/igorbarinov/awesome-data-engineering) (focus is more on tools)  \n\n\n## License\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n\nCC BY 4.0",
    "filename": "awesome-data-engineering.md"
  },
  {
    "content": "## Getting your certificate\n\nCongratulations on finishing the course!\n\nYou can find your certificate in your enrollment profile (you need to be logged in):\n\n* For the 2025 edition, it's https://courses.datatalks.club/de-zoomcamp-2025/enrollment\n\nIf you can't find a certificate in your profile, it means you didn't pass the project.\nIf you believe it's a mistake, write in the course channel in Slack.\n\n\n## Adding to LinkedIn\n\nYou can add your certificate to LinkedIn:\n\n* Log in to your LinkedIn account, then go to your profile.\n* On the right, in the \"Add profile\" section dropdown, choose \"Background\" and then select the drop-down triangle next to \"Licenses & Certifications\".\n* In \"Name\", enter \"Data Engineering Zoomcamp\".\n* In \"Issuing Organization\", enter \"DataTalksClub\".\n* (Optional) In \"Issue Date\", enter the time when the certificate was created.\n* (Optional) Select the checkbox This certification does not expire. \n* Put your certificate ID.\n* In \"Certification URL\", enter the URL for your certificate.\n\n[Adapted from here](https://support.edx.org/hc/en-us/articles/206501938-How-can-I-add-my-certificate-to-my-LinkedIn-profile-)",
    "filename": "certificates.md"
  },
  {
    "content": "### 2022 Cohort\n\n* **Start**: 17 January 2022\n* **Registration link**: https://airtable.com/shr6oVXeQvSI5HuWD\n* [Leaderboard](https://docs.google.com/spreadsheets/d/e/2PACX-1vR9oQiYnAVvzL4dagnhvp0sngqagF0AceD0FGjhS-dnzMTBzNQIal3-hOgkTibVQvfuqbQ69b0fvRnf/pubhtml)\n* Subscribe to our [public Google Calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ) (it works from Desktop only)",
    "filename": "cohorts/2022/README.md"
  },
  {
    "content": "## Course Project\n\nThe goal of this project is to apply everything we learned\nin this course and build an end-to-end data pipeline.\n\nRemember that to pass the project, you must evaluate 3 peers. If you don't do that, your project can't be considered compelete.\n\n\n### Submitting \n\n#### Project Cohort #2\n\nProject:\n\n* Form: https://forms.gle/JECXB9jYQ1vBXbsw6\n* Deadline: 2 May, 22:00 CET\n\nPeer reviewing:\n\n* Peer review assignments: [link](https://docs.google.com/spreadsheets/d/e/2PACX-1vShnv8T4iY_5NA8h0nySIS8Wzr-DZGGigEikIW4ZMSi9HlvhaEB4RhwmepVIuIUGaQHS90r5iHR2YXV/pubhtml?gid=964123374&single=true)\n* Form: https://forms.gle/Pb2fBwYLQ3GGFsaK6\n* Deadline: 9 May, 22:00 CET\n\n\n#### Project Cohort #1\n\nProject:\n\n* Form: https://forms.gle/6aeVcEVJipqR2BqC8\n* Deadline: 4 April, 22:00 CET\n\nPeer reviewing:\n\n* Peer review assignments: [link](https://docs.google.com/spreadsheets/d/e/2PACX-1vShnv8T4iY_5NA8h0nySIS8Wzr-DZGGigEikIW4ZMSi9HlvhaEB4RhwmepVIuIUGaQHS90r5iHR2YXV/pubhtml)\n* Form: https://forms.gle/AZ62bXMp4SGcVUmK7\n* Deadline: 11 April, 22:00 CET\n\nProject feedback: [link](https://docs.google.com/spreadsheets/d/e/2PACX-1vRcVCkO-jes5mbPAcikn9X_s2laJ1KhsO8aibHYQxxKqdCUYMVTEJLJQdM8C5aAUWKFl_0SJW4rme7H/pubhtml)",
    "filename": "cohorts/2022/project.md"
  },
  {
    "content": "## Week 1 Homework\n\nIn this homework we'll prepare the environment \nand practice with terraform and SQL\n\n\n## Question 1. Google Cloud SDK\n\nInstall Google Cloud SDK. What's the version you have? \n\nTo get the version, run `gcloud --version`\n\n## Google Cloud account \n\nCreate an account in Google Cloud and create a project.\n\n\n## Question 2. Terraform \n\nNow install terraform and go to the terraform directory (`week_1_basics_n_setup/1_terraform_gcp/terraform`)\n\nAfter that, run\n\n* `terraform init`\n* `terraform plan`\n* `terraform apply` \n\nApply the plan and copy the output (after running `apply`) to the form.\n\nIt should be the entire output - from the moment you typed `terraform init` to the very end.\n\n## Prepare Postgres \n\nRun Postgres and load data as shown in the videos\n\nWe'll use the yellow taxi trips from January 2021:\n\n```bash\nwget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv\n```\n\nYou will also need the dataset with zones:\n\n```bash \nwget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n```\n\nDownload this data and put it to Postgres\n\n## Question 3. Count records \n\nHow many taxi trips were there on January 15?\n\nConsider only trips that started on January 15.\n\n\n## Question 4. Largest tip for each day\n\nFind the largest tip for each day. \nOn which day it was the largest tip in January?\n\nUse the pick up time for your calculations.\n\n(note: it's not a typo, it's \"tip\", not \"trip\")\n\n\n## Question 5. Most popular destination\n\nWhat was the most popular destination for passengers picked up \nin central park on January 14?\n\nUse the pick up time for your calculations.\n\nEnter the zone name (not id). If the zone name is unknown (missing), write \"Unknown\" \n\n\n## Question 6. Most expensive locations\n\nWhat's the pickup-dropoff pair with the largest \naverage price for a ride (calculated based on `total_amount`)?\n\nEnter two zone names separated by a slash\n\nFor example:\n\n\"Jamaica Bay / Clinton East\"\n\nIf any of the zone names are unknown (missing), write \"Unknown\". For example, \"Unknown / Clinton East\". \n\n\n## Submitting the solutions\n\n* Form for submitting: https://forms.gle/yGQrkgRdVbiFs8Vd7\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 26 January (Wednesday), 22:00 CET\n\n\n## Solution\n\nHere is the solution to questions 3-6: [video](https://www.youtube.com/watch?v=HxHqH2ARfxM&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)",
    "filename": "cohorts/2022/week_1_basics_n_setup/homework.md"
  },
  {
    "content": "## Week 2: Data Ingestion\n\n### Data Lake (GCS)\n\n* What is a Data Lake\n* ELT vs. ETL\n* Alternatives to components (S3/HDFS, Redshift, Snowflake etc.)\n\n:movie_camera: [Video](https://www.youtube.com/watch?v=W3Zm6rjOq70&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n[Slides](https://docs.google.com/presentation/d/1RkH-YhBz2apIjYZAxUz2Uks4Pt51-fVWVN9CcH9ckyY/edit?usp=sharing)\n\n\n### Introduction to Workflow orchestration\n\n* What is an Orchestration Pipeline?\n* What is a DAG?\n* [Video](https://www.youtube.com/watch?v=0yK7LXwYeD0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n\n### Setting up Airflow locally\n\n* Setting up Airflow with Docker-Compose\n* [Video](https://www.youtube.com/watch?v=lqDMzReAtrw&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n* More information in the [airflow folder](airflow)\n\nIf you want to run a lighter version of Airflow with fewer services, check this [video](https://www.youtube.com/watch?v=A1p5LQ0zzaQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb). It's optional.\n\n\n### Ingesting data to GCP with Airflow\n\n* Extraction: Download and unpack the data\n* Pre-processing: Convert this raw data to parquet\n* Upload the parquet files to GCS\n* Create an external table in BigQuery\n* [Video](https://www.youtube.com/watch?v=9ksX9REfL8w&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=19)\n\n### Ingesting data to Local Postgres with Airflow\n\n* Converting the ingestion script for loading data to Postgres to Airflow DAG\n* [Video](https://www.youtube.com/watch?v=s2U8MWJH5xA&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n\n### Transfer service (AWS -> GCP)\n\nMoving files from AWS to GCP.\n\nYou will need an AWS account for this. This section is optional\n\n* [Video 1](https://www.youtube.com/watch?v=rFOFTfD1uGk&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n* [Video 2](https://www.youtube.com/watch?v=VhmmbqpIzeI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n\n### Homework \n\nIn the homework, you'll create a few DAGs for processing the NY Taxi data for 2019-2021\n\nMore information [here](homework.md)\n\n\n## Community notes\n\nDid you take notes? You can share them here.\n\n* [Notes from Alvaro Navas](https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/2_data_ingestion.md)\n* [Notes from Aaron Wright](https://github.com/ABZ-Aaron/DataEngineerZoomCamp/blob/master/week_2_data_ingestion/README.md)\n* [Notes from Abd](https://itnadigital.notion.site/Week-2-Data-Ingestion-ec2d0d36c0664bc4b8be6a554b2765fd)\n* [Blog post by Isaac Kargar](https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html)\n* [Blog, notes, walkthroughs by Sandy Behrens](https://learningdataengineering540969211.wordpress.com/2022/01/30/week-2-de-zoomcamp-2-3-2-ingesting-data-to-gcp-with-airflow/)\n* [Notes from Apurva Hegde](https://github.com/apuhegde/Airflow-LocalExecutor-In-Docker#readme)\n* [Notes from Vincenzo Galante](https://binchentso.notion.site/Data-Talks-Club-Data-Engineering-Zoomcamp-8699af8e7ff94ec49e6f9bdec8eb69fd)\n* Add your notes here (above this line)",
    "filename": "cohorts/2022/week_2_data_ingestion/README.md"
  },
  {
    "content": "## Setup (Official)\n\n### Pre-Reqs\n\n1. For the sake of standardization across this workshop's config,\n    rename your gcp-service-accounts-credentials file to `google_credentials.json` & store it in your `$HOME` directory\n    ``` bash\n        cd ~ && mkdir -p ~/.google/credentials/\n        mv <path/to/your/service-account-authkeys>.json ~/.google/credentials/google_credentials.json\n    ```\n\n2. You may need to upgrade your docker-compose version to v2.x+, and set the memory for your Docker Engine to minimum 5GB\n(ideally 8GB). If enough memory is not allocated, it might lead to airflow-webserver continuously restarting.\n\n3. Python version: 3.7+\n\n\n### Airflow Setup\n\n1. Create a new sub-directory called `airflow` in your `project` dir (such as the one we're currently in)\n\n2. **Set the Airflow user**:\n\n    On Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. \n    Otherwise the files created in `dags`, `logs` and `plugins` will be created with root user. \n    You have to make sure to configure them for the docker-compose:\n\n    ```bash\n    mkdir -p ./dags ./logs ./plugins\n    echo -e \"AIRFLOW_UID=$(id -u)\" > .env\n    ```\n\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \n\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\n    this content:\n\n    ```\n    AIRFLOW_UID=50000\n    ```\n\n   \n3. **Import the official docker setup file** from the latest Airflow version:\n   ```shell\n   curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml'\n   ```\n   \n4. It could be overwhelming to see a lot of services in here. \n   But this is only a quick-start template, and as you proceed you'll figure out which unused services can be removed.\n   Eg. [Here's](docker-compose-nofrills.yml) a no-frills version of that template.\n\n5. **Docker Build**:\n\n    When you want to run Airflow locally, you might want to use an extended image, \n    containing some additional dependencies - for example you might add new python packages, \n    or upgrade airflow providers to a later version.\n    \n    Create a `Dockerfile` pointing to Airflow version you've just downloaded, \n    such as `apache/airflow:2.2.3`, as the base image,\n       \n    And customize this `Dockerfile` by:\n    * Adding your custom packages to be installed. The one we'll need the most is `gcloud` to connect with the GCS bucket/Data Lake.\n    * Also, integrating `requirements.txt` to install libraries via  `pip install`\n\n6. **Docker Compose**:\n\n    Back in your `docker-compose.yaml`:\n   * In `x-airflow-common`: \n     * Remove the `image` tag, to replace it with your `build` from your Dockerfile, as shown\n     * Mount your `google_credentials` in `volumes` section as read-only\n     * Set environment variables: `GCP_PROJECT_ID`, `GCP_GCS_BUCKET`, `GOOGLE_APPLICATION_CREDENTIALS` & `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT`, as per your config.\n   * Change `AIRFLOW__CORE__LOAD_EXAMPLES` to `false` (optional)\n\n7. Here's how the final versions of your [Dockerfile](./Dockerfile) and [docker-compose.yml](./docker-compose.yaml) should look.\n\n\n## Problems\n\n### `File /.google/credentials/google_credentials.json was not found`\n\nFirst, make sure you have your credentials in your `$HOME/.google/credentials`.\nMaybe you missed the step and didn't copy the your JSON with credentials there?\nAlso, make sure the file-name is `google_credentials.json`.\n\nSecond, check that docker-compose can correctly map this directory to airflow worker.\n\nExecute `docker ps` to see the list of docker containers running on your host machine and find the ID of the airflow worker.\n\nThen execute `bash` on this container:\n\n```bash\ndocker exec -it <container-ID> bash\n```\n\nNow check if the file with credentials is actually there:\n\n```bash\nls -lh /.google/credentials/\n```\n\nIf it's empty, docker-compose couldn't map the folder with credentials. \nIn this case, try changing it to the absolute path to this folder:\n\n```yaml\n  volumes:\n    - ./dags:/opt/airflow/dags\n    - ./logs:/opt/airflow/logs\n    - ./plugins:/opt/airflow/plugins\n    # here: ----------------------------\n    - c:/Users/alexe/.google/credentials/:/.google/credentials:ro\n    # -----------------------------------\n```",
    "filename": "cohorts/2022/week_2_data_ingestion/airflow/1_setup_official.md"
  },
  {
    "content": "## Setup (No-frills)\n\n### Pre-Reqs\n\n1. For the sake of standardization across this workshop's config,\n    rename your gcp-service-accounts-credentials file to `google_credentials.json` & store it in your `$HOME` directory\n    ``` bash\n        cd ~ && mkdir -p ~/.google/credentials/\n        mv <path/to/your/service-account-authkeys>.json ~/.google/credentials/google_credentials.json\n    ```\n\n2. You may need to upgrade your docker-compose version to v2.x+, and set the memory for your Docker Engine to minimum 4GB\n(ideally 8GB). If enough memory is not allocated, it might lead to airflow-webserver continuously restarting.\n\n3. Python version: 3.7+\n\n\n### Airflow Setup\n\n1. Create a new sub-directory called `airflow` in your `project` dir (such as the one we're currently in)\n   \n2. **Set the Airflow user**:\n\n    On Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. \n    Otherwise the files created in `dags`, `logs` and `plugins` will be created with root user. \n    You have to make sure to configure them for the docker-compose:\n\n    ```bash\n    mkdir -p ./dags ./logs ./plugins\n    echo -e \"AIRFLOW_UID=$(id -u)\" >> .env\n    ```\n\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \n\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\n    this content:\n\n    ```\n    AIRFLOW_UID=50000\n    ```\n\n3. **Docker Build**:\n\n    When you want to run Airflow locally, you might want to use an extended image, \n    containing some additional dependencies - for example you might add new python packages, \n    or upgrade airflow providers to a later version.\n    \n    Create a `Dockerfile` pointing to the latest Airflow version such as `apache/airflow:2.2.3`, for the base image,\n       \n    And customize this `Dockerfile` by:\n    * Adding your custom packages to be installed. The one we'll need the most is `gcloud` to connect with the GCS bucket (Data Lake).\n    * Also, integrating `requirements.txt` to install libraries via  `pip install`\n\n4. Copy [docker-compose-nofrills.yml](docker-compose-nofrills.yml), [.env_example](.env_example) & [entrypoint.sh](scripts/entrypoint.sh) from this repo.\n    The changes from the official setup are:\n    * Removal of `redis` queue, `worker`, `triggerer`, `flower` & `airflow-init` services, \n    and changing from `CeleryExecutor` (multi-node) mode to `LocalExecutor` (single-node) mode \n    * Inclusion of `.env` for better parametrization & flexibility\n    * Inclusion of simple `entrypoint.sh` to the `webserver` container, responsible to initialize the database and create login-user (admin).\n    * Updated `Dockerfile` to grant permissions on executing `scripts/entrypoint.sh`\n        \n5. `.env`:\n    * Rebuild your `.env` file by making a copy of `.env_example` (but make sure your `AIRFLOW_UID` remains):\n        ```shell\n        mv .env_example .env\n        ```\n    * Set environment variables `AIRFLOW_UID`, `GCP_PROJECT_ID` & `GCP_GCS_BUCKET`, as per your config.\n    * Optionally, if your `google-credentials.json` is stored somewhere else, such as a path like `$HOME/.gc`, \n    modify the env-vars (`GOOGLE_APPLICATION_CREDENTIALS`, `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT`) and `volumes` path in `docker-compose-nofrills.yml`\n\n6. Here's how the final versions of your [Dockerfile](./Dockerfile) and [docker-compose-nofrills](./docker-compose-nofrills.yml) should look.\n\n\n## Problems\n\n### `no-frills setup does not work for me - WSL/Windows user `\n\nIf you are running Docker in Windows/WSL/WSL2 and you have encountered some `ModuleNotFoundError` or low performance issues, take a look at this [Airflow & WSL2 gist](https://gist.github.com/nervuzz/d1afe81116cbfa3c834634ebce7f11c5) focused entirely on troubleshooting possible problems.\n\n### `File /.google/credentials/google_credentials.json was not found`\n\nFirst, make sure you have your credentials in your `$HOME/.google/credentials`.\nMaybe you missed the step and didn't copy the your JSON with credentials there?\nAlso, make sure the file-name is `google_credentials.json`.\n\nSecond, check that docker-compose can correctly map this directory to airflow worker.\n\nExecute `docker ps` to see the list of docker containers running on your host machine and find the ID of the airflow worker.\n\nThen execute `bash` on this container:\n\n```bash\ndocker exec -it <container-ID> bash\n```\n\nNow check if the file with credentials is actually there:\n\n```bash\nls -lh /.google/credentials/\n```\n\nIf it's empty, docker-compose couldn't map the folder with credentials. \nIn this case, try changing it to the absolute path to this folder:\n\n```yaml\n  volumes:\n    - ./dags:/opt/airflow/dags\n    - ./logs:/opt/airflow/logs\n    - ./plugins:/opt/airflow/plugins\n    # here: ----------------------------\n    - c:/Users/alexe/.google/credentials/:/.google/credentials:ro\n    # -----------------------------------\n```",
    "filename": "cohorts/2022/week_2_data_ingestion/airflow/2_setup_nofrills.md"
  },
  {
    "content": "### Concepts\n\n [Airflow Concepts and Architecture](docs/1_concepts.md)\n\n### Workflow\n\n ![](docs/gcs_ingestion_dag.png)\n \n### Setup - Official Version\n (For the section on the Custom/Lightweight setup, scroll down)\n\n #### Setup\n  [Airflow Setup with Docker, through official guidelines](1_setup_official.md)\n\n #### Execution\n \n  1. Build the image (only first-time, or when there's any change in the `Dockerfile`, takes ~15 mins for the first-time):\n     ```shell\n     docker-compose build\n     ```\n   \n     or (for legacy versions)\n   \n     ```shell\n     docker build .\n     ```\n\n 2. Initialize the Airflow scheduler, DB, and other config\n    ```shell\n    docker-compose up airflow-init\n    ```\n\n 3. Kick up the all the services from the container:\n    ```shell\n    docker-compose up\n    ```\n\n 4. In another terminal, run `docker-compose ps` to see which containers are up & running (there should be 7, matching with the services in your docker-compose file).\n\n 5. Login to Airflow web UI on `localhost:8080` with default creds: `airflow/airflow`\n\n 6. Run your DAG on the Web Console.\n\n 7. On finishing your run or to shut down the container/s:\n    ```shell\n    docker-compose down\n    ```\n\n    To stop and delete containers, delete volumes with database data, and download images, run:\n    ```\n    docker-compose down --volumes --rmi all\n    ```\n\n    or\n    ```\n    docker-compose down --volumes --remove-orphans\n    ```\n       \n### Setup - Custom No-Frills Version (Lightweight)\nThis is a quick, simple & less memory-intensive setup of Airflow that works on a LocalExecutor.\n\n  #### Setup\n  [Airflow Setup with Docker, customized](2_setup_nofrills.md)\n\n  #### Execution\n  \n  1. Stop and delete containers, delete volumes with database data, & downloaded images (from the previous setup):\n    ```\n    docker-compose down --volumes --rmi all\n    ```\n\n   or\n    ```\n    docker-compose down --volumes --remove-orphans\n    ```\n    \n   Or, if you need to clear your system of any pre-cached Docker issues:\n    ```\n    docker system prune\n    ```\n    \n   Also, empty the airflow `logs` directory.\n    \n  2. Build the image (only first-time, or when there's any change in the `Dockerfile`):\n  Takes ~5-10 mins for the first-time\n    ```shell\n    docker-compose build\n    ```\n    or (for legacy versions)\n    ```shell\n    docker build .\n    ```\n\n  3. Kick up the all the services from the container (no need to specially initialize):\n    ```shell\n    docker-compose -f docker-compose-nofrills.yml up\n    ```\n\n  4. In another terminal, run `docker ps` to see which containers are up & running (there should be 3, matching with the services in your docker-compose file).\n\n  5. Login to Airflow web UI on `localhost:8080` with creds: `admin/admin` (explicit creation of admin user was required)\n\n  6. Run your DAG on the Web Console.\n\n  7. On finishing your run or to shut down the container/s:\n    ```shell\n    docker-compose down\n    ```\n    \n### Setup - Taken from DE Zoomcamp 2.3.4 - Optional: Lightweight Local Setup for Airflow\n\nUse the docker-compose_2.3.4.yaml file (and rename it to docker-compose.yaml). Don't forget to replace the variables `GCP_PROJECT_ID` and `GCP_GCS_BUCKET`.\n\n### Future Enhancements\n* Deploy self-hosted Airflow setup on Kubernetes cluster, or use a Managed Airflow (Cloud Composer) service by GCP\n\n### References\nFor more info, check out these official docs:\n   * https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html\n   * https://airflow.apache.org/docs/docker-stack/build.html\n   * https://airflow.apache.org/docs/docker-stack/recipes.html",
    "filename": "cohorts/2022/week_2_data_ingestion/airflow/README.md"
  },
  {
    "code": false,
    "content": "# Data Ingestion DAG Documentation\n\n## Overview\nThis script defines an Apache Airflow Directed Acyclic Graph (DAG) for ingesting and processing taxi trip data from NYC. The process includes downloading a CSV file, converting it to Parquet format, uploading it to Google Cloud Storage (GCS), and creating an external table in Google BigQuery to query the data. The script leverages various components from the Airflow framework, the Google Cloud Platform, and the PyArrow library.\n\n## Environment Variables\nThe script retrieves configuration values from environment variables:\n- **GCP_PROJECT_ID**: Google Cloud Project ID where resources are managed.\n- **GCP_GCS_BUCKET**: The name of the GCS bucket used for storing the uploaded files.\n- **AIRFLOW_HOME**: Local directory for storing intermediate files (default is `/opt/airflow/`).\n- **BIGQUERY_DATASET**: The BigQuery dataset where the external table will be created (default is `trips_data_all`).\n\n## Constants\nSeveral constants are defined at the beginning of the script:\n- **dataset_file**: Name of the dataset file to be downloaded (`yellow_tripdata_2021-01.csv`).\n- **dataset_url**: URL from which to download the dataset.\n- **parquet_file**: The name of the output file in Parquet format, derived from the CSV file name.\n\n## Function Definitions\n\n### `format_to_parquet(src_file)`\nThis function takes a CSV file located at `src_file` and converts it to Parquet format using the PyArrow library. The function checks if the input file ends with `.csv` and logs an error if not. Upon successful conversion, a Parquet file is created in the same directory as the CSV file.\n\n### `upload_to_gcs(bucket, object_name, local_file)`\nThis function uploads the specified local file to a Google Cloud Storage bucket. It accepts:\n- `bucket`: Name of the target GCS bucket.\n- `object_name`: Path and filename for the uploaded object in GCS.\n- `local_file`: Local path of the file to be uploaded.\n\nTo handle larger files efficiently, the function implements a workaround to adjust the multipart upload size. It creates a Cloud Storage client and uploads the specified file to the given bucket.\n\n## Airflow DAG Configuration\nThe DAG is configured with default arguments that specify its owner, start date, and retry behavior. These settings control the execution and fault tolerance of the tasks within the DAG.\n\n### DAG Declaration\nThe DAG is defined with the identifier `data_ingestion_gcs_dag` and is scheduled to run daily. The `catchup` flag is set to `False`, meaning that missed runs are not backfilled. Additionally, the maximum number of active runs is constrained to 1.\n\n## Task Definitions\n\n### 1. `download_dataset_task`\nThis task is a Bash operator that uses the `curl` command to download the dataset from the specified URL and saves it to the local Airflow home directory.\n\n### 2. `format_to_parquet_task`\nThis task is implemented using the PythonOperator. It invokes the `format_to_parquet` function to convert the downloaded CSV file to Parquet format. The source file path is passed through the task's operator keyword arguments.\n\n### 3. `local_to_gcs_task`\nAnother PythonOperator that invokes the `upload_to_gcs` function. It uploads the newly created Parquet file to the specified GCS bucket using the defined parameters.\n\n### 4. `bigquery_external_table_task`\nThis task utilizes the `BigQueryCreateExternalTableOperator` to create an external table in BigQuery that points to the Parquet file stored in GCS. It defines the external table's configuration, including the source format and URI for the data.\n\n## Task Dependencies\nThe dependencies between tasks are defined using the `>>` operator:\n- `download_dataset_task` must complete before `format_to_parquet_task`.\n- `format_to_parquet_task` must complete before `local_to_gcs_task`.\n- `local_to_gcs_task` must complete before `bigquery_external_table_task`.\n\nThis ensures that the tasks execute in the correct order, adhering to the data pipeline flow. \n\n## Conclusion\nThe provided Airflow DAG effectively automates the process of downloading NYC taxi trip data, processing it, and making it available for analysis in Google BigQuery. This script serves as a template for ETL processes involving data ingestion from external sources and demonstrates the use of Airflow's capabilities to manage complex workflows.",
    "filename": "cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py"
  },
  {
    "code": false,
    "content": "# Documentation for LocalIngestionDag\n\n## Overview\nThis script defines an Apache Airflow Directed Acyclic Graph (DAG) named `LocalIngestionDag`, which is designed to automate the process of downloading CSV files from a specified URL (related to NYC taxi trip data) and ingesting the data into a PostgreSQL database. The workflow is scheduled to run at 6 AM on the 2nd day of every month, starting from January 1, 2021.\n\n## Environment Variables\nThe code retrieves several environment variables required for database connectivity:\n- `PG_HOST`: The hostname of the PostgreSQL database server.\n- `PG_USER`: The username for database access.\n- `PG_PASSWORD`: The password for the specified user.\n- `PG_PORT`: The port on which the PostgreSQL server is running.\n- `PG_DATABASE`: The name of the database where data will be ingested.\n\nThe `AIRFLOW_HOME` variable is also set, which points to the base directory where Airflow stores its files and logs. If the `AIRFLOW_HOME` environment variable is not set, it defaults to `/opt/airflow/`.\n\n## DAG Definition\nThe DAG is instantiated using the `DAG` class from Airflow, with the following parameters:\n- **DAG ID**: \"LocalIngestionDag\"\n- **Schedule Interval**: \"0 6 2 * *\" (At 06:00 AM on the 2nd of every month)\n- **Start Date**: January 1, 2021\n\nThe `local_workflow` serves as the container for the defined tasks and manages the execution flow.\n\n## URLs and File Management\nThe script constructs several dynamic paths based on the execution date:\n- **URL_PREFIX**: Base URL for fetching the CSV files from AWS S3.\n- **URL_TEMPLATE**: A formatted string that generates the URL for the desired monthly CSV file.\n- **OUTPUT_FILE_TEMPLATE**: Template for the path where the downloaded file will be saved locally.\n- **TABLE_NAME_TEMPLATE**: Format for naming the database table to which the data will be ingested.\n\nThese templates utilize Jinja templating, allowing for dynamic file and table naming based on the execution date.\n\n## Tasks within the DAG\nThe DAG consists of two tasks defined within a context manager (`with local_workflow:`), ensuring that they are part of the same workflow.\n\n### Task 1: File Download with BashOperator\n- **Task ID**: `wget_task`\n- **Operator**: `BashOperator`\n- **Command**: Uses `curl` to download the CSV file from the constructed URL specified by `URL_TEMPLATE` and saves it to the path given by `OUTPUT_FILE_TEMPLATE`.\n\nThis task is responsible for retrieving the data file from the external source and ensuring it's available for the ingestion process that follows.\n\n```python\nwget_task = BashOperator(\n    task_id='wget',\n    bash_command=f'curl -sSL {URL_TEMPLATE} > {OUTPUT_FILE_TEMPLATE}'\n)\n```\n\n### Task 2: Data Ingestion with PythonOperator\n- **Task ID**: `ingest_task`\n- **Operator**: `PythonOperator`\n- **Callable**: Uses a callable named `ingest_callable` from an external module named `ingest_script`.\n\nThe `ingest_task` is configured with necessary arguments (PostgreSQL credentials, target table name, and the downloaded CSV file) that will be passed to the `ingest_callable`. This function is expected to handle the logic for ingesting the CSV data into the specified PostgreSQL table.\n\n```python\ningest_task = PythonOperator(\n    task_id=\"ingest\",\n    python_callable=ingest_callable,\n    op_kwargs=dict(\n        user=PG_USER,\n        password=PG_PASSWORD,\n        host=PG_HOST,\n        port=PG_PORT,\n        db=PG_DATABASE,\n        table_name=TABLE_NAME_TEMPLATE,\n        csv_file=OUTPUT_FILE_TEMPLATE\n    ),\n)\n```\n\n## Task Dependencies\nThe two tasks are linked using the bitwise operator (`>>`), ensuring that the `ingest_task` runs only after the successful completion of the `wget_task`. This dependency guarantees the ingestion process runs with valid and available data.\n\n```python\nwget_task >> ingest_task\n```\n\n## Conclusion\nIn summary, the `LocalIngestionDag` automates the task of downloading NYC taxi trip data and ingesting it into a PostgreSQL database on a monthly schedule. By leveraging Apache Airflow, this workflow enhances data processing efficiency and reliability, following best practices in ETL (Extract, Transform, Load) operations.",
    "filename": "cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Data Ingestion Script\n\n## Overview\n\nThis script is designed for ingesting CSV data into a PostgreSQL database. It leverages the Pandas library for data handling and SQLAlchemy for database interactions. The primary function of this script, `ingest_callable`, reads a CSV file in chunks and uploads the data to a specified table in the database, ensuring that the datetime columns are properly converted. \n\n## Dependencies\n\nThe script imports several key libraries:\n- **os**: Provides a way to interact with the operating system, though it's not explicitly used within the provided code.\n- **time**: Utilizes time functions, specifically for measuring execution duration.\n- **pandas**: A popular data manipulation library that is used here for reading CSV files and working with DataFrames.\n- **sqlalchemy**: A SQL toolkit and Object-Relational Mapping (ORM) library for Python, which facilitates database connections and operations.\n\n## Function: `ingest_callable`\n\nThe main function defined in the script is `ingest_callable`, which orchestrates the data ingestion process. Below are its parameters and a detailed description of its function.\n\n### Parameters\n- `user`: The username for database authentication.\n- `password`: The password for database authentication.\n- `host`: The database server hostname or IP address.\n- `port`: The port number on which the database server is listening.\n- `db`: The name of the database to connect to.\n- `table_name`: The name of the table into which data will be inserted.\n- `csv_file`: The path to the CSV file containing data to be ingested.\n- `execution_date`: A timestamp when the ingestion is performed (only printed for informational purposes).\n\n### Function Workflow\n\n1. **Connection Setup**: \n   - The function starts by printing the table name, CSV file name, and the execution date for logging purposes.\n   - It establishes a connection to the PostgreSQL database using SQLAlchemy's `create_engine` method.\n\n2. **Initial Data Insertion**:\n   - The script indicates that the connection has been established successfully.\n   - It initializes a timer to measure the duration of operations.\n   - The CSV file is read in chunks of 100,000 rows, which helps in managing memory usage effectively.\n   - The first chunk of data is read into a DataFrame (`df`) using the `next()` function.\n\n3. **Datetime Conversion**:\n   - The function converts the `tpep_pickup_datetime` and `tpep_dropoff_datetime` columns in the DataFrame to datetime objects to ensure correct data types before insertion into the database.\n\n4. **Database Table Creation**:\n   - It uses the `to_sql` method to create the target table in the database by replacing any existing table with the same name. This is done using the `if_exists='replace'` parameter. \n\n5. **Appending Initial Chunk**:\n   - The first chunk of data is appended to the newly created table, and the time taken for this operation is logged.\n\n6. **Iterative Chunk Processing**:\n   - A `while` loop is used to process subsequent chunks of data:\n     - A timer is started to measure the time taken for each chunk insertion.\n     - The script continues to read from the CSV until it reaches the end. The `StopIteration` exception signals the end of the file.\n     - Each new chunk undergoes the same datetime conversion as the first one and is then appended to the database table.\n\n7. **Logging Chunk Insertion**:\n   - After each chunk insertion, the time taken for the operation is logged to the console.\n\n8. **Completion Notification**:\n   - Once all chunks are processed, a completion message is printed to notify the user that the data ingestion has been finished.\n\n## Conclusion\n\nThis procedure is efficient for large datasets since it uses chunking to read the file, minimizing memory requirements and allowing for effective database writing. The script ensures that datetime columns are appropriately formatted, which is critical for time-based analyses in SQL databases. The clear logging of operations facilitates monitoring of the data ingestion process.",
    "filename": "cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py"
  },
  {
    "content": "## Airflow concepts\n\n\n### Airflow architecture\n![](arch-diag-airflow.png)\n\nRef: https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html\n\n* **Web server**:\nGUI to inspect, trigger and debug the behaviour of DAGs and tasks. \nAvailable at http://localhost:8080.\n\n* **Scheduler**:\nResponsible for scheduling jobs. Handles both triggering & scheduled workflows, submits Tasks to the executor to run, monitors all tasks and DAGs, and\nthen triggers the task instances once their dependencies are complete.\n\n* **Worker**:\nThis component executes the tasks given by the scheduler.\n\n* **Metadata database (postgres)**:\nBackend to the Airflow environment. Used by the scheduler, executor and webserver to store state.\n\n* **Other components** (seen in docker-compose services):\n    * `redis`: Message broker that forwards messages from scheduler to worker.\n    * `flower`: The flower app for monitoring the environment. It is available at http://localhost:5555.\n    * `airflow-init`: initialization service (customized as per this design)\n\nAll these services allow you to run Airflow with CeleryExecutor. \nFor more information, see [Architecture Overview](https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html).\n\n\n### Project Structure:\n\n* `./dags` - `DAG_FOLDER` for DAG files (use `./dags_local` for the local ingestion DAG)\n* `./logs` - contains logs from task execution and scheduler.\n* `./plugins` - for custom plugins\n\n\n### Workflow components\n\n* `DAG`: Directed acyclic graph, specifies the dependencies between a set of tasks with explicit execution order, and has a beginning as well as an end. (Hence, \u201cacyclic\u201d)\n    * `DAG Structure`: DAG Definition, Tasks (eg. Operators), Task Dependencies (control flow: `>>` or `<<` )\n    \n* `Task`: a defined unit of work (aka, operators in Airflow). The Tasks themselves describe what to do, be it fetching data, running analysis, triggering other systems, or more.\n    * Common Types: Operators (used in this workshop), Sensors, TaskFlow decorators\n    * Sub-classes of Airflow's BaseOperator\n\n* `DAG Run`: individual execution/run of a DAG\n    * scheduled or triggered\n\n* `Task Instance`: an individual run of a single task. Task instances also have an indicative state, which could be \u201crunning\u201d, \u201csuccess\u201d, \u201cfailed\u201d, \u201cskipped\u201d, \u201cup for retry\u201d, etc.\n    * Ideally, a task should flow from `none`, to `scheduled`, to `queued`, to `running`, and finally to `success`.\n\n\n### References\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/concepts/tasks.html",
    "filename": "cohorts/2022/week_2_data_ingestion/airflow/docs/1_concepts.md"
  },
  {
    "code": false,
    "content": "# Data Ingestion DAG for Google Cloud Storage\n\nThis document provides a high-level description of a DAG (Directed Acyclic Graph) implemented using Apache Airflow for ingesting a dataset containing yellow taxi trip data from New York City into Google Cloud Storage (GCS). The DAG includes tasks for downloading the dataset from a remote URL and uploading it to a specified GCS bucket.\n\n## Overview\n\nThe DAG is designed to run daily (`@daily`), ensuring that it fetches the most up-to-date dataset each day. It orchestrates two main tasks:\n\n1. **Downloading the Dataset**: Retrieves a CSV file containing trip data.\n2. **Uploading to Google Cloud Storage**: Transfers the downloaded file to a specified GCS bucket.\n\nThe workflow is structured to ensure that the upload task only begins after the download task completes successfully. This is indicated by the task dependency defined at the end of the DAG.\n\n## Environment Configuration\n\nThe script starts by importing necessary modules, including:\n\n- `os`: For accessing environment variables.\n- `datetime`: To handle date-related operations.\n- `airflow`: The core libraries for creating and managing the DAG.\n- `google.cloud.storage`: For interacting with Google Cloud Storage.\n\nSeveral environment variables are retrieved, allowing for configuration flexibility without hardcoding values. These include:\n\n- `GCP_PROJECT_ID`: The Google Cloud project ID (default value set).\n- `GCP_GCS_BUCKET`: The name of the GCS bucket for uploading the data.\n- `AIRFLOW_HOME`: The directory for Airflow home settings.\n\nA URL for the dataset and a local file path for downloads are constructed using these environment variables.\n\n## DAG Definition\n\nA `DAG` instance is created with specific configurations:\n\n- **dag_id**: Unique identifier for the DAG (`data_ingestion_gcs_dag`).\n- **schedule_interval**: Set to run daily.\n- **default_args**: Includes common parameters like the owner, start date, and retry settings.\n- **catchup**: Allows the DAG to run for past dates if it was inactive.\n- **max_active_runs**: Limits concurrent executions of the DAG to one.\n\n## Task 1: Downloading the Dataset\n\n### `download_dataset_task`\n\nThe first task in the DAG is defined using the `BashOperator`. This operator executes a shell command to download the dataset. \n\n- **task_id**: `download_dataset_task`\n- **bash_command**: Utilizes `curl` to silently fetch the dataset from a specified URL and saves it locally.\n\nThe task execution time is estimated to be around 2 minutes, depending on the download speed of the executing machine.\n\n## Task 2: Uploading to Google Cloud Storage\n\n### `upload_to_gcs_task`\n\nThe second task is also defined using the `BashOperator`. This command uploads the previously downloaded dataset to GCS while handling authentication:\n\n- **task_id**: `upload_to_gcs_task`\n- **bash_command**: \n  - Activates a Google Cloud service account using a key file located at `path_to_creds`.\n  - Uses `gsutil` to copy the local CSV file to the specified GCS bucket (`BUCKET`).\n\nThe upload task is anticipated to take approximately 20 minutes, but this duration varies based on the internet upload speed.\n\n## Task Dependencies\n\nThe workflow is structured such that the `upload_to_gcs_task` depends on the successful completion of the `download_dataset_task`. This is expressed using the `>>` operator, ensuring that data is uploaded only after it has been downloaded.\n\n## Comments and Considerations\n\n1. **Upload Performance**: The script includes commentary regarding transfer speeds, specifically noting that upload speeds can affect execution time. Suggested use of Spark for distributed processing for larger-scale data tasks indicates a thoughtful approach to performance.\n\n2. **Alternative Upload Approaches**: The commented-out `upload_to_gcs` Python function presents an alternative method for uploading data to GCS using the Google Cloud Storage client, showcasing flexibility in implementation. Though this method is not used, it indicates scalability considerations.\n\n3. **Future Enhancements**: The script has room for enhancements such as error handling, logging, or notifications upon task completion or failure, which are essential for production-grade ETL processes.\n\nThis documentation summarizes the functionality and design of the Airflow DAG, offering insights into its purpose and structure. The code is tailored for ingesting and storing significant datasets in a cloud environment efficiently.",
    "filename": "cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py"
  },
  {
    "content": "## Week 2 Homework\n\nIn this homework, we'll prepare data for the next week. We'll need\nto put these datasets to our data lake:\n\n* For the lessons, we'll need the Yellow taxi dataset (years 2019 and 2020)\n* For the homework, we'll need FHV Data (for-hire vehicles, for 2019 only)\n\nYou can find all the URLs on [the dataset page](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n\n\nIn this homework, we will:\n\n* Modify the DAG we created during the lessons for transferring the yellow taxi data\n* Create a new dag for transferring the FHV data\n* Create another dag for the Zones data\n\n\nIf you don't have access to GCP, you can do that locally and ingest data to Postgres \ninstead. If you have access to GCP, you don't need to do it for local Postgres -\nonly if you want.\n\nAlso note that for this homework we don't need the last step - creating a table in GCP.\nAfter putting all the files to the datalake, we'll create the tables in Week 3.\n\n\n\n## Question 1: Start date for the Yellow taxi data (1 point)\n\nYou'll need to parametrize the DAG for processing the yellow taxi data that\nwe created in the videos. \n\nWhat should be the start date for this dag?\n\n* 2019-01-01\n* 2020-01-01\n* 2021-01-01\n* days_ago(1)\n\n\n## Question 2: Frequency for the Yellow taxi data (1 point)\n\nHow often do we need to run this DAG?\n\n* Daily\n* Monthly\n* Yearly\n* Once\n\n\n## Re-running the DAGs for past dates\n\nTo execute your DAG for past dates, try this:\n\n* First, delete your DAG from the web interface (the bin icon)\n* Set the `catchup` parameter to `True`\n* Be careful with running a lot of jobs in parallel - your system may not like it. Don't set it higher than 3: `max_active_runs=3`\n* Rename the DAG to something like `data_ingestion_gcs_dag_v02` \n* Execute it from the Airflow GUI (the play button)\n\n\nAlso, there's no data for the recent months, but `curl` will exit successfully.\nTo make it fail on 404, add the `-f` flag:\n\n```bash\ncurl -sSLf { URL } > { LOCAL_PATH }\n```\n\nWhen you run this for all the data, the temporary files will be saved in Docker and will consume your \ndisk space. If it causes problems for you, add another step in your DAG that cleans everything up.\nIt could be a bash operator that runs this command:\n\n```bash\nrm name-of-csv-file.csv name-of-parquet-file.parquet\n```\n\n\n## Question 3: DAG for FHV Data (2 points)\n\nNow create another DAG - for uploading the FHV data. \n\nWe will need three steps: \n\n* Download the data\n* Parquetize it \n* Upload to GCS\n\nIf you don't have a GCP account, for local ingestion you'll need two steps:\n\n* Download the data\n* Ingest to Postgres\n\nUse the same frequency and the start date as for the yellow taxi dataset\n\nQuestion: how many DAG runs are green for data in 2019 after finishing everything? \n\nNote: when processing the data for 2020-01 you probably will get an error. It's up \nto you to decide what to do with it - for Week 3 homework we won't need 2020 data.\n\n\n## Question 4: DAG for Zones (2 points)\n\n\nCreate the final DAG - for Zones:\n\n* Download it\n* Parquetize \n* Upload to GCS\n\n(Or two steps for local ingestion: download -> ingest to postgres)\n\nHow often does it need to run?\n\n* Daily\n* Monthly\n* Yearly\n* Once\n\n\n## Submitting the solutions\n\n* Form for submitting: https://forms.gle/ViWS8pDf2tZD4zSu5\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: February 7, 17:00 CET",
    "filename": "cohorts/2022/week_2_data_ingestion/homework/homework.md"
  },
  {
    "code": false,
    "content": "# Overview\n\nThis Python script utilizes Apache Airflow to define multiple Directed Acyclic Graphs (DAGs) aimed at downloading, converting, and uploading taxi-related data. The data is sourced from the NYC Taxi & Limousine Commission (TLC) and is initially in CSV format, which is then converted to Parquet format before being uploaded to Google Cloud Storage (GCS).\n\nThe script defines several DAGs with distinct scheduling intervals, each dedicated to handling different types of taxi data, including Yellow, Green, FHV (For-Hire Vehicle), and Zone lookup data.\n\n## Environment Variables and Imports\n\nThe script begins by importing necessary libraries:\n- **OS**: For environment variable handling.\n- **Logging**: For logging messages.\n- **Datetime**: To work with dates for scheduling tasks.\n- **Airflow Libraries**: Includes `DAG`, operators like `BashOperator` and `PythonOperator` for task execution.\n- **Google Cloud Storage**: For uploading files to GCS.\n- **Pyarrow**: For converting between CSV and Parquet file formats.\n\nIt retrieves the following environment variables:\n- `GCP_PROJECT_ID`: Google Cloud project identifier.\n- `GCP_GCS_BUCKET`: The GCS bucket where the data will be uploaded.\n- `AIRFLOW_HOME`: The path where intermediate CSV and Parquet files will be stored.\n\n## Function Definitions\n\n### `format_to_parquet`\n\nThis function takes two arguments:\n- `src_file`: The path to the source CSV file.\n- `dest_file`: The path to the output Parquet file.\n\nIts role is to convert the specified CSV file into Parquet format using `pyarrow` functions. If the input file is not a CSV, it logs an error message and exits the function.\n\n### `upload_to_gcs`\n\nThis function uploads a specified local file to GCS. It takes three parameters:\n- `bucket`: The name of the GCS bucket.\n- `object_name`: The name of the object (file) to be created in GCS.\n- `local_file`: The local file path to be uploaded.\n\nA GCS client is created, and the local file is uploaded to the specified bucket and object name.\n\n## Default Arguments\n\nA dictionary called `default_args` is defined to standardize settings for the DAGs. Key elements include:\n- `\"owner\"`: Identifies the owner of the DAG as \"airflow\".\n- `\"depends_on_past\"`: Set to `False`, which means tasks do not depend on the success of past runs.\n- `\"retries\"`: Configured to retry a task once in case of failure.\n\n## DAG Creation Function: `donwload_parquetize_upload_dag`\n\nThis function is responsible for defining the workflow of tasks for each DAG. It takes five parameters, including the DAG object and templates for URLs, local paths, and GCS paths.\n\nInside the function:\n1. A task is defined to download the CSV file using `BashOperator` with a `curl` command.\n2. The CSV file is then converted to Parquet format with the `format_to_parquet` function via a `PythonOperator`.\n3. The converted file is uploaded to GCS using another `PythonOperator`.\n4. Finally, a task is set to remove both the local CSV and Parquet files after the upload is complete.\n\nThe tasks are scheduled in sequence, ensuring that each step only executes after the previous one is done.\n\n## DAG Instances\n\nThe script instantiates four DAGs, each targeting different datasets:\n\n### Yellow Taxi Data DAG\n\n- **DAG ID**: `yellow_taxi_data_v2`\n- **Schedule**: Executes on the 2nd of each month at 6:00 AM.\n- **File Template Definitions**: Includes URL templates for the Yellow Taxi dataset and corresponding local and GCS paths.\n\n### Green Taxi Data DAG\n\n- **DAG ID**: `green_taxi_data_v1`\n- **Schedule**: Executes on the 2nd of each month at 7:00 AM.\n- **File Template Definitions**: Similar to the Yellow Taxi DAG, but for the Green Taxi dataset.\n\n### FHV Taxi Data DAG\n\n- **DAG ID**: `hfv_taxi_data_v1`\n- **Schedule**: Executes on the 2nd of each month at 8:00 AM.\n- **End Date**: The execution is limited to a maximum date of January 1, 2020.\n- **File Template Definitions**: Handles data specifically for FHV trips.\n\n### Zones Data DAG\n\n- **DAG ID**: `zones_data_v1`\n- **Schedule**: Executes just once, defined with `\"@once\"`.\n- **File Template Definitions**: Fetches taxi zone lookup data.\n\n## Conclusion\n\nThis script streamlines the pipeline of downloading, converting, and uploading taxi datasets to GCS, specified for different categories: Yellow, Green, FHV, and Taxi Zone Data. The use of Airflow enables a streamlined orchestration of these tasks, managing dependencies and scheduling effectively, thus providing an efficient way to handle large data sets in cloud environments.",
    "filename": "cohorts/2022/week_2_data_ingestion/homework/solution.py"
  },
  {
    "content": "## Generate AWS Access key\n- Login in to AWS account  \n- Search for IAM\n  ![aws iam](../../images/aws/iam.png)\n- Click on `Manage access key`\n- Click on `Create New Access Key`\n- Download the csv, your access key and secret would be in that csv (Please note that once lost secret cannot be recovered)\n\n## Transfer service\nhttps://console.cloud.google.com/transfer/cloud/jobs",
    "filename": "cohorts/2022/week_2_data_ingestion/transfer_service/README.md"
  },
  {
    "content": "## Setup (Official)\n\n### Pre-Reqs\n\n1. For the sake of standardization across this workshop's config,\n    rename your gcp-service-accounts-credentials file to `google_credentials.json` & store it in your `$HOME` directory\n    ``` bash\n        cd ~ && mkdir -p ~/.google/credentials/\n        mv <path/to/your/service-account-authkeys>.json ~/.google/credentials/google_credentials.json\n    ```\n\n2. You may need to upgrade your docker-compose version to v2.x+, and set the memory for your Docker Engine to minimum 5GB\n(ideally 8GB). If enough memory is not allocated, it might lead to airflow-webserver continuously restarting.\n\n3. Python version: 3.7+\n\n\n### Airflow Setup\n\n1. Create a new sub-directory called `airflow` in your `project` dir (such as the one we're currently in)\n\n2. **Set the Airflow user**:\n\n    On Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. \n    Otherwise the files created in `dags`, `logs` and `plugins` will be created with root user. \n    You have to make sure to configure them for the docker-compose:\n\n    ```bash\n    mkdir -p ./dags ./logs ./plugins\n    echo -e \"AIRFLOW_UID=$(id -u)\" > .env\n    ```\n\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \n\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\n    this content:\n\n    ```\n    AIRFLOW_UID=50000\n    ```\n\n3. **Import the official docker setup file** from the latest Airflow version:\n   ```shell\n   curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml'\n   ```\n   \n4. It could be overwhelming to see a lot of services in here. \n   But this is only a quick-start template, and as you proceed you'll figure out which unused services can be removed.\n   Eg. [Here's](docker-compose-nofrills.yml) a no-frills version of that template.\n\n\n5. **Docker Build**:\n\n    When you want to run Airflow locally, you might want to use an extended image, \n    containing some additional dependencies - for example you might add new python packages, \n    or upgrade airflow providers to a later version.\n    \n    Create a `Dockerfile` pointing to Airflow version you've just downloaded, \n    such as `apache/airflow:2.2.3`, as the base image,\n       \n    And customize this `Dockerfile` by:\n    * Adding your custom packages to be installed. The one we'll need the most is `gcloud` to connect with the GCS bucket/Data Lake.\n    * Also, integrating `requirements.txt` to install libraries via  `pip install`\n\n\n6. **Docker Compose**:\n\n    Back in your `docker-compose.yaml`:\n   * In `x-airflow-common`: \n     * Remove the `image` tag, to replace it with your `build` from your Dockerfile, as shown\n     * Mount your `google_credentials` in `volumes` section as read-only\n     * Set environment variables: `GCP_PROJECT_ID`, `GCP_GCS_BUCKET`, `GOOGLE_APPLICATION_CREDENTIALS` & `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT`, as per your config.\n\n   * Change `AIRFLOW__CORE__LOAD_EXAMPLES` to `false` (optional)\n\n7. Here's how the final versions of your [Dockerfile](./Dockerfile) and [docker-compose.yml](./docker-compose.yaml) should look.\n\n\n## Problems\n\n### `File /.google/credentials/google_credentials.json was not found`\n\nFirst, make sure you have your credentials in your `$HOME/.google/credentials`.\nMaybe you missed the step and didn't copy the your JSON with credentials there?\nAlso, make sure the file-name is `google_credentials.json`.\n\nSecond, check that docker-compose can correctly map this directory to airflow worker.\n\nExecute `docker ps` to see the list of docker containers running on your host machine and find the ID of the airflow worker.\n\nThen execute `bash` on this container:\n\n```bash\ndocker exec -it <container-ID> bash\n```\n\nNow check if the file with credentials is actually there:\n\n```bash\nls -lh /.google/credentials/\n```\n\nIf it's empty, docker-compose couldn't map the folder with credentials. \nIn this case, try changing it to the absolute path to this folder:\n\n```yaml\n  volumes:\n    - ./dags:/opt/airflow/dags\n    - ./logs:/opt/airflow/logs\n    - ./plugins:/opt/airflow/plugins\n    # here: ----------------------------\n    - c:/Users/alexe/.google/credentials/:/.google/credentials:ro\n    # -----------------------------------\n```",
    "filename": "cohorts/2022/week_3_data_warehouse/airflow/1_setup_official.md"
  },
  {
    "content": "## Setup (No-frills)\n\n### Pre-Reqs\n\n1. For the sake of standardization across this workshop's config,\n    rename your gcp-service-accounts-credentials file to `google_credentials.json` & store it in your `$HOME` directory\n    ``` bash\n        cd ~ && mkdir -p ~/.google/credentials/\n        mv <path/to/your/service-account-authkeys>.json ~/.google/credentials/google_credentials.json\n    ```\n\n2. You may need to upgrade your docker-compose version to v2.x+, and set the memory for your Docker Engine to minimum 4GB\n(ideally 8GB). If enough memory is not allocated, it might lead to airflow-webserver continuously restarting.\n\n3. Python version: 3.7+\n\n\n### Airflow Setup\n\n1. Create a new sub-directory called `airflow` in your `project` dir (such as the one we're currently in)\n   \n2. **Set the Airflow user**:\n\n    On Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. \n    Otherwise the files created in `dags`, `logs` and `plugins` will be created with root user. \n    You have to make sure to configure them for the docker-compose:\n\n    ```bash\n    mkdir -p ./dags ./logs ./plugins\n    echo -e \"AIRFLOW_UID=$(id -u)\" >> .env\n    ```\n\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \n\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\n    this content:\n\n    ```\n    AIRFLOW_UID=50000\n    ```\n\n3. **Docker Build**:\n\n    When you want to run Airflow locally, you might want to use an extended image, \n    containing some additional dependencies - for example you might add new python packages, \n    or upgrade airflow providers to a later version.\n    \n    Create a `Dockerfile` pointing to the latest Airflow version such as `apache/airflow:2.2.3`, for the base image,\n       \n    And customize this `Dockerfile` by:\n    * Adding your custom packages to be installed. The one we'll need the most is `gcloud` to connect with the GCS bucket (Data Lake).\n    * Also, integrating `requirements.txt` to install libraries via  `pip install`\n\n4. Copy [docker-compose-nofrills.yml](docker-compose-nofrills.yml), [.env_example](.env_example) & [entrypoint.sh](scripts/entrypoint.sh) from this repo.\n    The changes from the official setup are:\n    * Removal of `redis` queue, `worker`, `triggerer`, `flower` & `airflow-init` services, \n    and changing from `CeleryExecutor` (multi-node) mode to `LocalExecutor` (single-node) mode \n    * Inclusion of `.env` for better parametrization & flexibility\n    * Inclusion of simple `entrypoint.sh` to the `webserver` container, responsible to initialize the database and create login-user (admin).\n    * Updated `Dockerfile` to grant permissions on executing `scripts/entrypoint.sh`\n        \n5. `.env`:\n    * Rebuild your `.env` file by making a copy of `.env_example` (but make sure your `AIRFLOW_UID` remains):\n        ```shell\n        mv .env_example .env\n        ```\n    * Set environment variables `AIRFLOW_UID`, `GCP_PROJECT_ID` & `GCP_GCS_BUCKET`, as per your config.\n    * Optionally, if your `google-credentials.json` is stored somewhere else, such as a path like `$HOME/.gc`, \n    modify the env-vars (`GOOGLE_APPLICATION_CREDENTIALS`, `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT`) and `volumes` path in `docker-compose-nofrills.yml`\n\n6. Here's how the final versions of your [Dockerfile](./Dockerfile) and [docker-compose-nofrills](./docker-compose-nofrills.yml) should look.\n\n\n## Problems\n\n### `no-frills setup does not work for me - WSL/Windows user `\n\nIf you are running Docker in Windows/WSL/WSL2 and you have encountered some `ModuleNotFoundError` or low performance issues,\ntake a look at this [Airflow & WSL2 gist](https://gist.github.com/nervuzz/d1afe81116cbfa3c834634ebce7f11c5) focused entirely on troubleshooting possible problems.\n\n### `File /.google/credentials/google_credentials.json was not found`\n\nFirst, make sure you have your credentials in your `$HOME/.google/credentials`.\nMaybe you missed the step and didn't copy the your JSON with credentials there?\nAlso, make sure the file-name is `google_credentials.json`.\n\nSecond, check that docker-compose can correctly map this directory to airflow worker.\n\nExecute `docker ps` to see the list of docker containers running on your host machine and find the ID of the airflow worker.\n\nThen execute `bash` on this container:\n\n```bash\ndocker exec -it <container-ID> bash\n```\n\nNow check if the file with credentials is actually there:\n\n```bash\nls -lh /.google/credentials/\n```\n\nIf it's empty, docker-compose couldn't map the folder with credentials. \nIn this case, try changing it to the absolute path to this folder:\n\n```yaml\n  volumes:\n    - ./dags:/opt/airflow/dags\n    - ./logs:/opt/airflow/logs\n    - ./plugins:/opt/airflow/plugins\n    # here: ----------------------------\n    - c:/Users/alexe/.google/credentials/:/.google/credentials:ro\n    # -----------------------------------\n```",
    "filename": "cohorts/2022/week_3_data_warehouse/airflow/2_setup_nofrills.md"
  },
  {
    "content": "### Concepts\n\n [Airflow Concepts and Architecture](../week_2_data_ingestion/airflow/docs/1_concepts.md)\n\n### Workflow\n\n ![](docs/gcs_2_bq_dag_graph_view.png)\n \n ![](docs/gcs_2_bq_dag_tree_view.png)\n \n### Setup - Official Version\n (For the section on the Custom/Lightweight setup, scroll down)\n\n #### Setup\n  [Airflow Setup with Docker, through official guidelines](1_setup_official.md)\n\n #### Execution\n \n  1. Build the image (only first-time, or when there's any change in the `Dockerfile`, takes ~15 mins for the first-time):\n     ```shell\n     docker-compose build\n     ```\n   \n     or (for legacy versions)\n   \n     ```shell\n     docker build .\n     ```\n\n 2. Initialize the Airflow scheduler, DB, and other config\n    ```shell\n    docker-compose up airflow-init\n    ```\n\n 3. Kick up the all the services from the container:\n    ```shell\n    docker-compose up\n    ```\n\n 4. In another terminal, run `docker-compose ps` to see which containers are up & running (there should be 7, matching with the services in your docker-compose file).\n\n 5. Login to Airflow web UI on `localhost:8080` with default creds: `airflow/airflow`\n\n 6. Run your DAG on the Web Console.\n\n 7. On finishing your run or to shut down the container/s:\n    ```shell\n    docker-compose down\n    ```\n\n    To stop and delete containers, delete volumes with database data, and download images, run:\n    ```\n    docker-compose down --volumes --rmi all\n    ```\n\n    or\n    ```\n    docker-compose down --volumes --remove-orphans\n    ```\n       \n### Setup - Custom No-Frills Version (Lightweight)\nThis is a quick, simple & less memory-intensive setup of Airflow that works on a LocalExecutor.\n\n  #### Setup\n  [Airflow Setup with Docker, customized](2_setup_nofrills.md)\n\n  #### Execution\n  \n  1. Stop and delete containers, delete volumes with database data, & downloaded images (from the previous setup):\n    ```\n    docker-compose down --volumes --rmi all\n    ```\n\n   or\n    ```\n    docker-compose down --volumes --remove-orphans\n    ```\n    \n   Or, if you need to clear your system of any pre-cached Docker issues:\n    ```\n    docker system prune\n    ```\n    \n   Also, empty the airflow `logs` directory.\n    \n  2. Build the image (only first-time, or when there's any change in the `Dockerfile`):\n  Takes ~5-10 mins for the first-time\n    ```shell\n    docker-compose build\n    ```\n    or (for legacy versions)\n    ```shell\n    docker build .\n    ```\n\n  3. Kick up the all the services from the container (no need to specially initialize):\n    ```shell\n    docker-compose -f docker-compose-nofrills.yml up\n    ```\n\n  4. In another terminal, run `docker ps` to see which containers are up & running (there should be 3, matching with the services in your docker-compose file).\n\n  5. Login to Airflow web UI on `localhost:8080` with creds: `admin/admin` (explicit creation of admin user was required)\n\n  6. Run your DAG on the Web Console.\n\n  7. On finishing your run or to shut down the container/s:\n    ```shell\n    docker-compose down\n    ```\n    \n   \n\n### Future Enhancements\n* Deploy self-hosted Airflow setup on Kubernetes cluster, or use a Managed Airflow (Cloud Composer) service by GCP\n\n### References\nFor more info, check out these official docs:\n   * https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html\n   * https://airflow.apache.org/docs/docker-stack/build.html\n   * https://airflow.apache.org/docs/docker-stack/recipes.html",
    "filename": "cohorts/2022/week_3_data_warehouse/airflow/README.md"
  },
  {
    "code": false,
    "content": "# Documentation of the Airflow DAG Script\n\nThis document provides a high-level overview of an Apache Airflow Directed Acyclic Graph (DAG) script designed for data processing involving Google Cloud Storage (GCS) and Google BigQuery. The script performs a series of tasks that transfer, create, and process datasets related to trip data.\n\n## Environment Variables and Constants\n\nThe script pulls several configurations from environment variables, which allows for dynamic setup based on deployment in different environments. The key constants used in the script are:\n\n- **PROJECT_ID**: The Google Cloud project ID.\n- **BUCKET**: The GCS bucket name where data files are stored.\n- **path_to_local_home**: The directory where Airflow is expected to be running, defaulting to `/opt/airflow/`.\n- **BIGQUERY_DATASET**: The BigQuery dataset name, with a default value of 'trips_data_all'.\n- **DATASET**: A constant string set to \"tripdata\", representing the dataset's name.\n- **COLOUR_RANGE**: A dictionary mapping the color of the trip data to their respective datetime columns used for partitioning.\n- **INPUT_PART**: The path where raw input files are located, set as \"raw\".\n- **INPUT_FILETYPE**: The file type of the data being processed, specifically \"parquet\".\n\n## DAG Configuration\n\nThe DAG is created with specific properties and configurations that manage how the tasks are executed:\n\n- **dag_id**: The identifier for the DAG is \"gcs_2_bq_dag\".\n- **schedule_interval**: This DAG is scheduled to run daily.\n- **default_args**: A dictionary containing default arguments such as the owner, start date, and retry configurations.\n- **catchup**: Set to `False`, meaning the DAG will not attempt to catch up on missed schedules.\n- **max_active_runs**: Limited to 1, ensuring that concurrent runs do not occur.\n- **tags**: A way to categorize the DAG for easier identification, here tagged as 'dtc-de'.\n\n## Task Definitions\n\nThe script defines multiple tasks using a loop that iterates over the `COLOUR_RANGE` dictionary. Each color in the range corresponds to a separate processing flow for the trip data files. The tasks defined in the loop include:\n\n### 1. Move Files from GCS to GCS\n\nEach iteration of the loop creates a task using `GCSToGCSOperator`. This operator is responsible for moving files from a source location to a destination within the same GCS bucket. \n- **task_id**: Dynamically generated based on the color and dataset (e.g., `move_yellow_tripdata_files_task`).\n- **source_bucket**: The bucket defined by the `BUCKET` variable.\n- **source_object**: Pattern matching files in the raw input directory.\n- **destination_object**: The destination path where the files will be moved.\n- **move_object**: Set to `True`, indicating that the files should be moved rather than copied.\n\n### 2. Create External Table in BigQuery\n\nThe next task utilizes `BigQueryCreateExternalTableOperator` to create an external table in BigQuery that points to the moved files.\n- **task_id**: Again, dynamically generated for identification.\n- **table_resource**: Contains the project, dataset, and table reference, as well as the configuration for the external data.\n- **externalDataConfiguration**: Autodetection settings and details about the data source format.\n\n### 3. Create Partitioned Table\n\nThe third task involves creating a partitioned table within BigQuery using the `BigQueryInsertJobOperator`. This job uses a SQL query defined in the `CREATE_BQ_TBL_QUERY`.\n- **task_id**: Unique identifier for tracking the task.\n- **configuration**: Specifies the query to be executed, designed to create or replace the designated partitioned table based on data from the external table.\n\n### Task Dependencies\n\nThe tasks are linked in a specific sequence using bitwise shift operators (`>>`), establishing the order of execution:\n\n1. First, each file is moved from GCS to GCS.\n2. Then, an external table is created for the moved files in BigQuery.\n3. Finally, the partitioned table is created based on the external table definition.\n\nThis sequential dependency ensures that each task waits for the completion of the preceding task before it starts, maintaining data integrity and proper workflow management.\n\n## Conclusion\n\nIn summary, this script automates the process of transferring raw trip data files from Google Cloud Storage to a structured format in Google BigQuery. It establishes a clear workflow for data ingestion, ensuring that datasets are accessible and correctly partitioned for efficient querying. The use of Airflow allows for this process to be scheduled and monitored, facilitating data operations within cloud environments.",
    "filename": "cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py"
  },
  {
    "content": "## Week 5 Homework\n\nIn this homework we'll put what we learned about Spark\nin practice.\n\nWe'll use high volume for-hire vehicles (HVFHV) dataset for that.\n\n## Question 1. Install Spark and PySpark\n\n* Install Spark\n* Run PySpark\n* Create a local spark session \n* Execute `spark.version`\n\nWhat's the output?\n\n\n## Question 2. HVFHW February 2021\n\nDownload the HVFHV data for february 2021:\n\n```bash\nwget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n```\n\nRead it with Spark using the same schema as we did \nin the lessons. We will use this dataset for all\nthe remaining questions.\n\nRepartition it to 24 partitions and save it to\nparquet.\n\nWhat's the size of the folder with results (in MB)?\n\n\n## Question 3. Count records \n\nHow many taxi trips were there on February 15?\n\nConsider only trips that started on February 15.\n\n\n## Question 4. Longest trip for each day\n\nNow calculate the duration for each trip.\n\nTrip starting on which day was the longest? \n\n\n## Question 5. Most frequent `dispatching_base_num`\n\nNow find the most frequently occurring `dispatching_base_num` \nin this dataset.\n\nHow many stages this spark job has?\n\n> Note: the answer may depend on how you write the query,\n> so there are multiple correct answers. \n> Select the one you have.\n\n\n## Question 6. Most common locations pair\n\nFind the most common pickup-dropoff pair. \n\nFor example:\n\n\"Jamaica Bay / Clinton East\"\n\nEnter two zone names separated by a slash\n\nIf any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\". \n\n\n## Bonus question. Join type\n\n(not graded) \n\nFor finding the answer to Q6, you'll need to perform a join.\n\nWhat type of join is it?\n\nAnd how many stages your spark job has?\n\n\n## Submitting the solutions\n\n* Form for submitting: https://forms.gle/dBkVK9yT8cSMDwuw7\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 07 March (Monday), 22:00 CET",
    "filename": "cohorts/2022/week_5_batch_processing/homework.md"
  },
  {
    "content": "## Week 6 Homework\n[Form](https://forms.gle/mSzfpPCXskWCabeu5)\n\nThe homework is mostly theoretical. In the last question you have to provide working code link, please keep in mind that this\nquestion is not scored.\n\nDeadline: 14 March, 22:00 CET",
    "filename": "cohorts/2022/week_6_stream_processing/homework.md"
  },
  {
    "content": "## Data Engineering Zoomcamp 2023 Cohort\n\n* [Launch stream with course overview](https://www.youtube.com/watch?v=-zpVha7bw5A)\n* [Course Google calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ)\n* [FAQ](https://datatalks.club/faq/data-engineering-zoomcamp.html)\n* [Public Leaderboard](leaderboard.md) and [Private Leaderboard](https://docs.google.com/spreadsheets/d/e/2PACX-1vTbL00GcdQp0bJt9wf1ROltMq7s3qyxl-NYF7Pvk79Jfxgwfn9dNWmPD_yJHTDq_Wzvps8EIr6cOKWm/pubhtml)\n* [Course Playlist: Only 2023 Live videos & homeworks](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW)\n\n[**Week 1: Introduction & Prerequisites**](week_1_docker_sql/)\n\n* [Homework SQL](week_1_docker_sql/homework.md) and [solution](https://www.youtube.com/watch?v=KIh_9tZiroA)\n* [Homework Terraform](week_1_terraform/homework.md)\n* [Office hours](https://www.youtube.com/watch?v=RVTryVvSyw4&list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW)\n\n[**Week 2: Workflow Orchestration**](week_2_workflow_orchestration)\n\n* [Homework](week_2_workflow_orchestration/homework.md)\n* [Office hours part 1](https://www.youtube.com/watch?v=a_nmLHb8hzw&list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW) and [part 2](https://www.youtube.com/watch?v=PK8yyMY54Vk&list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW&index=7) \n\n[**Week 3: Data Warehouse**](week_3_data_warehouse)\n\n* [Homework](week_3_data_warehouse/homework.md)\n* [Office hours](https://www.youtube.com/watch?v=QXfmtJp3bXE&list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW)\n\n[**Week 4: Analytics Engineering**](week_4_analytics_engineering/)\n\n* [Homework](week_4_analytics_engineering/homework.md)\n* [PipeRider + dbt Workshop](workshops/piperider.md)\n* [Office hours](https://www.youtube.com/watch?v=ODYg_r72qaE&list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW)\n\n[**Week 5: Batch processing**](week_5_batch_processing/)\n\n* [Homework](week_5_batch_processing/homework.md)\n* [Office hours](https://www.youtube.com/watch?v=5_69yL2PPYI&list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW)\n\n[**Week 6: Stream Processing**](week_6_stream_processing)\n\n* [Homework](week_6_stream_processing/homework.md)\n\n\n[**Week 7, 8 & 9: Project**](project.md)\n\nMore information [here](project.md)",
    "filename": "cohorts/2023/README.md"
  },
  {
    "content": "## Leaderboard \n\nThis is the top [100 leaderboard](https://docs.google.com/spreadsheets/d/e/2PACX-1vTbL00GcdQp0bJt9wf1ROltMq7s3qyxl-NYF7Pvk79Jfxgwfn9dNWmPD_yJHTDq_Wzvps8EIr6cOKWm/pubhtml)\nof participants of Data Engineering Zoomcamp 2023 edition!\n\n<table>\n<tr>\n  <th>Name</th>\n  <th>Project</th>\n  <th>Social</th>\n  <th>Links and comments</th>\n</tr>\n<tr>\n<td>Katharina Eichinger</td>\n<td><a href=\"https://github.com/PandaKata/dezoomcamp-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/katharina-eichinger/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/PandaKata\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Alia Hamwi</td>\n<td><a href=\"https://github.com/AliaHa3/data-engineering-zoomcamp-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/alia-hamwi/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/AliaHa3\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Emmanuel Ikpesu</td>\n<td><a href=\"https://github.com/uchiharon/DataTalksClub_de-zoomcamp_CapStone_Project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/emmanuel-ikpesu-393708132/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/uchiharon\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://medium.com/@emmanarutops2/automating-data-pipelines-using-prefect-block-98d9b16f16bc\">Automating Data Pipelines Using Prefect\u00a0Block</a></li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Sanya Syed</td>\n<td><a href=\"https://github.com/sanyassyed/sf_eviction\">Project</a></td>\n<td> <a href=\"http://linkedin.com/in/sanyasy\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/sanyassyed\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://resume.creddle.io/resume/1so01cu6gx7\">My Resume</a></li>\n</ul>\n\n> I am excited about the prospect of securing a challenging role as a Data Engineer, where I can utilise my skills and expertise to contribute meaningfully to an organisation's data-driven initiatives. </details></td>\n</tr>\n<tr>\n<td>Aminu Lawal</td>\n<td><a href=\"https://github.com/zabull1/cycling_DE_project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/aminu-lawal-600920100/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/zabull1\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Lisa Reiber</td>\n<td><a href=\"https://github.com/lisallreiber/biketheft_berlin\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/lisareiber/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/lisallreiber\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://lookerstudio.google.com/u/2/reporting/8a06d083-e46f-403a-bcb0-d3ff23434e24/page/p_nmv21l7w4c\">Project Dashboard</a></li>\n</ul>\n\n> always happy to connect with other data enthusiasts over topics like low-budget data engineering solutions for non-profits or AI solutions for non-profits</details></td>\n</tr>\n<tr>\n<td>Vincenzo Galante</td>\n<td><a href=\"https://lookerstudio.google.com/u/0/reporting/ebdf68e1-27f7-435b-8add-a4018681f801/page/BkBJD\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/galantevincenzo/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/VincenzoGalante\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\n\n> Thank you for having this course!</details></td>\n</tr>\n<tr>\n<td>Grzegorz G\u0105tkowski </td>\n<td><a href=\"https://github.com/GrzegorzGatkowski/Air_Pollution_Pipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/grzegorz-g%C4%85tkowski-811727125/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/GrzegorzGatkowski\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Matt Young</td>\n<td><a href=\"https://github.com/directdetour/BeerReviewsDataPipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/matt-young-11377720/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/directdetour\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://twitter.com/ymatty\">Twitter</a></li>\n</ul>\n\n> Experienced Developer | Cloud & Data Enthusiast | Open to Cloud & Data Engineering Roles \ud83c\udf29\ufe0f\n\u279c C#, SQL, JavaScript, Python | BI, Data Analytics | AWS, Azure, GCP\n\nPassionate about data pipelines, storage, and processing. Excited to implement advanced cloud solutions and enable data-driven insights. Seeking Data Engineering opportunities to leverage my extensive SQL/Data Analytics experience and to transition into the world of cloud-based data solutions. Let's connect and collaborate on innovative data projects! #DataEngineering #CloudTechnology</details></td>\n</tr>\n<tr>\n<td>Sam Hatley</td>\n<td><a href=\"https://github.com/sam-hatley/real-estate-data\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/samhatley/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/sam-hatley\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Evan Hofmeister</td>\n<td><a href=\"https://github.com/EvanHofmeister/Housing-Wealth-Pipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/evanhofmeister/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/EvanHofmeister\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Barys Kazarkin</td>\n<td><a href=\"https://github.com/KazarkinBarys/Data_Engineering_Zoomcamp_Project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/barys-kazarkin-b9904b203/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/KazarkinBarys\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Joshua Ati</td>\n<td><a href=\"https://github.com/joshuaati/DE_airline_pipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/joshua-ati-460750110/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/joshuaati\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Oleg Agapov</td>\n<td><a href=\"https://github.com/oleg-agapov/de-zoomcamp-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/oagapov/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/oleg-agapov/\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://twitter.com/oleg_agapov_\">Twitter</a></li>\n<li><a href=\"https://olegagapov.com/\">Website</a></li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Mikhail Kuklin</td>\n<td><a href=\"https://github.com/MikhailKuklin/data-pipeline-COVID19-monitoring\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/mikhail-kuklin-194a9544/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/MikhailKuklin\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://mikhailkuklin.wordpress.com\">Personal webpage</a></li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Emmanuel Letremble</td>\n<td><a href=\"https://github.com/Valkea/DE_bootcamp_project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/letremble\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Valkea\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://valkea.github.io\">Portfolio</a></li>\n</ul>\n\n> Thanks to the DataTalks.Club for completing my Full Stack & Machine Learning skill sets with some extra DE knowledge.</details></td>\n</tr>\n<tr>\n<td>Victor Kuang</td>\n<td><a href=\"https://github.com/vykuang/toronto-service-calls-2023\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/vykuang/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/vykuang\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Antonis Angelakis</td>\n<td><a href=\"https://github.com/angeanto/dezoomcamp-project-youtube\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/antonios-angelakis-249899101\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/angeanto\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Christian Ruiz</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Alex Pilugin</td>\n<td><a href=\"https://github.com/skipper-com/dtc_de_course_project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/alexander-pilugin/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/skipper-com?tab=repositories\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Ahmad Rizky</td>\n<td><a href=\"https://linktr.ee/ahmdxrzky\">Project</a></td>\n<td> <a href=\"https://linkedin.com/in/ahmdxrzky\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/ahmdxrzky\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Juan Francisco Hernandez Hernandez </td>\n<td><a href=\"https://github.com/JuanPacoHernandez/TelecommDescriptive-Analysis\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/juan-paco-hernandez/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/JuanPacoHernandez\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\n\n> Thanks to Data Talks Club, it was amazing learning for me as a Career changer.</details></td>\n</tr>\n<tr>\n<td>Iurii Chernigin</td>\n<td><a href=\"https://github.com/iurii-chernigin/audio-streaming-data-platform\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/iurii-chernigin/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/iurii-chernigin\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Franklyne Kibet</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Federico Zambelli</td>\n<td><a href=\"https://github.com/wtfzambo/subreddit-analytics\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/fzambo/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/wtfzambo\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Marilina Orihuela</td>\n<td><a href=\"https://github.com/mary435/MLA_Dashboard\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/marilina-orihuela/?locale=en_US\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/mary435\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Alejandro R. M\u00e1rmol Ruiz</td>\n<td><a href=\"https://github.com/marmola90/dezoomcampam\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/alejandro-marmol-81a998167/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/marmola90\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Daniel Takeshi</td>\n<td><a href=\"https://github.com/danietakeshi/de-zoomcamp-2023/tree/main/project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/daniel-takeshi\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/danietakeshi\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Xia He-Bleinagel</td>\n<td><a href=\"https://github.com/Data-Think-2021/DE-Final-Project-CO2\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/xia-he-bleinagel-51773585/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Data-Think-2021\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://xiahe-bleinagel.com/\">Personal website</a></li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Thorsten Foltz</td>\n<td></td>\n<td> <a href=\"https://www.linkedin.com/in/thorsten-foltz-a91481127/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Danh Vo</td>\n<td><a href=\"https://github.com/datavadoz/eu-airbnb\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/0798a811b\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/datavadoz\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Joseph Ologunja</td>\n<td><a href=\"https://github.com/Joseun/data-engineering-zoomcamp/tree/main/cohorts/2023/week_7_project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/josephologunja/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Joseun\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Roman Zabolotin</td>\n<td><a href=\"https://github.com/rzabolotin/de_zoomcamp_2023_project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/rzabolotin/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/rzabolotin\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Aditya Gupta </td>\n<td><a href=\"https://github.com/itsadityagupta/yelposphere\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/itsadityagupta\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/itsadityagupta\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://peerlist.io/itsadityagupta\">Portfolio</a></li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Vladimir Bugaevskii</td>\n<td><a href=\"https://github.com/vbugaevskii/de-zoomcamp-cycling-2023\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/vbugaevskii/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/vbugaevskii\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Fozan Talat</td>\n<td><a href=\"https://github.com/Fozan-Talat/divvy-bikeshare-de-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/fozan-talat/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Fozan-Talat\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Alain Boisvert</td>\n<td><a href=\"https://github.com/boisalai/twitter-dashboard\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/alain-boisvert-98b058156/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/boisalai\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>reneboy garcia</td>\n<td><a href=\"https://github.com/reneboygarcia/capstone_project_mongodb.git\">Project</a></td>\n<td> <a href=\"http://www.linkedin.com/in/eboygarcia\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/reneboygarcia\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\n\n> \"Success is not always about the grand achievements; it's about the small victories that accumulate over time.\" - Unknown</details></td>\n</tr>\n<tr>\n<td>Svetlana Kononova</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Dmitrii Nikolaev</td>\n<td></td>\n<td> <a href=\"https://www.linkedin.com/in/dnnikolaev/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/melvinru\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://t.me/melvinru\">DN Telegram</a></li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Francis Romio</td>\n<td><a href=\"https://github.com/romiof/brazil-weather\">Project</a></td>\n<td> <a href=\"https://br.linkedin.com/in/francisromio\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/romiof\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Saul Acevedo</td>\n<td><a href=\"https://github.com/seacevedo/Solana-Pipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/saul-acevedo-739b17122\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/seacevedo\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Alina Li</td>\n<td><a href=\"https://github.com/alinali87/de-zoomcamp-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/alinali87/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Alexander Eryuzhev</td>\n<td><a href=\"https://github.com/aeryuzhev/de-zoomcamp-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/alexander-eryuzhev/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Paul Nwosu</td>\n<td><a href=\"https://github.com/paulonye/Cloudrunjobs\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/nwosu-paul-1b7b2218b/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/paulonye\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li>https://medium.com/@nwosupaul141/serverless-deployment-of-a-prefect-data-pipeline-on-google-cloud-run-8c48765f2480</li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Param mirani </td>\n<td><a href=\"https://github.com/Param-29/stock-data-pipeline\">Project</a></td>\n<td> <a href=\"https://in.linkedin.com/in/param-mirani\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Param-29\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Oscar Garcia - ozkary</td>\n<td><a href=\"https://github.com/ozkary/data-engineering-mta-turnstile/\">Project</a></td>\n<td> <a href=\"https://github.com/ozkary\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://twitter.com/ozkary\">Twitter</a>  * <a href=\"https://www.youtube.com/channel/UCpaqmBQr8YE6ikLXXyt8D7g\">You Tube</a> * <a href=\"https://www.ozkary.com\">blog</a></li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Hector Torres</td>\n<td><a href=\"https://github.com/hdt94/dtc-de-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/hdt94/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/hdt94/\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://twitter.com/hdt94\">Twitter @hdt94</a></li>\n</ul>\n\n> Currently looking for a position as data engineer</details></td>\n</tr>\n<tr>\n<td>Dewi Nurfitri Oktaviani</td>\n<td><a href=\"https://github.com/oktavianidewi/github-data-pipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/dewi-nurfitri-oktaviani-6b450b22/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/oktavianidewi\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://medium.com/@oktavianidewi\">medium</a></li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Ryno Marx</td>\n<td></td>\n<td> <a href=\"https://www.linkedin.com/in/ryno-m-402a58120\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Hidir Cem Altun</td>\n<td></td>\n<td> <a href=\"https://www.linkedin.com/in/hidir-cem-altun-914aaa65/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/HCA97\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Francis Mark Cayco</td>\n<td><a href=\"https://github.com/PeteCastle/League-of-Legends-Analytics\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/francis-mark-cayco-33511a190/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/PeteCastle\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Adrian Baumann</td>\n<td><a href=\"https://github.com/adrian-baumann/dwd-temp-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/adrianbaumann/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/adrian-baumann\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Vladislav Garist</td>\n<td><a href=\"https://github.com/garistvlad/data-engineering-zoomcamp/tree/main/week-7\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/vgarist/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/garistvlad\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Gerald Ooi</td>\n<td></td>\n<td> <a href=\"https://www.linkedin.com/in/geraldooi/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Roman</td>\n<td></td>\n<td> <a href=\"https://www.linkedin.com/in/roman-yakovlev-86b2b4130\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/romanyakovlev\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Aleksandr Krasnov</td>\n<td></td>\n<td> <a href=\"https://www.linkedin.com/in/aleksandr-krasnov/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://www.linkedin.com/in/aleksandr-krasnov/\">Open to work</a></li>\n</ul></details></td>\n</tr>\n<tr>\n<td>Jaesung Ryu</td>\n<td><a href=\"https://github.com/Haebuk/GHArchive-Data-Pipeline-Project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/jaesungryu\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Haebuk\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Ant\u00f3nio Dami\u00e3o Rodrigues</td>\n<td><a href=\"https://github.com/adamiaonr/de-zoomcamp-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/adamiaonrod/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/adamiaonr\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Alicia Escontrela</td>\n<td><a href=\"https://github.com/aliescont/dezoomcamp-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/alicia-escontrela/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/aliescont\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Chalermdej Lematavekul</td>\n<td><a href=\"https://github.com/Chalermdej-l/Final_Project_FredETE\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/chalermdej-l/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Chalermdej-l?tab=repositories\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\n\n> Thank you so much for the course. Learn so many thing from here.</details></td>\n</tr>\n<tr>\n<td>Muhammed Jimoh</td>\n<td><a href=\"https://github.com/Manny-97/DE-ZOOMCAMP-PROJECT\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/%F0%9F%91%A8%F0%9F%8F%BE%E2%80%8D%F0%9F%92%BB-muhammed-jimoh-45120a14a/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Manny-97\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Bartosz Sk\u0142odowski</td>\n<td><a href=\"https://github.com/bartoszsklodowski/de_zoomcamp_project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/bartosz-sk%C5%82odowski/?locale=en_US\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/bartoszsklodowski\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Daniel Rigney</td>\n<td><a href=\"https://github.com/danielyrigney/USDA-Data-Pipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/daniel-rigney-data/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/danielyrigney\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Daniel Gheorghita</td>\n<td><a href=\"https://github.com/daniel-gheorghita/dezoomcamp/tree/main/7_project_Belgium_housing_market\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/daniel-gheorghita-4a59903a/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/daniel-gheorghita\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Daniel Gheorghita</td>\n<td><a href=\"https://github.com/daniel-gheorghita/belgian_housing_buy_vs_rent\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/daniel-gheorghita-4a59903a/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/daniel-gheorghita\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Niel Kemp</td>\n<td></td>\n<td> <a href=\"https://www.linkedin.com/in/nielkemp/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Shahmir</td>\n<td><a href=\"https://github.com/Light2Dark/quality-of-life\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/shahmir-varqha\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Light2Dark\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\nLinks:\n\n<ul>\n<li><a href=\"https://smolwaffle.com\">Portfolio</a></li>\n</ul>\n\n> I've added a bunch of new features since the reviews! Check it out</details></td>\n</tr>\n<tr>\n<td>Matt Bertrand</td>\n<td><a href=\"https://github.com/mbertrand/eo-climate-pipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/bertrandmatt/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/mbertrand\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Nikolay Galkov</td>\n<td><a href=\"https://github.com/ngalkov/DEZoomcamp_project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/nikolay-galkov/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/ngalkov\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Hiroko Sakai</td>\n<td><a href=\"https://github.com/hirobo/world-earthquake\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/hirokos/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/hirobo\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Rohit Joshi</td>\n<td><a href=\"https://github.com/Rohitjoshi07/FHVDataAnalysis\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/rohit-joshi09\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/RohitJoshi07\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Valerii Bazyrov</td>\n<td></td>\n<td> <a href=\"https://www.linkedin.com/in/lantenak/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/lantenak\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Juan Pablo Ricapito</td>\n<td><a href=\"https://github.com/EzicStar/BA-turnstiles-pipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/juan-pablo-ricapito-112332186/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/EzicStar\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Ashraf Omara</td>\n<td><a href=\"https://github.com/AshrafOmara12/Ukraine-Conflict-Twitter-Data-Pipeline\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/ashraf-omara-48294a106/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/AshrafOmara12\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\n\n> I need to thank all of the data club community for this amazing contribution. </details></td>\n</tr>\n<tr>\n<td>Wasawat Boonyarittikit</td>\n<td><a href=\"https://github.com/ChungWasawat/dtc_de_project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/wasawat-boonyarittikit-b1698b179/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/ChungWasawat\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td></td>\n</tr>\n<tr>\n<td>Fedor Faizov</td>\n<td><a href=\"https://github.com/Fedrpi/de-zoomcamp-bandcamp-project\">Project</a></td>\n<td> <a href=\"https://www.linkedin.com/in/fedor-faizov-a75b32245/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Fedrpi\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n<td><details>\n<summary>More info</summary>\n\n\n\n> Absolutly amazing course <3 </details></td>\n\n</tr>\n</table>",
    "filename": "cohorts/2023/leaderboard.md"
  },
  {
    "content": "## Course Project\n\nThe goal of this project is to apply everything we learned\nin this course and build an end-to-end data pipeline.\n\nYou will have two attempts to submit your project. If you don't have \ntime to submit your project by the end of attempt #1 (you started the \ncourse late, you have vacation plans, life/work got in the way, etc.)\nor you fail your first attempt, \nthen you will have a second chance to submit your project as attempt\n#2. \n\nThere are only two attempts.\n\nRemember that to pass the project, you must evaluate 3 peers. If you don't do that,\nyour project can't be considered complete.\n\nTo find the projects assigned to you, use the peer review assignments link \nand find your hash in the first column. You will see three rows: you need to evaluate \neach of these projects. For each project, you need to submit the form once,\nso in total, you will make three submissions. \n\n\n### Submitting\n\n#### Project Attempt #1\n\nProject:\n\n* Form: https://forms.gle/zTJiVYSmCgsENj6y8\n* Deadline: 10 April, 22:00 CET\n\nPeer reviewing:\n\n* Peer review assignments: [link](https://docs.google.com/spreadsheets/d/e/2PACX-1vRYQ0A9C7AkRK-YPSFhqaRMmuPR97QPfl2PjI8n11l5jntc6YMHIJXVVS0GQNqAYIGwzyevyManDB08/pubhtml?gid=0&single=true) (\"project-01\" sheet)\n* Form: https://forms.gle/1bxmgR8yPwV359zb7\n* Deadline: 17 April, 22:00 CET\n\nProject feedback: [link](https://docs.google.com/spreadsheets/d/e/2PACX-1vQuMt9m1XlPrCACqnsFTXTV_KGiSnsl9UjL7kdTMsLJ8DLu3jNJlPzoUKG6baxc8APeEQ8RaSP1U2VX/pubhtml?gid=27207346&single=true) (\"project-01\" sheet)\n\n#### Project Attempt #2\n\nProject:\n\n* Form: https://forms.gle/gCXUSYBm1KgMKXVm8\n* Deadline: 4 May, 22:00 CET\n\nPeer reviewing:\n\n* Peer review assignments: [link](https://docs.google.com/spreadsheets/d/e/2PACX-1vRYQ0A9C7AkRK-YPSFhqaRMmuPR97QPfl2PjI8n11l5jntc6YMHIJXVVS0GQNqAYIGwzyevyManDB08/pubhtml?gid=303437788&single=true) (\"project-02\" sheet)\n* Form: https://forms.gle/2x5MT4xxczR8isy37\n* Deadline: 11 May, 22:00 CET\n\nProject feedback: [link](https://docs.google.com/spreadsheets/d/e/2PACX-1vQuMt9m1XlPrCACqnsFTXTV_KGiSnsl9UjL7kdTMsLJ8DLu3jNJlPzoUKG6baxc8APeEQ8RaSP1U2VX/pubhtml?gid=246029638&single=true)\n\n### Evaluation criteria\n\nSee [here](../../week_7_project/README.md)\n\n\n### Misc\n\nTo get the hash for your project, use this function to hash your email:\n\n```python\nfrom hashlib import sha1\n\ndef compute_hash(email):\n    return sha1(email.lower().encode('utf-8')).hexdigest()\n```\n\nOr use [this website](http://www.sha1-online.com/).",
    "filename": "cohorts/2023/project.md"
  },
  {
    "content": "## Week 1 Homework\n\nIn this homework we'll prepare the environment \nand practice with Docker and SQL\n\n\n## Question 1. Knowing docker tags\n\nRun the command to get information on Docker \n\n```docker --help```\n\nNow run the command to get help on the \"docker build\" command\n\nWhich tag has the following text? - *Write the image ID to the file* \n\n- `--imageid string`\n- `--iidfile string`\n- `--idimage string`\n- `--idfile string`\n\n\n## Question 2. Understanding docker first run \n\nRun docker with the python:3.9 image in an interactive mode and the entrypoint of bash.\nNow check the python modules that are installed ( use pip list). \nHow many python packages/modules are installed?\n\n- 1\n- 6\n- 3\n- 7\n\n# Prepare Postgres\n\nRun Postgres and load data as shown in the videos\nWe'll use the green taxi trips from January 2019:\n\n```wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-01.csv.gz```\n\nYou will also need the dataset with zones:\n\n```wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv```\n\nDownload this data and put it into Postgres (with jupyter notebooks or with a pipeline)\n\n\n## Question 3. Count records \n\nHow many taxi trips were totally made on January 15?\n\nTip: started and finished on 2019-01-15. \n\nRemember that `lpep_pickup_datetime` and `lpep_dropoff_datetime` columns are in the format timestamp (date and hour+min+sec) and not in date.\n\n- 20689\n- 20530\n- 17630\n- 21090\n\n## Question 4. Largest trip for each day\n\nWhich was the day with the largest trip distance\nUse the pick up time for your calculations.\n\n- 2019-01-18\n- 2019-01-28\n- 2019-01-15\n- 2019-01-10\n\n## Question 5. The number of passengers\n\nIn 2019-01-01 how many trips had 2 and 3 passengers?\n \n- 2: 1282 ; 3: 266\n- 2: 1532 ; 3: 126\n- 2: 1282 ; 3: 254\n- 2: 1282 ; 3: 274\n\n\n## Question 6. Largest tip\n\nFor the passengers picked up in the Astoria Zone which was the drop off zone that had the largest tip?\nWe want the name of the zone, not the id.\n\nNote: it's not a typo, it's `tip` , not `trip`\n\n- Central Park\n- Jamaica\n- South Ozone Park\n- Long Island City/Queens Plaza\n\n\n## Submitting the solutions\n\n* Form for submitting: [form](https://forms.gle/EjphSkR1b3nsdojv7)\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 30 January (Monday), 22:00 CET\n\n\n## Solution\n\nSee here: https://www.youtube.com/watch?v=KIh_9tZiroA",
    "filename": "cohorts/2023/week_1_docker_sql/homework.md"
  },
  {
    "content": "## Week 1 Homework\n\nIn this homework we'll prepare the environment by creating resources in GCP with Terraform.\n\nIn your VM on GCP install Terraform. Copy the files from the course repo\n[here](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/1_terraform_gcp/terraform) to your VM.\n\nModify the files as necessary to create a GCP Bucket and Big Query Dataset.\n\n\n## Question 1. Creating Resources\n\nAfter updating the main.tf and variable.tf files run:\n\n```\nterraform apply\n```\n\nPaste the output of this command into the homework submission form.\n\n\n## Submitting the solutions\n\n* Form for submitting: [form](https://forms.gle/S57Xs3HL9nB3YTzj9)\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 30 January (Monday), 22:00 CET",
    "filename": "cohorts/2023/week_1_terraform/homework.md"
  },
  {
    "content": "## Week 2: Workflow Orchestration\n\nPython code from videos is linked [below](#code-repository).\n\nAlso, if you find the commands too small to view in Kalise's videos, here's the [transcript with code for the second Prefect video](https://github.com/discdiver/prefect-zoomcamp/tree/main/flows/01_start) and the [fifth Prefect video](https://github.com/discdiver/prefect-zoomcamp/tree/main/flows/03_deployments).\n\n### Data Lake (GCS)\n\n* What is a Data Lake\n* ELT vs. ETL\n* Alternatives to components (S3/HDFS, Redshift, Snowflake etc.)\n* [Video](https://www.youtube.com/watch?v=W3Zm6rjOq70&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n* [Slides](https://docs.google.com/presentation/d/1RkH-YhBz2apIjYZAxUz2Uks4Pt51-fVWVN9CcH9ckyY/edit?usp=sharing)\n\n\n### 1. Introduction to Workflow orchestration\n\n* What is orchestration?\n* Workflow orchestrators vs. other types of orchestrators\n* Core features of a workflow orchestration tool\n* Different types of workflow orchestration tools that currently exist \n\n:movie_camera: [Video](https://www.youtube.com/watch?v=8oLs6pzHp68&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n\n### 2. Introduction to Prefect concepts\n\n* What is Prefect?\n* Installing Prefect\n* Prefect flow\n* Creating an ETL\n* Prefect task\n* Blocks and collections\n* Orion UI\n\n:movie_camera: [Video](https://www.youtube.com/watch?v=cdtN6dhp708&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n### 3. ETL with GCP & Prefect\n\n* Flow 1: Putting data to Google Cloud Storage \n\n:movie_camera: [Video](https://www.youtube.com/watch?v=W-rMz_2GwqQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n\n### 4. From Google Cloud Storage to Big Query\n\n* Flow 2: From GCS to BigQuery\n\n:movie_camera: [Video](https://www.youtube.com/watch?v=Cx5jt-V5sgE&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n### 5. Parametrizing Flow & Deployments \n\n* Parametrizing the script from your flow\n* Parameter validation with Pydantic\n* Creating a deployment locally\n* Setting up Prefect Agent\n* Running the flow\n* Notifications\n\n:movie_camera: [Video](https://www.youtube.com/watch?v=QrDxPjX10iw&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n### 6. Schedules & Docker Storage with Infrastructure\n\n* Scheduling a deployment\n* Flow code storage\n* Running tasks in Docker\n\n:movie_camera: [Video](https://www.youtube.com/watch?v=psNSzqTsi-s&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n### 7. Prefect Cloud and Additional Resources \n\n\n* Using Prefect Cloud instead of local Prefect\n* Workspaces\n* Running flows on GCP\n\n:movie_camera: [Video](https://www.youtube.com/watch?v=gGC23ZK7lr8&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n\n* [Prefect docs](https://docs.prefect.io/)\n* [Pefect Discourse](https://discourse.prefect.io/)\n* [Prefect Cloud](https://app.prefect.cloud/)\n* [Prefect Slack](https://prefect-community.slack.com)\n\n### Code repository\n\n[Code from videos](https://github.com/discdiver/prefect-zoomcamp) (with a few minor enhancements)\n\n### Homework \nHomework can be found [here](./homework.md).\n\n## Community notes\n\nDid you take notes? You can share them here.\n\n* [Blog by Marcos Torregrosa (Prefect)](https://www.n4gash.com/2023/data-engineering-zoomcamp-semana-2/)\n* [Notes from Victor Padilha](https://github.com/padilha/de-zoomcamp/tree/master/week2)\n* [Notes by Alain Boisvert](https://github.com/boisalai/de-zoomcamp-2023/blob/main/week2.md)\n* [Notes by Candace Williams](https://github.com/teacherc/de_zoomcamp_candace2023/blob/main/week_2/week2_notes.md)\n* [Notes from Xia He-Bleinagel](https://xiahe-bleinagel.com/2023/02/week-2-data-engineering-zoomcamp-notes-prefect/)\n* [Notes from froukje](https://github.com/froukje/de-zoomcamp/blob/main/week_2_workflow_orchestration/notes/notes_week_02.md)\n* [Notes from Balaji](https://github.com/Balajirvp/DE-Zoomcamp/blob/main/Week%202/Detailed%20Week%202%20Notes.ipynb)\n* More on [Pandas vs SQL, Prefect capabilities, and testing your data](https://medium.com/@verazabeida/zoomcamp-2023-week-3-7f27bb8c483f), by Vera\n* Add your notes here (above this line)",
    "filename": "cohorts/2023/week_2_workflow_orchestration/README.md"
  },
  {
    "content": "## Week 2 Homework\n\nThe goal of this homework is to familiarise users with workflow orchestration and observation. \n\n\n## Question 1. Load January 2020 data\n\nUsing the `etl_web_to_gcs.py` flow that loads taxi data into GCS as a guide, create a flow that loads the green taxi CSV dataset for January 2020 into GCS and run it. Look at the logs to find out how many rows the dataset has.\n\nHow many rows does that dataset have?\n\n* 447,770\n* 766,792\n* 299,234\n* 822,132\n\n\n## Question 2. Scheduling with Cron\n\nCron is a common scheduling specification for workflows. \n\nUsing the flow in `etl_web_to_gcs.py`, create a deployment to run on the first of every month at 5am UTC. What\u2019s the cron schedule for that?\n\n- `0 5 1 * *`\n- `0 0 5 1 *`\n- `5 * 1 0 *`\n- `* * 5 1 0`\n\n\n## Question 3. Loading data to BigQuery \n\nUsing `etl_gcs_to_bq.py` as a starting point, modify the script for extracting data from GCS and loading it into BigQuery. This new script should not fill or remove rows with missing values. (The script is really just doing the E and L parts of ETL).\n\nThe main flow should print the total number of rows processed by the script. Set the flow decorator to log the print statement.\n\nParametrize the entrypoint flow to accept a list of months, a year, and a taxi color. \n\nMake any other necessary changes to the code for it to function as required.\n\nCreate a deployment for this flow to run in a local subprocess with local flow code storage (the defaults).\n\nMake sure you have the parquet data files for Yellow taxi data for Feb. 2019 and March 2019 loaded in GCS. Run your deployment to append this data to your BiqQuery table. How many rows did your flow code process?\n\n- 14,851,920\n- 12,282,990\n- 27,235,753\n- 11,338,483\n\n\n\n## Question 4. Github Storage Block\n\nUsing the `web_to_gcs` script from the videos as a guide, you want to store your flow code in a GitHub repository for collaboration with your team. Prefect can look in the GitHub repo to find your flow code and read it. Create a GitHub storage block from the UI or in Python code and use that in your Deployment instead of storing your flow code locally or baking your flow code into a Docker image. \n\nNote that you will have to push your code to GitHub, Prefect will not push it for you.\n\nRun your deployment in a local subprocess (the default if you don\u2019t specify an infrastructure). Use the Green taxi data for the month of November 2020.\n\nHow many rows were processed by the script?\n\n- 88,019\n- 192,297\n- 88,605\n- 190,225\n\n\n\n## Question 5. Email or Slack notifications\n\nQ5. It\u2019s often helpful to be notified when something with your dataflow doesn\u2019t work as planned. Choose one of the options below for creating email or slack notifications.\n\nThe hosted Prefect Cloud lets you avoid running your own server and has Automations that allow you to get notifications when certain events occur or don\u2019t occur. \n\nCreate a free forever Prefect Cloud account at app.prefect.cloud and connect your workspace to it following the steps in the UI when you sign up. \n\nSet up an Automation that will send yourself an email when a flow run completes. Run the deployment used in Q4 for the Green taxi data for April 2019. Check your email to see the notification.\n\nAlternatively, use a Prefect Cloud Automation or a self-hosted Orion server Notification to get notifications in a Slack workspace via an incoming webhook. \n\nJoin my temporary Slack workspace with [this link](https://join.slack.com/t/temp-notify/shared_invite/zt-1odklt4wh-hH~b89HN8MjMrPGEaOlxIw). 400 people can use this link and it expires in 90 days. \n\nIn the Prefect Cloud UI create an [Automation](https://docs.prefect.io/ui/automations) or in the Prefect Orion UI create a [Notification](https://docs.prefect.io/ui/notifications/) to send a Slack message when a flow run enters a Completed state. Here is the Webhook URL to use: https://hooks.slack.com/services/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\n\nTest the functionality.\n\nAlternatively, you can grab the webhook URL from your own Slack workspace and Slack App that you create. \n\n\nHow many rows were processed by the script?\n\n- `125,268`\n- `377,922`\n- `728,390`\n- `514,392`\n\n\n## Question 6. Secrets\n\nPrefect Secret blocks provide secure, encrypted storage in the database and obfuscation in the UI. Create a secret block in the UI that stores a fake 10-digit password to connect to a third-party service. Once you\u2019ve created your block in the UI, how many characters are shown as asterisks (*) on the next page of the UI?\n\n- 5\n- 6\n- 8\n- 10\n\n\n## Submitting the solutions\n\n* Form for submitting: https://forms.gle/PY8mBEGXJ1RvmTM97\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 8 February (Wednesday), 22:00 CET\n\n\n## Solution\n\n* Video: https://youtu.be/L04lvYqNlc0\n* Code: https://github.com/discdiver/prefect-zoomcamp/tree/main/flows/04_homework",
    "filename": "cohorts/2023/week_2_workflow_orchestration/homework.md"
  },
  {
    "content": "## Week 3 Homework\n<b><u>Important Note:</b></u> <p>You can load the data however you would like, but keep the files in .GZ Format. \nIf you are using orchestration such as Airflow or Prefect do not load the data into Big Query using the orchestrator.</br> \nStop with loading the files into a bucket. </br></br>\n<u>NOTE:</u> You can use the CSV option for the GZ files when creating an External Table</br>\n\n<b>SETUP:</b></br>\nCreate an external table using the fhv 2019 data. </br>\nCreate a table in BQ using the fhv 2019 data (do not partition or cluster this table). </br>\nData can be found here: https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/fhv </p>\n\n## Question 1:\nWhat is the count for fhv vehicle records for year 2019?\n- 65,623,481\n- 43,244,696\n- 22,978,333\n- 13,942,414\n\n## Question 2:\nWrite a query to count the distinct number of affiliated_base_number for the entire dataset on both the tables.</br> \nWhat is the estimated amount of data that will be read when this query is executed on the External Table and the Table?\n\n- 25.2 MB for the External Table and 100.87MB for the BQ Table\n- 225.82 MB for the External Table and 47.60MB for the BQ Table\n- 0 MB for the External Table and 0MB for the BQ Table\n- 0 MB for the External Table and 317.94MB for the BQ Table \n\n\n## Question 3:\nHow many records have both a blank (null) PUlocationID and DOlocationID in the entire dataset?\n- 717,748\n- 1,215,687\n- 5\n- 20,332\n\n## Question 4:\nWhat is the best strategy to optimize the table if query always filter by pickup_datetime and order by affiliated_base_number?\n- Cluster on pickup_datetime Cluster on affiliated_base_number\n- Partition by pickup_datetime Cluster on affiliated_base_number\n- Partition by pickup_datetime Partition by affiliated_base_number\n- Partition by affiliated_base_number Cluster on pickup_datetime\n\n## Question 5:\nImplement the optimized solution you chose for question 4. Write a query to retrieve the distinct affiliated_base_number between pickup_datetime 2019/03/01 and 2019/03/31 (inclusive).</br> \nUse the BQ table you created earlier in your from clause and note the estimated bytes. Now change the table in the from clause to the partitioned table you created for question 4 and note the estimated bytes processed. What are these values? Choose the answer which most closely matches.\n- 12.82 MB for non-partitioned table and 647.87 MB for the partitioned table\n- 647.87 MB for non-partitioned table and 23.06 MB for the partitioned table\n- 582.63 MB for non-partitioned table and 0 MB for the partitioned table\n- 646.25 MB for non-partitioned table and 646.25 MB for the partitioned table\n\n\n## Question 6: \nWhere is the data stored in the External Table you created?\n\n- Big Query\n- GCP Bucket\n- Container Registry\n- Big Table\n\n\n## Question 7:\nIt is best practice in Big Query to always cluster your data:\n- True\n- False\n\n\n## (Not required) Question 8:\nA better format to store these files may be parquet. Create a data pipeline to download the gzip files and convert them into parquet. Upload the files to your GCP Bucket and create an External and BQ Table. \n\n\nNote: Column types for all files used in an External Table must have the same datatype. While an External Table may be created and shown in the side panel in Big Query, this will need to be validated by running a count query on the External Table to check if any errors occur. \n \n## Submitting the solutions\n\n* Form for submitting: https://forms.gle/rLdvQW2igsAT73HTA\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 13 February (Monday), 22:00 CET\n\n\n## Solution\n\nSolution: https://www.youtube.com/watch?v=j8r2OigKBWE",
    "filename": "cohorts/2023/week_3_data_warehouse/homework.md"
  },
  {
    "content": "## Week 4 Homework \n\nIn this homework, we'll use the models developed during the week 4 videos and enhance the already presented dbt project using the already loaded Taxi data for fhv vehicles for year 2019 in our DWH.\n\nThis means that in this homework we use the following data [Datasets list](https://github.com/DataTalksClub/nyc-tlc-data/)\n* Yellow taxi data - Years 2019 and 2020\n* Green taxi data - Years 2019 and 2020 \n* fhv data - Year 2019. \n\nWe will use the data loaded for:\n\n* Building a source table: `stg_fhv_tripdata`\n* Building a fact table: `fact_fhv_trips`\n* Create a dashboard \n\nIf you don't have access to GCP, you can do this locally using the ingested data from your Postgres database\ninstead. If you have access to GCP, you don't need to do it for local Postgres -\nonly if you want to.\n\n> **Note**: if your answer doesn't match exactly, select the closest option \n\n### Question 1: \n\n**What is the count of records in the model fact_trips after running all models with the test run variable disabled and filtering for 2019 and 2020 data only (pickup datetime)?** \n\nYou'll need to have completed the [\"Build the first dbt models\"](https://www.youtube.com/watch?v=UVI30Vxzd6c) video and have been able to run the models via the CLI. \nYou should find the views and models for querying in your DWH.\n\n- 41648442\n- 51648442\n- 61648442\n- 71648442\n\n\n### Question 2: \n\n**What is the distribution between service type filtering by years 2019 and 2020 data as done in the videos?**\n\nYou will need to complete \"Visualising the data\" videos, either using [google data studio](https://www.youtube.com/watch?v=39nLTs74A3E) or [metabase](https://www.youtube.com/watch?v=BnLkrA7a6gM). \n\n- 89.9/10.1\n- 94/6\n- 76.3/23.7\n- 99.1/0.9\n\n\n\n### Question 3: \n\n**What is the count of records in the model stg_fhv_tripdata after running all models with the test run variable disabled (:false)?**  \n\nCreate a staging model for the fhv data for 2019 and do not add a deduplication step. Run it via the CLI without limits (is_test_run: false).\nFilter records with pickup time in year 2019.\n\n- 33244696\n- 43244696\n- 53244696\n- 63244696\n\n\n### Question 4: \n\n**What is the count of records in the model fact_fhv_trips after running all dependencies with the test run variable disabled (:false)?**  \n\nCreate a core model for the stg_fhv_tripdata joining with dim_zones.\nSimilar to what we've done in fact_trips, keep only records with known pickup and dropoff locations entries for pickup and dropoff locations. \nRun it via the CLI without limits (is_test_run: false) and filter records with pickup time in year 2019.\n\n- 12998722\n- 22998722\n- 32998722\n- 42998722\n\n### Question 5: \n\n**What is the month with the biggest amount of rides after building a tile for the fact_fhv_trips table?**\n\nCreate a dashboard with some tiles that you find interesting to explore the data. One tile should show the amount of trips per month, as done in the videos for fact_trips, based on the fact_fhv_trips table.\n\n- March\n- April\n- January\n- December\n\n\n\n## Submitting the solutions\n\n* Form for submitting: https://forms.gle/6A94GPutZJTuT5Y16\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 25 February (Saturday), 22:00 CET\n\n\n## Solution\n\n* Video: https://www.youtube.com/watch?v=I_K0lNu9WQw&list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW\n* Answers:\n  * Question 1: 61648442,\n  * Question 2: 89.9/10.1\n  * Question 3: 43244696\n  * Question 4: 22998722\n  * Question 5: January",
    "filename": "cohorts/2023/week_4_analytics_engineering/homework.md"
  },
  {
    "content": "## Week 5 Homework \n\nIn this homework we'll put what we learned about Spark in practice.\n\nFor this homework we will be using the FHVHV 2021-06 data found here. [FHVHV Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz )\n\n\n### Question 1: \n\n**Install Spark and PySpark** \n\n- Install Spark\n- Run PySpark\n- Create a local spark session\n- Execute spark.version.\n\nWhat's the output?\n- 3.3.2\n- 2.1.4\n- 1.2.3\n- 5.4\n</br></br>\n\n\n### Question 2: \n\n**HVFHW June 2021**\n\nRead it with Spark using the same schema as we did in the lessons.</br> \nWe will use this dataset for all the remaining questions.</br>\nRepartition it to 12 partitions and save it to parquet.</br>\nWhat is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.</br>\n\n\n- 2MB\n- 24MB\n- 100MB\n- 250MB\n</br></br>\n\n\n### Question 3: \n\n**Count records**  \n\nHow many taxi trips were there on June 15?</br></br>\nConsider only trips that started on June 15.</br>\n\n- 308,164\n- 12,856\n- 452,470\n- 50,982\n</br></br>\n\n\n### Question 4: \n\n**Longest trip for each day**  \n\nNow calculate the duration for each trip.</br>\nHow long was the longest trip in Hours?</br>\n\n- 66.87 Hours\n- 243.44 Hours\n- 7.68 Hours\n- 3.32 Hours\n</br></br>\n\n### Question 5: \n\n**User Interface**\n\n Spark\u2019s User Interface which shows application's dashboard runs on which local port?</br>\n\n- 80\n- 443\n- 4040\n- 8080\n</br></br>\n\n\n### Question 6: \n\n**Most frequent pickup location zone**\n\nLoad the zone lookup data into a temp view in Spark</br>\n[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)</br>\n\nUsing the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?</br>\n\n- East Chelsea\n- Astoria\n- Union Sq\n- Crown Heights North\n</br></br>\n\n\n\n\n## Submitting the solutions\n\n* Form for submitting: https://forms.gle/EcSvDs6vp64gcGuD8\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 06 March (Monday), 22:00 CET\n\n\n## Solution\n\n* Video: https://www.youtube.com/watch?v=ldoDIT32pJs\n* Answers:\n  * Question 1: 3.3.2\n  * Question 2: 24MB\n  * Question 3: 452,470\n  * Question 4: 66.87 Hours\n  * Question 5: 4040\n  * Question 6: Crown Heights North",
    "filename": "cohorts/2023/week_5_batch_processing/homework.md"
  },
  {
    "content": "## Week 6 Homework \n\nIn this homework, there will be two sections, the first session focus on theoretical questions related to Kafka \nand streaming concepts and the second session asks to create a small streaming application using preferred \nprogramming language (Python or Java).\n\n### Question 1: \n\n**Please select the statements that are correct**\n\n- Kafka Node is responsible to store topics [x]\n- Zookeeper is removed from Kafka cluster starting from version 4.0 [x]\n- Retention configuration ensures the messages not get lost over specific period of time. [x]\n- Group-Id ensures the messages are distributed to associated consumers [x]\n\n\n### Question 2: \n\n**Please select the Kafka concepts that support reliability and availability**\n\n- Topic Replication [x]\n- Topic Partioning\n- Consumer Group Id\n- Ack All [x]\n\n\n\n### Question 3: \n\n**Please select the Kafka concepts that support scaling**  \n\n- Topic Replication\n- Topic Paritioning [x]\n- Consumer Group Id [x]\n- Ack All\n\n\n### Question 4: \n\n**Please select the attributes that are good candidates for partitioning key. \nConsider cardinality of the field you have selected and scaling aspects of your application**  \n\n- payment_type [x]\n- vendor_id [x]\n- passenger_count\n- total_amount\n- tpep_pickup_datetime\n- tpep_dropoff_datetime\n\n\n### Question 5: \n\n**Which configurations below should be provided for Kafka Consumer but not needed for Kafka Producer**\n\n- Deserializer Configuration [x]\n- Topics Subscription [x]\n- Bootstrap Server \n- Group-Id [x]\n- Offset [x]\n- Cluster Key and Cluster-Secret\n\n\n### Question 6:\n\nPlease implement a streaming application, for finding out popularity of PUlocationID across green and fhv trip datasets.\nPlease use the datasets [fhv_tripdata_2019-01.csv.gz](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/fhv) \nand [green_tripdata_2019-01.csv.gz](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/green)\n\nPS: If you encounter memory related issue, you can use the smaller portion of these two datasets as well, \nit is not necessary to find exact number in the  question.\n\nYour code should include following\n1. Producer that reads csv files and publish rides in corresponding kafka topics (such as rides_green, rides_fhv)\n2. Pyspark-streaming-application that reads two kafka topics\n   and writes both of them in topic rides_all and apply aggregations to find most popular pickup location.\n\n   \n## Submitting the solutions\n\n* Form for submitting: https://forms.gle/rK7268U92mHJBpmW7\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 13 March (Monday), 22:00 CET\n\n\n## Solution\n\nWe will publish the solution here after deadline#\n\nFor Question 6 ensure, \n\n1) Download fhv_tripdata_2019-01.csv and green_tripdata_2019-01.csv under resources/fhv_tripdata \nand resources/green_tripdata resprctively. ps: You need to unzip the compressed files\n\n2) Update the client.properties settings using your Confluent Cloud api keys and cluster. \n3) And create the topics(all_rides, fhv_taxi_rides, green_taxi_rides) in Confluent Cloud UI\n\n4) Run Producers for two datasets\n```\npython3 producer_confluent --type green\npython3 producer_confluent --type fhv\n```\n\n5) Run pyspark streaming\n```\n./spark-submit.sh streaming_confluent.py\n```",
    "filename": "cohorts/2023/week_6_stream_processing/homework.md"
  },
  {
    "code": false,
    "content": "# RideCSVProducer\n\nThis script is a Kafka producer that reads ride data from CSV files and publishes the relevant information to a designated Kafka topic. It leverages the Confluent Kafka library to manage the Kafka producer functionality, and allows for different types of ride data (Green and FHV) to be processed and sent. \n\n## Libraries and Imports\n\nThe script begins by importing essential libraries:\n\n- **confluent_kafka**: A Python client for Apache Kafka that allows message production and consumption.\n- **argparse**: A module for parsing command-line arguments, enabling users to specify input parameters when running the script.\n- **csv**: A module for reading and writing CSV files, which is integral for processing the ride data.\n- **typing**: Provides type hints, specifically for the `Dict` data type to specify the structure of input parameters.\n- **time**: Used for adding delays (with `sleep`).\n- **settings**: This module (assumed to exist) contains configurations such as Kafka connection details and file paths.\n\n## RideCSVProducer Class\n\n### Initialization\n\nThe core class defined in the script is `RideCSVProducer`, which is initialized with:\n\n- **probs (Dict)**: A dictionary with the Kafka producer configuration parameters.\n- **ride_type (str)**: The type of ride data being processed (either 'green' or 'fhv').\n\nWithin the constructor, a Kafka producer instance is created using the provided configuration, and the ride type is stored for later processing.\n\n### Parsing CSV Rows\n\nThe `parse_row` method is responsible for interpreting individual rows of the CSV files based on the ride type:\n\n- For 'green' rides, it extracts the `PULocationID` (pickup) and `DOLocationID` (drop-off) from specific columns and uses `vendor_id` as the key.\n- For 'fhv' (for-hire vehicle) rides, it retrieves values from different columns but similarly constructs a record of `PULocationID` and `DOLocationID`, with `dispatching_base_num` used as the key.\n\nThis method returns a tuple containing the key and the formatted record.\n\n### Reading Records\n\nThe `read_records` method reads the CSV file specified by `resource_path`, skipping the header, and processes each row using `parse_row`. It accumulates keys and records in two lists, which are then zipped together and returned as a list of tuples (key-value pairs).\n\n### Publishing Data\n\nThe `publish` method sends the key-value pairs to the specified Kafka topic. It iterates over the records, attempting to produce each key-value pair:\n\n- It uses `poll(0)` to maintain the internal state of the producer.\n- The `produce` method is called to send the data to the Kafka topic.\n- Several exceptions are handled to ensure graceful error management:\n  - `KeyboardInterrupt` allows the user to stop production with a keyboard interrupt.\n  - `BufferError` indicates when the producer's buffer is full, prompting a short pause (`poll(0.1)`) to allow processing.\n  - A general exception handles any other unexpected issues.\n\nAfter all records are sent, `flush` ensures that any remaining messages are transmitted, and the script sleeps for 10 seconds before concluding.\n\n## Main Execution\n\nThe script's entry point starts by setting up an `ArgumentParser` to accept a `--type` argument, which determines whether to process 'green' or 'fhv' rides. Based on this argument, it sets the appropriate Kafka topic and data file path for the rides.\n\nWith this information, a `RideCSVProducer` instance is created, and the script reads the corresponding records from the specified CSV file. Finally, it calls the `publish` method to send these records to the designated Kafka topic.\n\n## Summary\n\nIn summary, this script is a Kafka producer for ride data that:\n\n1. Imports necessary libraries and settings.\n2. Defines a `RideCSVProducer` class responsible for handling the parsing of CSV rows, reading the CSV file contents, and publishing the records to a Kafka topic.\n3. Uses command-line arguments to determine the type of ride being processed, adjusts the topic and file path accordingly, and executes the necessary methods to read and publish the ride data. \n\nThis structured approach allows for flexible integration with Kafka while maintaining clean and manageable code.",
    "filename": "cohorts/2023/week_6_stream_processing/producer_confluent.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThe provided source code is a Python script that appears to be part of a data processing application utilizing PySpark for handling taxi ride data. It defines constants for file paths and Kafka topics, sets a schema for taxi ride data, and includes a function to read configuration settings from a properties file.\n\n## Constants\n### File Paths\n- `GREEN_TRIP_DATA_PATH`: This constant stores the file path to a CSV file containing green taxi trip data for January 2019.\n- `FHV_TRIP_DATA_PATH`: This constant holds the file path to a CSV file for For-Hire Vehicle (FHV) trip data, also for January 2019.\n\n### Kafka Configuration\n- `BOOTSTRAP_SERVERS`: This constant defines the address of the Kafka server. In this instance, it points to a locally hosted server on port 9092.\n  \n### Topic Names\n- `RIDES_TOPIC`: This variable represents the Kafka topic named 'all_rides,' which is likely intended to consolidate both FHV and green taxi ride data.\n- `FHV_TAXI_TOPIC`: This variable points to the Kafka topic specifically for For-Hire Vehicle rides.\n- `GREEN_TAXI_TOPIC`: This variable signifies the Kafka topic for green taxi rides.\n\n## Schema Definition\n### ALL_RIDE_SCHEMA\n- `ALL_RIDE_SCHEMA`: This variable defines the schema for a DataFrame containing ride data using `pyspark.sql.types`. It specifies two fields: \n  - `PUlocationID`: Expected to be a string representation of the ID where the ride was picked up.\n  - `DOlocationID`: Expected to be a string representation of the ID where the ride was dropped off.\n\nThis schema is important for validating the structure of the ride data as it is read into Spark.\n\n## Function Definition\n### `read_ccloud_config(config_file)`\nThis function is designed to read configurations from a specified properties file (`config_file`). \n\n#### Parameters\n- `config_file`: A string specifying the path to the configuration file.\n\n#### Implementation Steps\n1. An empty dictionary named `conf` is initialized to store configuration parameters.\n2. The file is opened for reading; each line is processed individually:\n   - Whitespace is stripped, and comments (lines that start with `#`) are ignored.\n   - Each valid line is expected to contain a `parameter=value` format.\n   - The parameter and its corresponding value are extracted and added to the `conf` dictionary.\n3. After processing all lines, the dictionary containing the configuration settings is returned.\n\n### Configuration Variable\n- `CONFLUENT_CLOUD_CONFIG`: This variable holds the configuration settings read from a file called `client_original.properties`. This might be used later in the application to connect to a cloud-based service, potentially related to the Kafka setup.\n\n## Summary\nThe script sets up foundational components for a data processing workflow involving taxi ride data using PySpark and Kafka. It establishes constants for file paths, configures Kafka topic names, defines a schema for ride data, and includes a utility function for reading configuration settings. This modular approach allows for easy adjustments to configuration parameters and prepares the environment for further data processing steps, which may include reading the CSV files and publishing messages to Kafka topics based on the defined schemas.",
    "filename": "cohorts/2023/week_6_stream_processing/settings.py"
  },
  {
    "code": false,
    "content": "# Spark Streaming Application for Taxi Rides Processing\n\nThis documentation outlines the functionality of a Spark Streaming application that processes taxi ride information from Kafka topics. The application reads from specific topics, parses the data, performs aggregations, and then publishes results back to Kafka or outputs to the console.\n\n## Overview of the Components\n\nThe application consists of several functions and a main execution block, which orchestrates the flow of data from Kafka, processes it, and outputs results. The main components are:\n\n- **Reading from Kafka**: Functions to consume data from Kafka topics.\n- **Data Parsing**: Functions to transform and enrich the data according to a predefined schema.\n- **Data Sinking**: Functions to output processed data either to the console or back into Kafka.\n- **Aggregation**: A function to perform group-by operations on the data.\n\n## Functions\n\n### `read_from_kafka(consume_topic: str)`\n\nThis function establishes a connection to a specified Kafka topic to read streaming data. It configures the necessary options for secure communication with Confluent Cloud and specifies parameters such as the starting offsets and checkpoint location.\n\n- **Parameters**:\n  - `consume_topic`: The Kafka topic from which to read data.\n\n- **Returns**: A Spark Streaming DataFrame that will receive streaming data from the specified Kafka topic.\n\n### `parse_rides(df, schema)`\n\nThis function is responsible for transforming the raw streaming DataFrame by casting the key and value columns to strings and splitting the value column based on a predefined schema. It ensures that the DataFrame conforms to the expected structure and type, handling data types according to the schema.\n\n- **Parameters**:\n  - `df`: A Spark DataFrame that is streaming data.\n  - `schema`: A predefined schema that describes the structure of the data.\n\n- **Returns**: A structured Spark Streaming DataFrame containing the parsed columns.\n\n### `sink_console(df, output_mode: str = 'complete', processing_time: str = '5 seconds')`\n\nThis function is used to output the processed streaming DataFrame to the console for monitoring or debugging purposes. It utilizes Spark's writeStream capabilities to define the output format and frequency with which data is printed.\n\n- **Parameters**:\n  - `df`: The DataFrame containing data to be output.\n  - `output_mode`: The way data is outputted (default is 'complete').\n  - `processing_time`: Time interval between streaming queries (default is '5 seconds').\n\n- **Returns**: A StreamingQuery object representing the execution of the streaming query.\n\n### `sink_kafka(df, topic, output_mode: str = 'complete')`\n\nThis function outputs a Spark Streaming DataFrame to a specified Kafka topic. Similar to the console sink, it sets up parameters for secure communication and a checkpoint location while allowing customization of output mode.\n\n- **Parameters**:\n  - `df`: The DataFrame to be written to Kafka.\n  - `topic`: The Kafka topic where the data will be published.\n  - `output_mode`: The output mode for the streaming DataFrame (default is 'complete').\n\n- **Returns**: A StreamingQuery object managing the streaming process.\n\n### `op_groupby(df, column_names)`\n\nThis function performs a grouping operation on the DataFrame based on specified column names and counts the occurrences of each group. This is essential for generating aggregates based on specific features from the ride data.\n\n- **Parameters**:\n  - `df`: The DataFrame to be aggregated.\n  - `column_names`: A list of column names to group by.\n\n- **Returns**: A DataFrame containing the counts of groups.\n\n## Main Execution Flow\n\n### Step 1: Consume Data from Kafka\n\nThe execution begins with creating a Spark session and setting the log level to 'WARN'. It then reads data from two Kafka topics \u2014 `GREEN_TAXI_TOPIC` and `FHV_TAXI_TOPIC`. Each topic produces a streaming DataFrame that will be processed further.\n\n```python\ndf_green_rides = read_from_kafka(consume_topic=GREEN_TAXI_TOPIC)\ndf_fhv_rides = read_from_kafka(consume_topic=FHV_TAXI_TOPIC)\n```\n\n### Step 2: Publish Data back to Kafka\n\nThe next step involves sending the streamed data read from the green and FHV rides back to a unified Kafka topic named `RIDES_TOPIC`. This helps in centralizing the data for further processing.\n\n```python\nkafka_sink_green_query = sink_kafka(df=df_green_rides, topic=RIDES_TOPIC, output_mode='append')\nkafka_sink_fhv_query = sink_kafka(df=df_fhv_rides, topic=RIDES_TOPIC, output_mode='append')\n```\n\n### Step 3: Read and Parse Data from `RIDES_TOPIC`\n\nAfter publishing the rides to the `RIDES_TOPIC`, the application reads the data from this topic. The data is parsed using the predefined schema `ALL_RIDE_SCHEMA` to convert it into a structured format.\n\n```python\ndf_all_rides = read_from_kafka(consume_topic=RIDES_TOPIC)\ndf_all_rides = parse_rides(df_all_rides, ALL_RIDE_SCHEMA)\n```\n\n### Step 4: Perform Aggregation\n\nSubsequently, the application performs a grouping operation to count the number of rides by pickup location ID (`PULocationID`). The results are sorted in descending order based on the count of rides.\n\n```python\ndf_pu_location_count = op_groupby(df_all_rides, ['PULocationID'])\ndf_pu_location_count = df_pu_location_count.sort(F.col('count').desc())\n```\n\n### Step 5: Output Aggregated Data\n\nFinally, the aggregated results are sent to the console for monitoring. This step aids in real-time observation of the processed data counts.\n\n```python\nconsole_sink_pu_location = sink_console(df_pu_location_count, output_mode='complete')\n```\n\n## Conclusion\n\nThis Spark Streaming application effectively processes taxi ride data from multiple Kafka topics, performs aggregations, and provides output both to the console and back to Kafka. The use of structured streaming ensures real-time data processing with robust handling of schema and data types.",
    "filename": "cohorts/2023/week_6_stream_processing/streaming_confluent.py"
  },
  {
    "content": "## Workshop: Maximizing Confidence in Your Data Model Changes with dbt and PipeRider\n\nTo learn how to use PipeRider together with dbt for detecting changes in model and data, sign up for a workshop\n\n- Video: https://www.youtube.com/watch?v=O-tyUOQccSs\n- Repository: https://github.com/InfuseAI/taxi_rides_ny_duckdb\n\n\n## Homework\n\nThe following questions follow on from the original Week 4 homework, and so use the same data as required by those questions:\n\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2023/week_4_analytics_engineering/homework.md\n\nYellow taxi data - Years 2019 and 2020\nGreen taxi data - Years 2019 and 2020\nfhv data - Year 2019.\n\n### Question 1:\n\nWhat is the distribution between vendor id filtering by years 2019 and 2020 data?\n\nYou will need to run PipeRider and check the report\n\n* 70.1/29.6/0.5\n* 60.1/39.5/0.4\n* 90.2/9.5/0.3\n* 80.1/19.7/0.2\n\n### Question 2:\n\nWhat is the composition of total amount (positive/zero/negative) filtering by years 2019 and 2020 data?\n\nYou will need to run PipeRider and check the report\n\n\n* 51.4M/15K/48.6K\n* 21.4M/5K/248.6K\n* 61.4M/25K/148.6K\n* 81.4M/35K/14.6K\n\n### Question 3:\n\nWhat is the numeric statistics (average/standard deviation/min/max/sum) of trip distances filtering by years 2019 and 2020 data?\n\nYou will need to run PipeRider and check the report\n\n\n* 1.95/35.43/0/16.3K/151.5M\n* 3.95/25.43/23.88/267.3K/281.5M\n* 5.95/75.43/-63.88/67.3K/81.5M\n* 2.95/35.43/-23.88/167.3K/181.5M\n\n\n\n## Submitting the solutions\n\n* Form for submitting: https://forms.gle/WyLQHBu1DNwNTfqe8\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 20 March, 22:00 CET\n\n\n## Solution\n\nVideo: https://www.youtube.com/watch?v=inNrUys7W8U&list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW",
    "filename": "cohorts/2023/workshops/piperider.md"
  },
  {
    "content": "## Module 1 Homework\n\nATTENTION: At the very end of the submission form, you will be required to include a link to your GitHub repository or other public code-hosting site. This repository should contain your code for solving the homework. If your solution includes code that is not in file format (such as SQL queries or shell commands), please include these directly in the README file of your repository.\n\n## Docker & SQL\n\nIn this homework we'll prepare the environment \nand practice with Docker and SQL\n\n\n## Question 1. Knowing docker tags\n\nRun the command to get information on Docker \n\n```docker --help```\n\nNow run the command to get help on the \"docker build\" command:\n\n```docker build --help```\n\nDo the same for \"docker run\".\n\nWhich tag has the following text? - *Automatically remove the container when it exits* \n\n- `--delete`\n- `--rc`\n- `--rmc`\n- `--rm`\n\n\n## Question 2. Understanding docker first run \n\nRun docker with the python:3.9 image in an interactive mode and the entrypoint of bash.\nNow check the python modules that are installed ( use ```pip list``` ). \n\nWhat is version of the package *wheel* ?\n\n- 0.42.0\n- 1.0.0\n- 23.0.1\n- 58.1.0\n\n\n# Prepare Postgres\n\nRun Postgres and load data as shown in the videos\nWe'll use the green taxi trips from September 2019:\n\n```wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz```\n\nYou will also need the dataset with zones:\n\n```wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv```\n\nDownload this data and put it into Postgres (with jupyter notebooks or with a pipeline)\n\n\n## Question 3. Count records \n\nHow many taxi trips were totally made on September 18th 2019?\n\nTip: started and finished on 2019-09-18. \n\nRemember that `lpep_pickup_datetime` and `lpep_dropoff_datetime` columns are in the format timestamp (date and hour+min+sec) and not in date.\n\n- 15767\n- 15612\n- 15859\n- 89009\n\n## Question 4. Longest trip for each day\n\nWhich was the pick up day with the longest trip distance?\nUse the pick up time for your calculations.\n\nTip: For every trip on a single day, we only care about the trip with the longest distance. \n\n- 2019-09-18\n- 2019-09-16\n- 2019-09-26\n- 2019-09-21\n\n\n## Question 5. Three biggest pick up Boroughs\n\nConsider lpep_pickup_datetime in '2019-09-18' and ignoring Borough has Unknown\n\nWhich were the 3 pick up Boroughs that had the maximum total_amount?\n \n- \"Brooklyn\" \"Manhattan\" \"Queens\"\n- \"Bronx\" \"Brooklyn\" \"Manhattan\"\n- \"Bronx\" \"Manhattan\" \"Queens\" \n- \"Brooklyn\" \"Queens\" \"Staten Island\"\n\n\n## Question 6. Largest tip\n\nFor the passengers picked up in September 2019 in the zone name Astoria which was the drop off zone that had the largest tip?\nWe want the name of the zone, not the id.\n\nNote: it's not a typo, it's `tip` , not `trip`\n\n- Central Park\n- Jamaica\n- JFK Airport\n- Long Island City/Queens Plaza\n\n\n\n## Terraform\n\nIn this section homework we'll prepare the environment by creating resources in GCP with Terraform.\n\nIn your VM on GCP/Laptop/GitHub Codespace install Terraform. \nCopy the files from the course repo\n[here](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/01-docker-terraform/1_terraform_gcp/terraform) to your VM/Laptop/GitHub Codespace.\n\nModify the files as necessary to create a GCP Bucket and Big Query Dataset.\n\n\n## Question 7. Creating Resources\n\nAfter updating the main.tf and variable.tf files run:\n\n```\nterraform apply\n```\n\nPaste the output of this command into the homework submission form.\n\n\n## Submitting the solutions\n\n* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/hw01\n* You can submit your homework multiple times. In this case, only the last submission will be used. \n\nDeadline: 29 January, 23:00 CET",
    "filename": "cohorts/2024/01-docker-terraform/homework.md"
  },
  {
    "content": "## Question 1. Knowing docker tags\n```\n\u276f docker run --help | grep \"Automatically remove\"\n--rm                               Automatically remove\n```\n\n- `|` pipe operator redirects the previous command output as an input to the command after the operator\n- `docker run --help` -----> outputs `|` ---------> inputs to `grep \"Automatically remove\"`\n- `grep` allows you to search through text\n  \nAnswer: `--rm`\n\n\n## Question 2. Understanding docker first run\n\n- Run python:3.9 image with `docker run -it python:3.9 bash`\n- Since you opened with `it` tag, the container will be interactive`\n- Since the docker command ends with `bash`, the entrypoint into the container will be `bash`\n\n```shell\nroot@root: docker run -it python:3.9 bash\nroot@b67c6949422a:/# pip list\nPackage    Version\n---------- -------\npip        23.0.1\nsetuptools 58.1.0\nwheel      0.45.1\n```\n\nSince it's been a while since 2024 cohort, your wheel version might differ and may not be in the options provided.\n\nAnswer: For me it was `0.45.1`\n\n\n## Question 3. Count records\n\n- Trips that started and finished on 2019-09-18\n- Format timestamp(date and hour+min+sec) to date.\n\n```sql\nSELECT COUNT(*) FROM \"csv_green_tripdata_2019_09\"\nWHERE DATE(\"lpep_pickup_datetime\") = '2019-09-18' AND\n      DATE(\"lpep_dropoff_datetime\") = '2019-09-18';\n```\n```\n+-------+\n| count |\n|-------|\n| 15612 |\n+-------+\n```\n\nAnswer: `15612`\n\n\n## Question 4. Longest trip for each day\n```sql\nSELECT\n    DATE(\"lpep_pickup_datetime\") AS \"pickup_date\",\n    MAX(\"trip_distance\") AS \"longest_trip\"\nFROM\n    \"csv_green_tripdata_2019_09\"\nGROUP BY\n    DATE(\"lpep_pickup_datetime\")\nORDER BY\n    \"longest_trip\" DESC\nLIMIT 1;\n```\n```\n+-------------+--------------+\n| pickup_date | longest_trip |\n|-------------+--------------|\n| 2019-09-26  | 341.64       |\n+-------------+--------------+\n```\n\nAnswer: `2019-09-26`\n\n\n## Question 5. Three biggest pickup zones\n```sql\nSELECT\n    \"zone\".\"Zone\",\n    ROUND(SUM((\"total_amount\")::NUMERIC), 3) AS \"total_amount\"\nFROM\n    \"csv_green_tripdata_2019_09\"\nINNER JOIN\n    \"zone\" ON \"csv_green_tripdata_2019_09\".\"PULocationID\" = \"zone\".\"LocationID\"\nWHERE\n    DATE(\"lpep_pickup_datetime\") = '2019-09-18'\nGROUP BY\n    \"zone\".\"Zone\"\nORDER BY\n    \"total_amount\" DESC\nLIMIT 3;\n```\n```\n+---------------------+--------------+\n| Zone                | total_amount |\n|---------------------+--------------|\n| East Harlem North   | 17893.060    |\n| East Harlem South   | 17152.160    |\n| Morningside Heights | 11259.680    |\n+---------------------+--------------+\n```\n\nAnswer: `East Harlem North, East Harlem South, Morningside Heights`\n\n\n## Question 6. Largest tip\n```sql\nSELECT\n    puz.\"Zone\" AS pickup_zone,\n    doz.\"Zone\" AS dropoff_zone,\n    g.\"tip_amount\"\nFROM\n    \"csv_green_tripdata_2019_09\" g\nINNER JOIN\n    \"zone\" puz ON g.\"PULocationID\" = puz.\"LocationID\"\nINNER JOIN\n    \"zone\" doz ON g.\"DOLocationID\" = doz.\"LocationID\"\nWHERE\n    puz.\"Zone\" = 'Astoria'\nORDER BY\n    g.\"tip_amount\" DESC\nLIMIT 1;\n```\n\n```\n+-------------+--------------+------------+\n| pickup_zone | dropoff_zone | tip_amount |\n|-------------+--------------+------------|\n| Astoria     | JFK Airport  | 62.31      |\n+-------------+--------------+------------+\n```\n\nAnswer: `JFK Airport`\n\n\n## Question 7. Terraform Workflow\n\n> self-explanatory",
    "filename": "cohorts/2024/01-docker-terraform/solutions.md"
  },
  {
    "content": "> [!NOTE]  \n>If you're looking for Airflow videos from the 2022 edition, check the [2022 cohort folder](../cohorts/2022/week_2_data_ingestion/). \n>\n>If you're looking for Prefect videos from the 2023 edition, check the [2023 cohort folder](../cohorts/2023/week_2_data_ingestion/).\n\n# Week 2: Workflow Orchestration\n\nWelcome to Week 2 of the Data Engineering Zoomcamp! \ud83d\ude80\ud83d\ude24 This week, we'll be covering workflow orchestration with Mage.\n\nMage is an open-source, hybrid framework for transforming and integrating data. \u2728\n\nThis week, you'll learn how to use the Mage platform to author and share _magical_ data pipelines. This will all be covered in the course, but if you'd like to learn a bit more about Mage, check out our docs [here](https://docs.mage.ai/introduction/overview). \n\n* [2.2.1 - \ud83d\udcef Intro to Orchestration](#221----intro-to-orchestration)\n* [2.2.2 - \ud83e\uddd9\u200d\u2642\ufe0f Intro to Mage](#222---%EF%B8%8F-intro-to-mage)\n* [2.2.3 - \ud83d\udc18 ETL: API to Postgres](#223----etl-api-to-postgres)\n* [2.2.4 - \ud83e\udd13 ETL: API to GCS](#224----etl-api-to-gcs)\n* [2.2.5 - \ud83d\udd0d ETL: GCS to BigQuery](#225----etl-gcs-to-bigquery)\n* [2.2.6 - \ud83d\udc68\u200d\ud83d\udcbb Parameterized Execution](#226----parameterized-execution)\n* [2.2.7 - \ud83e\udd16 Deployment (Optional)](#227----deployment-optional)\n* [2.2.8 - \ud83d\uddd2\ufe0f Homework](#228---\ufe0f-homework)\n* [2.2.9 - \ud83d\udc63 Next Steps](#229----next-steps)\n\n## \ud83d\udcd5 Course Resources\n\n### 2.2.1 - \ud83d\udcef Intro to Orchestration\n\nIn this section, we'll cover the basics of workflow orchestration. We'll discuss what it is, why it's important, and how it can be used to build data pipelines.\n\nVideos\n- 2.2.1a - What is Orchestration?\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/Li8-MWHhTbo)](https://youtu.be/Li8-MWHhTbo&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=17)\n\nResources\n- [Slides](https://docs.google.com/presentation/d/17zSxG5Z-tidmgY-9l7Al1cPmz4Slh4VPK6o2sryFYvw/)\n\n### 2.2.2 - \ud83e\uddd9\u200d\u2642\ufe0f Intro to Mage\n\nIn this section, we'll introduce the Mage platform. We'll cover what makes Mage different from other orchestrators, the fundamental concepts behind Mage, and how to get started. To cap it off, we'll spin Mage up via Docker \ud83d\udc33 and run a simple pipeline.\n\nVideos\n- 2.2.2a - What is Mage?\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/AicKRcK3pa4)](https://youtu.be/AicKRcK3pa4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=18)\n\n- 2.2.2b - Configuring Mage\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/tNiV7Wp08XE)](https://youtu.be/tNiV7Wp08XE&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=19)\n\n- 2.2.2c - A Simple Pipeline\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/stI-gg4QBnI)](https://youtu.be/stI-gg4QBnI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=20)\n\nResources\n- [Getting Started Repo](https://github.com/mage-ai/mage-zoomcamp)\n- [Slides](https://docs.google.com/presentation/d/1y_5p3sxr6Xh1RqE6N8o2280gUzAdiic2hPhYUUD6l88/)\n\n### 2.2.3 - \ud83d\udc18 ETL: API to Postgres\n\nHooray! Mage is up and running. Now, let's build a _real_ pipeline. In this section, we'll build a simple ETL pipeline that loads data from an API into a Postgres database. Our database will be built using Docker\u2014 it will be running locally, but it's the same as if it were running in the cloud.\n\nVideos\n- 2.2.3a - Configuring Postgres\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/pmhI-ezd3BE)](https://youtu.be/pmhI-ezd3BE&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=21)\n\n- 2.2.3b - Writing an ETL Pipeline : API to postgres\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/Maidfe7oKLs)](https://youtu.be/Maidfe7oKLs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=22)\n\n\n### 2.2.4 - \ud83e\udd13 ETL: API to GCS\n\nOk, so we've written data _locally_ to a database, but what about the cloud? In this tutorial, we'll walk through the process of using Mage to extract, transform, and load data from an API to Google Cloud Storage (GCS). \n\nWe'll cover both writing _partitioned_ and _unpartitioned_ data to GCS and discuss _why_ you might want to do one over the other. Many data teams start with extracting data from a source and writing it to a data lake _before_ loading it to a structured data source, like a database.\n\nVideos\n- 2.2.4a - Configuring GCP\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/00LP360iYvE)](https://youtu.be/00LP360iYvE&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=23)\n\n- 2.2.4b - Writing an ETL Pipeline : API to GCS\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/w0XmcASRUnc)](https://youtu.be/w0XmcASRUnc&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=24)\n\nResources\n- [DTC Zoomcamp GCP Setup](../01-docker-terraform/1_terraform_gcp/2_gcp_overview.md)\n\n### 2.2.5 - \ud83d\udd0d ETL: GCS to BigQuery\n\nNow that we've written data to GCS, let's load it into BigQuery. In this section, we'll walk through the process of using Mage to load our data from GCS to BigQuery. This closely mirrors a very common data engineering workflow: loading data from a data lake into a data warehouse.\n\nVideos\n- 2.2.5a - Writing an ETL Pipeline : GCS to BigQuery\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/JKp_uzM-XsM)](https://youtu.be/JKp_uzM-XsM&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=25)\n\n### 2.2.6 - \ud83d\udc68\u200d\ud83d\udcbb Parameterized Execution\n\nBy now you're familiar with building pipelines, but what about adding parameters? In this video, we'll discuss some built-in runtime variables that exist in Mage and show you how to define your own! We'll also cover how to use these variables to parameterize your pipelines. Finally, we'll talk about what it means to *backfill* a pipeline and how to do it in Mage.\n\nVideos\n- 2.2.6a - Parameterized Execution\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/H0hWjWxB-rg)](https://youtu.be/H0hWjWxB-rg&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=26)\n\n\n- 2.2.6b - Backfills\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/ZoeC6Ag5gQc)](https://youtu.be/ZoeC6Ag5gQc&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=27)\n\nResources\n- [Mage Variables Overview](https://docs.mage.ai/development/variables/overview)\n- [Mage Runtime Variables](https://docs.mage.ai/getting-started/runtime-variable)\n\n### 2.2.7 - \ud83e\udd16 Deployment (Optional)\n\nIn this section, we'll cover deploying Mage using Terraform and Google Cloud. This section is optional\u2014 it's not *necessary* to learn Mage, but it might be helpful if you're interested in creating a fully deployed project. If you're using Mage in your final project, you'll need to deploy it to the cloud.\n\nVideos\n- 2.2.7a - Deployment Prerequisites\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/zAwAX5sxqsg)](https://youtu.be/zAwAX5sxqsg&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=28)\n\n- 2.2.7b - Google Cloud Permissions\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/O_H7DCmq2rA)](https://youtu.be/O_H7DCmq2rA&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=29)\n\n- 2.2.7c - Deploying to Google Cloud - Part 1\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/9A872B5hb_0)](https://youtu.be/9A872B5hb_0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=30)\n\n- 2.2.7d - Deploying to Google Cloud - Part 2\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/0YExsb2HgLI)](https://youtu.be/0YExsb2HgLI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=31)\n\nResources\n- [Installing Terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)\n- [Installing `gcloud` CLI](https://cloud.google.com/sdk/docs/install)\n- [Mage Terraform Templates](https://github.com/mage-ai/mage-ai-terraform-templates)\n\nAdditional Mage Guides\n- [Terraform](https://docs.mage.ai/production/deploying-to-cloud/using-terraform)\n- [Deploying to GCP with Terraform](https://docs.mage.ai/production/deploying-to-cloud/gcp/setup)\n\n### 2.2.8 - \ud83d\uddd2\ufe0f Homework \n\nWe've prepared a short exercise to test you on what you've learned this week. You can find the homework [here](../cohorts/2024/02-workflow-orchestration/homework.md). This follows closely from the contents of the course and shouldn't take more than an hour or two to complete. \ud83d\ude04\n\n### 2.2.9 - \ud83d\udc63 Next Steps\n\nCongratulations! You've completed Week 2 of the Data Engineering Zoomcamp. We hope you've enjoyed learning about Mage and that you're excited to use it in your final project. If you have any questions, feel free to reach out to us on Slack. Be sure to check out our \"Next Steps\" video for some inspiration for the rest of your journey \ud83d\ude04.\n\nVideos\n- 2.2.9 - Next Steps\n\n[![](https://markdown-videos-api.jorgenkh.no/youtube/uUtj7N0TleQ)](https://youtu.be/uUtj7N0TleQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=32)\n\nResources\n- [Slides](https://docs.google.com/presentation/d/1yN-e22VNwezmPfKrZkgXQVrX5owDb285I2HxHWgmAEQ/edit#slide=id.g262fb0d2905_0_12)\n\n### \ud83d\udcd1 Additional Resources\n\n- [Mage Docs](https://docs.mage.ai/)\n- [Mage Guides](https://docs.mage.ai/guides)\n- [Mage Slack](https://www.mage.ai/chat)\n\n\n# Community notes\n\nDid you take notes? You can share them here:\n\n## 2024 notes\n\n* [2024 Videos transcripts week 2](https://drive.google.com/drive/folders/1yxT0uMMYKa6YOxanh91wGqmQUMS7yYW7?usp=sharing) by Maria Fisher\n* [Notes from Jonah Oliver](https://www.jonahboliver.com/blog/de-zc-w2)\n* [Notes from Linda](https://github.com/inner-outer-space/de-zoomcamp-2024/blob/main/2-workflow-orchestration/readme.md)\n* [Notes from Kirill](https://github.com/kirill505/data-engineering-zoomcamp/blob/main/02-workflow-orchestration/README.md)\n* [Notes from Zharko](https://www.zharconsulting.com/contents/data/data-engineering-bootcamp-2024/week-2-ingesting-data-with-mage/)\n* Add your notes above this line\n\n## 2023 notes\n\nSee [here](../cohorts/2023/week_2_workflow_orchestration#community-notes)\n\n\n## 2022 notes\n\nSee [here](../cohorts/2022/week_2_data_ingestion#community-notes)",
    "filename": "cohorts/2024/02-workflow-orchestration/README.md"
  },
  {
    "content": "## Module 2 Homework\n\nATTENTION: At the end of the submission form, you will be required to include a link to your GitHub repository or other public code-hosting site. This repository should contain your code for solving the homework. If your solution includes code that is not in file format, please include these directly in the README file of your repository.\n\n> In case you don't get one option exactly, select the closest one \n\nFor the homework, we'll be working with the _green_ taxi dataset located here:\n\n`https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/green/download`\n\nTo get a `wget`-able link, use this prefix (note that the link itself gives 404):\n\n`https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/`\n\n### Assignment\n\nThe goal will be to construct an ETL pipeline that loads the data, performs some transformations, and writes the data to a database (and Google Cloud!).\n\n- Create a new pipeline, call it `green_taxi_etl`\n- Add a data loader block and use Pandas to read data for the final quarter of 2020 (months `10`, `11`, `12`).\n  - You can use the same datatypes and date parsing methods shown in the course.\n  - `BONUS`: load the final three months using a for loop and `pd.concat`\n- Add a transformer block and perform the following:\n  - Remove rows where the passenger count is equal to 0 _and_ the trip distance is equal to zero.\n  - Create a new column `lpep_pickup_date` by converting `lpep_pickup_datetime` to a date.\n  - Rename columns in Camel Case to Snake Case, e.g. `VendorID` to `vendor_id`.\n  - Add three assertions:\n    - `vendor_id` is one of the existing values in the column (currently)\n    - `passenger_count` is greater than 0\n    - `trip_distance` is greater than 0\n- Using a Postgres data exporter (SQL or Python), write the dataset to a table called `green_taxi` in a schema `mage`. Replace the table if it already exists.\n- Write your data as Parquet files to a bucket in GCP, partioned by `lpep_pickup_date`. Use the `pyarrow` library!\n- Schedule your pipeline to run daily at 5AM UTC.\n\n### Questions\n\n## Question 1. Data Loading\n\nOnce the dataset is loaded, what's the shape of the data?\n\n* 266,855 rows x 20 columns\n* 544,898 rows x 18 columns\n* 544,898 rows x 20 columns\n* 133,744 rows x 20 columns\n\n## Question 2. Data Transformation\n\nUpon filtering the dataset where the passenger count is greater than 0 _and_ the trip distance is greater than zero, how many rows are left?\n\n* 544,897 rows\n* 266,855 rows\n* 139,370 rows\n* 266,856 rows\n\n## Question 3. Data Transformation\n\nWhich of the following creates a new column `lpep_pickup_date` by converting `lpep_pickup_datetime` to a date?\n\n* `data = data['lpep_pickup_datetime'].date`\n* `data('lpep_pickup_date') = data['lpep_pickup_datetime'].date`\n* `data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date`\n* `data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt().date()`\n\n## Question 4. Data Transformation\n\nWhat are the existing values of `VendorID` in the dataset?\n\n* 1, 2, or 3\n* 1 or 2\n* 1, 2, 3, 4\n* 1\n\n## Question 5. Data Transformation\n\nHow many columns need to be renamed to snake case?\n\n* 3\n* 6\n* 2\n* 4\n\n## Question 6. Data Exporting\n\nOnce exported, how many partitions (folders) are present in Google Cloud?\n\n* 96\n* 56\n* 67\n* 108\n\n## Submitting the solutions\n\n* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2\n* Check the link above to see the due date\n  \n## Solution\n\nWill be added after the due date",
    "filename": "cohorts/2024/02-workflow-orchestration/homework.md"
  },
  {
    "content": "## Module 3 Homework\n\nSolution: https://www.youtube.com/watch?v=8g_lRKaC9ro\n\nATTENTION: At the end of the submission form, you will be required to include a link to your GitHub repository or other public code-hosting site. This repository should contain your code for solving the homework. If your solution includes code that is not in file format (such as SQL queries or shell commands), please include these directly in the README file of your repository.\n\n<b><u>Important Note:</b></u> <p> For this homework we will be using the 2022 Green Taxi Trip Record Parquet Files from the New York\nCity Taxi Data found here: </br> https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page </br>\nIf you are using orchestration such as Mage, Airflow or Prefect do not load the data into Big Query using the orchestrator.</br> \nStop with loading the files into a bucket. </br></br>\n<u>NOTE:</u> You will need to use the PARQUET option files when creating an External Table</br>\n\n<b>SETUP:</b></br>\nCreate an external table using the Green Taxi Trip Records Data for 2022. </br>\nCreate a table in BQ using the Green Taxi Trip Records for 2022 (do not partition or cluster this table). </br>\n</p>\n\n## Question 1:\nQuestion 1: What is count of records for the 2022 Green Taxi Data??\n- 65,623,481\n- 840,402\n- 1,936,423\n- 253,647\n\n## Question 2:\nWrite a query to count the distinct number of PULocationIDs for the entire dataset on both the tables.</br> \nWhat is the estimated amount of data that will be read when this query is executed on the External Table and the Table?\n\n- 0 MB for the External Table and 6.41MB for the Materialized Table\n- 18.82 MB for the External Table and 47.60 MB for the Materialized Table\n- 0 MB for the External Table and 0MB for the Materialized Table\n- 2.14 MB for the External Table and 0MB for the Materialized Table\n\n\n## Question 3:\nHow many records have a fare_amount of 0?\n- 12,488\n- 128,219\n- 112\n- 1,622\n\n## Question 4:\nWhat is the best strategy to make an optimized table in Big Query if your query will always order the results by PUlocationID and filter based on lpep_pickup_datetime? (Create a new table with this strategy)\n- Cluster on lpep_pickup_datetime Partition by PUlocationID\n- Partition by lpep_pickup_datetime  Cluster on PUlocationID\n- Partition by lpep_pickup_datetime and Partition by PUlocationID\n- Cluster on by lpep_pickup_datetime and Cluster on PUlocationID\n\n## Question 5:\nWrite a query to retrieve the distinct PULocationID between lpep_pickup_datetime\n06/01/2022 and 06/30/2022 (inclusive)</br>\n\nUse the materialized table you created earlier in your from clause and note the estimated bytes. Now change the table in the from clause to the partitioned table you created for question 4 and note the estimated bytes processed. What are these values? </br>\n\nChoose the answer which most closely matches.</br> \n\n- 22.82 MB for non-partitioned table and 647.87 MB for the partitioned table\n- 12.82 MB for non-partitioned table and 1.12 MB for the partitioned table\n- 5.63 MB for non-partitioned table and 0 MB for the partitioned table\n- 10.31 MB for non-partitioned table and 10.31 MB for the partitioned table\n\n\n## Question 6: \nWhere is the data stored in the External Table you created?\n\n- Big Query\n- GCP Bucket\n- Big Table\n- Container Registry\n\n\n## Question 7:\nIt is best practice in Big Query to always cluster your data:\n- True\n- False\n\n\n## (Bonus: Not worth points) Question 8:\nNo Points: Write a `SELECT count(*)` query FROM the materialized table you created. How many bytes does it estimate will be read? Why?\n\n \n## Submitting the solutions\n\n* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/hw3",
    "filename": "cohorts/2024/03-data-warehouse/homework.md"
  },
  {
    "content": "## Module 4 Homework \n\nIn this homework, we'll use the models developed during the week 4 videos and enhance the already presented dbt project using the already loaded Taxi data for fhv vehicles for year 2019 in our DWH.\n\nThis means that in this homework we use the following data [Datasets list](https://github.com/DataTalksClub/nyc-tlc-data/)\n* Yellow taxi data - Years 2019 and 2020\n* Green taxi data - Years 2019 and 2020 \n* fhv data - Year 2019. \n\nWe will use the data loaded for:\n\n* Building a source table: `stg_fhv_tripdata`\n* Building a fact table: `fact_fhv_trips`\n* Create a dashboard \n\nIf you don't have access to GCP, you can do this locally using the ingested data from your Postgres database\ninstead. If you have access to GCP, you don't need to do it for local Postgres - only if you want to.\n\n> **Note**: if your answer doesn't match exactly, select the closest option \n\n### Question 1: \n\n**What happens when we execute dbt build --vars '{'is_test_run':'true'}'**\nYou'll need to have completed the [\"Build the first dbt models\"](https://www.youtube.com/watch?v=UVI30Vxzd6c) video. \n- It's the same as running *dbt build*\n- It applies a _limit 100_ to all of our models\n- It applies a _limit 100_ only to our staging models\n- Nothing\n\n### Question 2: \n\n**What is the code that our CI job will run? Where is this code coming from?**  \n\n- The code that has been merged into the main branch\n- The code that is behind the creation object on the dbt_cloud_pr_ schema\n- The code from any development branch that has been opened based on main\n- The code from the development branch we are requesting to merge to main\n\n\n### Question 3 (2 points)\n\n**What is the count of records in the model fact_fhv_trips after running all dependencies with the test run variable disabled (:false)?**  \nCreate a staging model for the fhv data, similar to the ones made for yellow and green data. Add an additional filter for keeping only records with pickup time in year 2019.\nDo not add a deduplication step. Run this models without limits (is_test_run: false).\n\nCreate a core model similar to fact trips, but selecting from stg_fhv_tripdata and joining with dim_zones.\nSimilar to what we've done in fact_trips, keep only records with known pickup and dropoff locations entries for pickup and dropoff locations. \nRun the dbt model without limits (is_test_run: false).\n\n- 12998722\n- 22998722\n- 32998722\n- 42998722\n\n### Question 4 (2 points)\n\n**What is the service that had the most rides during the month of July 2019 month with the biggest amount of rides after building a tile for the fact_fhv_trips table and the fact_trips tile as seen in the videos?**\n\nCreate a dashboard with some tiles that you find interesting to explore the data. One tile should show the amount of trips per month, as done in the videos for fact_trips, including the fact_fhv_trips data.\n\n- FHV\n- Green\n- Yellow\n- FHV and Green\n\n\n## Submitting the solutions\n\n* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/hw4\n\nDeadline: 22 February (Thursday), 22:00 CET\n\n\n## Solution (To be published after deadline)\n\n* Video: https://youtu.be/3OPggh5Rca8\n* Answers:\n  * Question 1: It applies a _limit 100_ only to our staging models\n  * Question 2: The code from the development branch we are requesting to merge to main\n  * Question 3: 22998722\n  * Question 4: Yellow",
    "filename": "cohorts/2024/04-analytics-engineering/homework.md"
  },
  {
    "content": "## Module 5 Homework \n\nSolution: https://www.youtube.com/watch?v=YtddC7vJOgQ\n\nIn this homework we'll put what we learned about Spark in practice.\n\nFor this homework we will be using the FHV 2019-10 data found here. [FHV Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz)\n\n### Question 1: \n\n**Install Spark and PySpark** \n\n- Install Spark\n- Run PySpark\n- Create a local spark session\n- Execute spark.version.\n\nWhat's the output?\n\n> [!NOTE]\n> To install PySpark follow this [guide](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/pyspark.md)\n\n### Question 2: \n\n**FHV October 2019**\n\nRead the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n\nRepartition the Dataframe to 6 partitions and save it to parquet.\n\nWhat is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n\n- 1MB\n- 6MB\n- 25MB\n- 87MB\n\n\n\n### Question 3: \n\n**Count records** \n\nHow many taxi trips were there on the 15th of October?\n\nConsider only trips that started on the 15th of October.\n\n- 108,164\n- 12,856\n- 452,470\n- 62,610\n\n> [!IMPORTANT]\n> Be aware of columns order when defining schema\n\n### Question 4: \n\n**Longest trip for each day** \n\nWhat is the length of the longest trip in the dataset in hours?\n\n- 631,152.50 Hours\n- 243.44 Hours\n- 7.68 Hours\n- 3.32 Hours\n\n\n\n### Question 5: \n\n**User Interface**\n\nSpark\u2019s User Interface which shows the application's dashboard runs on which local port?\n\n- 80\n- 443\n- 4040\n- 8080\n\n\n\n### Question 6: \n\n**Least frequent pickup location zone**\n\nLoad the zone lookup data into a temp view in Spark</br>\n[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)\n\nUsing the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?</br>\n\n- East Chelsea\n- Jamaica Bay\n- Union Sq\n- Crown Heights North\n\n\n## Submitting the solutions\n\n- Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/hw5\n- Deadline: See the website",
    "filename": "cohorts/2024/05-batch/homework.md"
  },
  {
    "content": "## Module 6 Homework \n\nIn this homework, we're going to extend Module 5 Homework and learn about streaming with PySpark.\n\nInstead of Kafka, we will use Red Panda, which is a drop-in\nreplacement for Kafka. \n\nEnsure you have the following set up (if you had done the previous homework and the module):\n\n- Docker (see [module 1](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/01-docker-terraform))\n- PySpark (see [module 5](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/05-batch/setup))\n\nFor this homework we will be using the files from Module 5 homework:\n\n- Green 2019-10 data from [here](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz)\n\n\n\n## Start Red Panda\n\nLet's start redpanda in a docker container. \n\nThere's a `docker-compose.yml` file in the homework folder (taken from [here](https://github.com/redpanda-data-blog/2023-python-gsg/blob/main/docker-compose.yml))\n\nCopy this file to your homework directory and run\n\n```bash\ndocker-compose up\n```\n\n(Add `-d` if you want to run in detached mode)\n\n\n## Question 1: Redpanda version\n\nNow let's find out the version of redpandas. \n\nFor that, check the output of the command `rpk help` _inside the container_. The name of the container is `redpanda-1`.\n\nFind out what you need to execute based on the `help` output.\n\nWhat's the version, based on the output of the command you executed? (copy the entire version)\n\n\n## Question 2. Creating a topic\n\nBefore we can send data to the redpanda server, we\nneed to create a topic. We do it also with the `rpk`\ncommand we used previously for figuring out the version of \nredpandas.\n\nRead the output of `help` and based on it, create a topic with name `test-topic` \n\nWhat's the output of the command for creating a topic? Include the entire output in your answer.\n\n\n## Question 3. Connecting to the Kafka server\n\nWe need to make sure we can connect to the server, so\nlater we can send some data to its topics\n\nFirst, let's install the kafka connector (up to you if you\nwant to have a separate virtual environment for that)\n\n```bash\npip install kafka-python\n```\n\nYou can start a jupyter notebook in your solution folder or\ncreate a script\n\nLet's try to connect to our server:\n\n```python\nimport json\nimport time \n\nfrom kafka import KafkaProducer\n\ndef json_serializer(data):\n    return json.dumps(data).encode('utf-8')\n\nserver = 'localhost:9092'\n\nproducer = KafkaProducer(\n    bootstrap_servers=[server],\n    value_serializer=json_serializer\n)\n\nproducer.bootstrap_connected()\n```\n\nProvided that you can connect to the server, what's the output\nof the last command?\n\n\n## Question 4. Sending data to the stream\n\nNow we're ready to send some test data:\n\n```python\nt0 = time.time()\n\ntopic_name = 'test-topic'\n\nfor i in range(10):\n    message = {'number': i}\n    producer.send(topic_name, value=message)\n    print(f\"Sent: {message}\")\n    time.sleep(0.05)\n\nproducer.flush()\n\nt1 = time.time()\nprint(f'took {(t1 - t0):.2f} seconds')\n```\n\nHow much time did it take? Where did it spend most of the time?\n\n* Sending the messages\n* Flushing\n* Both took approximately the same amount of time\n\n(Don't remove `time.sleep` when answering this question)\n\n\n## Reading data with `rpk`\n\nYou can see the messages that you send to the topic\nwith `rpk`:\n\n```bash\nrpk topic consume test-topic\n```\n\nRun the command above and send the messages one more time to \nsee them\n\n\n## Sending the taxi data\n\nNow let's send our actual data:\n\n* Read the green csv.gz file\n* We will only need these columns:\n  * `'lpep_pickup_datetime',`\n  * `'lpep_dropoff_datetime',`\n  * `'PULocationID',`\n  * `'DOLocationID',`\n  * `'passenger_count',`\n  * `'trip_distance',`\n  * `'tip_amount'`\n\nIterate over the records in the dataframe\n\n```python\nfor row in df_green.itertuples(index=False):\n    row_dict = {col: getattr(row, col) for col in row._fields}\n    print(row_dict)\n    break\n\n    # TODO implement sending the data here\n```\n\nNote: this way of iterating over the records is more efficient compared\nto `iterrows`\n\n\n## Question 5: Sending the Trip Data\n\n* Create a topic `green-trips` and send the data there\n* How much time in seconds did it take? (You can round it to a whole number)\n* Make sure you don't include sleeps in your code\n\n\n## Creating the PySpark consumer\n\nNow let's read the data with PySpark. \n\nSpark needs a library (jar) to be able to connect to Kafka, \nso we need to tell PySpark that it needs to use it:\n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\n\npyspark_version = pyspark.__version__\nkafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n\nspark = SparkSession \\\n    .builder \\\n    .master(\"local[*]\") \\\n    .appName(\"GreenTripsConsumer\") \\\n    .config(\"spark.jars.packages\", kafka_jar_package) \\\n    .getOrCreate()\n```\n\nNow we can connect to the stream:\n\n```python\ngreen_stream = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"green-trips\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .load()\n```\n\nIn order to test that we can consume from the stream, \nlet's see what will be the first record there. \n\nIn Spark streaming, the stream is represented as a sequence of \nsmall batches, each batch being a small RDD (or a small dataframe).\n\nSo we can execute a function over each mini-batch.\nLet's run `take(1)` there to see what do we have in the stream:\n\n```python\ndef peek(mini_batch, batch_id):\n    first_row = mini_batch.take(1)\n\n    if first_row:\n        print(first_row[0])\n\nquery = green_stream.writeStream.foreachBatch(peek).start()\n```\n\nYou should see a record like this:\n\n```\nRow(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 3, 12, 22, 42, 9, 411000), timestampType=0)\n```\n\nNow let's stop the query, so it doesn't keep consuming messages\nfrom the stream\n\n```python\nquery.stop()\n```\n\n## Question 6. Parsing the data\n\nThe data is JSON, but currently it's in binary format. We need\nto parse it and turn it into a streaming dataframe with proper\ncolumns.\n\nSimilarly to PySpark, we define the schema\n\n```python\nfrom pyspark.sql import types\n\nschema = types.StructType() \\\n    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n    .add(\"PULocationID\", types.IntegerType()) \\\n    .add(\"DOLocationID\", types.IntegerType()) \\\n    .add(\"passenger_count\", types.DoubleType()) \\\n    .add(\"trip_distance\", types.DoubleType()) \\\n    .add(\"tip_amount\", types.DoubleType())\n```\n\nAnd apply this schema:\n\n```python\nfrom pyspark.sql import functions as F\n\ngreen_stream = green_stream \\\n  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n  .select(\"data.*\")\n```\n\nHow does the record look after parsing? Copy the output. \n\n\n### Question 7: Most popular destination\n\nNow let's finally do some streaming analytics. We will\nsee what's the most popular destination currently \nbased on our stream of data (which ideally we should \nhave sent with delays like we did in workshop 2)\n\n\nThis is how you can do it:\n\n* Add a column \"timestamp\" using the `current_timestamp` function\n* Group by:\n  * 5 minutes window based on the timestamp column (`F.window(col(\"timestamp\"), \"5 minutes\")`)\n  * `\"DOLocationID\"`\n* Order by count\n\nYou can print the output to the console using this \ncode\n\n```python\nquery = popular_destinations \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .option(\"truncate\", \"false\") \\\n    .start()\n\nquery.awaitTermination()\n```\n\nWrite the most popular destination, your answer should be *either* the zone ID or the zone name of this destination. (You will need to re-send the data for this to work)\n\n\n## Submitting the solutions\n\n* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/hw6\n\n\n## Solution\n\nWe will publish the solution here after deadline.",
    "filename": "cohorts/2024/06-streaming/homework.md"
  },
  {
    "content": "## Data Engineering Zoomcamp 2024 Cohort\n\n* [Pre-launch Q&A stream](https://www.youtube.com/watch?v=91b8u9GmqB4)\n* [Launch stream with course overview](https://www.youtube.com/live/AtRhA-NfS24?si=5JzA_E8BmJjiLi8l)\n* [Deadline calendar](https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml)\n* [Course Google calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ)\n* [FAQ](https://datatalks.club/faq/data-engineering-zoomcamp.html)\n* Course Playlist: Only 2024 Live videos & homeworks (TODO)\n* [Public Leaderboard of Top-100 Participants](leaderboard.md)\n\n\n[**Module 1: Introduction & Prerequisites**](01-docker-terraform/)\n\n* [Homework](01-docker-terraform/homework.md)\n\n\n[**Module 2: Workflow Orchestration**](02-workflow-orchestration)\n\n* [Homework](02-workflow-orchestration/homework.md)\n* Office hours\n\n[**Workshop 1: Data Ingestion**](workshops/dlt.md)\n\n* Workshop with dlt\n* [Homework](workshops/dlt.md)\n\n\n[**Module 3: Data Warehouse**](03-data-warehouse)\n\n* [Homework](03-data-warehouse/homework.md)\n\n\n[**Module 4: Analytics Engineering**](04-analytics-engineering/)\n\n* [Homework](04-analytics-engineering/homework.md)\n\n\n[**Module 5: Batch processing**](05-batch/)\n\n* [Homework](05-batch/homework.md)\n\n\n[**Module 6: Stream Processing**](06-streaming)\n\n* [Homework](06-streaming/homework.md)\n\n\n[**Project**](project.md)\n\nMore information [here](project.md)",
    "filename": "cohorts/2024/README.md"
  },
  {
    "content": "## Leaderboard \n\nThis is the top [100 leaderboard](https://courses.datatalks.club/de-zoomcamp-2024/leaderboard)\nof participants of Data Engineering Zoomcamp 2024 edition!\n\n<table>\n<tr>\n  <th>Name</th>\n  <th>Projects</th>\n  <th>Social</th>\n  <th>Comments</th>\n</tr>\n<tr>\n  <td>Ashraf Mohammad</td>\n  <td><a href=\"https://github.com/Ashraf1395/customer_retention_analytics\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a><a href=\"https://github.com/Ashraf1395/supply_chain_finance.git\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"www.linkedin.com/in/ashraf1395\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"www.github.com/Ashraf1395\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nReally Recommend this bootcamp , if you want to get hands on data engineering experience.     My two Capstone project: www.github.com/Ashraf1395/supply_chain_finance, www.github.com/Ashraf1395/customer_retention_analytics\n</details></td>\n</tr>\n<tr>\n  <td>Jorge Vladimir Abrego Arevalo</td>\n  <td><a href=\"https://github.com/JorgeAbrego/weather_stream_project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a><a href=\"https://github.com/JorgeAbrego/capital_bikeshare_project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/jorge-abrego/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/JorgeAbrego\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Purnendu Shekhar Shukla</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Krishna Anand</td>\n  <td><a href=\"https://github.com/anandaiml19/DE_Zoomcamp_Project2/tree/main\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a><a href=\"https://github.com/anandaiml19/Data-Engineering-Zoomcamp-Project1\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/krishna-anand-v-g-70bba623/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/anandaiml19\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Abhijit Chakraborty</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Hekmatullah Sajid</td>\n  <td><a href=\"https://github.com/hekmatullah-sajid/EcoEnergy-Germany\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/hekmatullah-sajid/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/hekmatullah-sajid\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Lottie Jane Pollard</td>\n  <td><a href=\"https://github.com/LottieJaneDev/usgs_earthquake_data_pipeline\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/lottiejanedev/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/LottieJaneDev/usgs_earthquake_data_pipeline\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>AviAnna</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Ketut Garjita</td>\n  <td><a href=\"https://github.com/garjita63/dezoomcamp2024-project1\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/ketutgarjitadba/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/garjita63\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nI would like to express my thanks and appreciation to the Data Talks Club for organizing this excellent Data Engineering Zoomcamp training. This made me valuable experience in deepening new knowledge for me even though previously I had mostly worked as a Database Administrator for various platform databases. Thank you also to the community (datatalks-club.slack.com), especially slack course-data-engineering, as well as other slack communities such as mageai.slack.com.\n</details></td>\n</tr>\n<tr>\n  <td>Diogo Costa</td>\n  <td><a href=\"https://github.com/techwithcosta/youtube-ai-analytics\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/costadms/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/techwithcosta\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nGreat course! Check out my YouTube channel: https://www.youtube.com/@TechWithCosta\n</details></td>\n</tr>\n<tr>\n  <td>Francisco Ortiz Tena</td>\n  <td><a href=\"https://github.com/FranciscoOrtizTena/de_zoomcamp_project_01/\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/francisco-ortiz-tena/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/FranciscoOrtizTena\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nIt is an awesome course!\n</details></td>\n</tr>\n<tr>\n  <td>Nevenka Lukic</td>\n  <td><a href=\"https://github.com/nenalukic/air-quality-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/nevenka-lukic/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/nenalukic\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nThis DE Zoomcamp was fantastic learning and networking experiences. Many thanks to organizers and big recommendations to anyone!\n</details></td>\n</tr>\n<tr>\n  <td>Mukhammad Sofyan Rizka Akbar</td>\n  <td><a href=\"https://github.com/SofyanAkbar94/Project-DE-Zoomcamp-2024\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://id.linkedin.com/in/m-sofyan-r-a-aa00a4118\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/SofyanAkbar94/\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nThanks for providing this course, especially for Alexey and other Datatalk hosts and I hope I can join ML, ML Ops, and LLM Zoomcamp. See you soon :)\n</details></td>\n</tr>\n<tr>\n  <td>Mahmoud Mahdy Zaky</td>\n  <td><a href=\"https://github.com/MahmoudMahdy448/Football-Data-Analytics/tree/main\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/mahmoud-mahdy-zaky\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/MahmoudMahdy448\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Brilliant Pancake</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Jobert M. Gutierrez</td>\n  <td><a href=\"https://github.com/bizzaccelerator/Footballers-transfers-Insights.git\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"www.linkedin.com/in/jobertgutierrez\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/bizzaccelerator\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Olusegun Samson Ayeni</td>\n  <td><a href=\"https://github.com/iamraphson/IMDB-pipeline-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a><a href=\"https://github.com/iamraphson/DE-2024-project-book-recommendation\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/iamraphson/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/iamraphson\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Lily Chau</td>\n  <td><a href=\"https://github.com/lilychau1/uk-power-analytics\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a><a href=\"https://github.com/lilychau1/uk-power-analytics/tree/main\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"www.linkedin.com/in/lilychau1\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/lilychau1\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nBig thank you to Alexey and all other speakers. This is one of the best online learning platforms I have ever come across.\n</details></td>\n</tr>\n<tr>\n  <td>Aleksandr Kolmakov</td>\n  <td><a href=\"https://github.com/Feanaur/marine-species-analytics\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a><a href=\"https://github.com/Feanaur/marine-species-analytics\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/aleksandr-kolmakov/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/alex-kolmakov\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Kang Zhi Yong</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Eduardo Mu\u00f1oz Sala</td>\n  <td><a href=\"https://github.com/edumunozsala/GDELT-Events-Data-Eng-Project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/edumunozsala/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/edumunozsala\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Kirill Bazarov</td>\n  <td><a href=\"https://github.com/kirill505/de-zoomcamp-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/kirill-bazarov-66ba3152\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/kirill505\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Shayan Shafiee Moghadam</td>\n  <td><a href=\"https://github.com/shayansm2/DE-zoomcamp-playground/tree/de-zoomcamp-2nd-project/github-events-analyzer\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a><a href=\"https://github.com/shayansm2/tech-career-explorer/tree/de-zoomcamp-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/shayan-shafiee-moghadam/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/shayansm2\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Landry N.</td>\n  <td><a href=\"https://github.com/drux31/capstone-dezoomcamp\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://github.com/drux31\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nThanks for the awsome course.\n</details></td>\n</tr>\n<tr>\n  <td>Condescending Austin</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Lee Durbin</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Loving Einstein</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Carlos Vecina Tebar</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Abiodun Oki</td>\n  <td></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/okibaba/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Okibaba\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nthoroughly enjoyed the course, great work Alexey & course team!\n</details></td>\n</tr>\n<tr>\n  <td>Jimoh</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Sleepy Villani</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Ella Cinders</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Max Lutz</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Jessica De Silva</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Daniel Okello</td>\n  <td></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/okellodaniel/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/okellodaniel\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Kirill Sitnikov</td>\n  <td><a href=\"https://github.com/Siddha911/Citibike-data-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"Siddha911\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nThank you Alexey and all DTC team! I\u2019m so glad that I knew about your courses and projects!\n</details></td>\n</tr>\n<tr>\n  <td>edumad</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Duy Quoc Vo</td>\n  <td><a href=\"https://github.com/voduyquoc/air_pollution_tracking\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/voduyquoc/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/voduyquoc\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nNA\n</details></td>\n</tr>\n<tr>\n  <td>Xiang Li</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Sugeng Wahyudi</td>\n  <td><a href=\"https://github.com/Gengsu07/DEGengsuProject\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/sugeng-wahyudi-8a3939132/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Gengsu07\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nThanks a lot, this was amazing. Can't miss another course and zoomcamp from datatalks.club\n</details></td>\n</tr>\n<tr>\n  <td>Anatolii Kryvko</td>\n  <td><a href=\"https://github.com/Nogromi/ukraine-vaccinations/tree/master\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/anatolii-kryvko-69b538107/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Nogromi\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>David Vanegas</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Honey Badger</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Abdelrahman Kamal</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Jean Paul Rodriguez</td>\n  <td><a href=\"https://github.com/jeanpaulrd1/de-zc-final-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/jean-paul-rodriguez\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/jeanpaulrd1/de-zc-final-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Eager Pasteur</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Damian Pszczo\u0142a</td>\n  <td><a href=\"https://github.com/d4mp3/GLDAS-Data-Pipeline\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/damian-pszczo%C5%82a-7aba54241/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/d4mp3\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>ManPrat</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>forrest_parnassus</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Ramazan Abylkassov</td>\n  <td><a href=\"https://github.com/ramazanabylkassov/aviation_stack_project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/ramazan-abylkassov-23965097/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/ramazanabylkassov\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nLook mom, I am on leaderboard!\n</details></td>\n</tr>\n<tr>\n  <td>Digamber Deshmukh</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Andrew Lee</td>\n  <td><a href=\"https://github.com/wndrlxx/ca-trademarks-data-pipeline\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Matt R</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Raul Antonio Catacora Grundy</td>\n  <td><a href=\"https://github.com/Cerpint4xt/data-engineering-all-news-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/raul-catacora-grundy-208315236/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Cerpint4xt\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nI just want to thank everyone, all the instructors, collaborators for creating this amazing set of resources and such a solid community based on sharing and caring. Many many thanks and shout out to you guys\n</details></td>\n</tr>\n<tr>\n  <td>Ranga H.</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Salma Gouda</td>\n  <td><a href=\"https://github.com/salmagouda/data-engineering-capstone/tree/main\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://linkedin.com/in/salmagouda\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/salmagouda\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Artsiom Turevich</td>\n  <td><a href=\"https://github.com/aturevich/zoomcamp_de_project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/artsiom-turevich/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"a.turevich\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nA long time ago in a galaxy far, far away...\n</details></td>\n</tr>\n<tr>\n  <td>Abhirup Ghosh</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Sonny Pham</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Peter Tran</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Ritika Tilwalia</td>\n  <td><a href=\"https://github.com/rtilwalia/Fashion-Campus-Orders\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/ritika-tilwalia/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/rtilwalia\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Eager Yalow</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Dave Samaniego</td>\n  <td><a href=\"https://github.com/nishiikata/de-zoomcamp-2024-mage-capstone\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/dave-s-32545014a\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/nishiikata\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nThank you DataTalksClub for the course. It was challenging learning many new things, but I had fun along the way too!\n</details></td>\n</tr>\n<tr>\n  <td>Lucid Keldysh</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Isaac Ndirangu Muturi</td>\n  <td><a href=\"https://github.com/Isaac-Ndirangu-Muturi-749/End_to_end_data_pipeline--Optimizing_Online_Retail_Analytics_with_Data_and_Analytics_Engineering\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/isaac-muturi-3b6b2b237\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Isaac-Ndirangu-Muturi-749\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nAmazing learning experience\n</details></td>\n</tr>\n<tr>\n  <td>Agitated Wing</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Hanaa HAMMAD</td>\n  <td></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/hanaahammad/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/hanaahammad\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nGrateful to this great course\n</details></td>\n</tr>\n<tr>\n  <td>Jonah Oliver</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Paul Emilio Arizpe Colorado</td>\n  <td><a href=\"https://github.com/kiramishima/crimes_in_mexico_city_analysis\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/parizpe/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/kiramishima\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nDataTalksClub brought me the opportunity to learn data engineering. Thanks for all :D\n</details></td>\n</tr>\n<tr>\n  <td>Asma-Chlo\u00eb FARAH</td>\n  <td><a href=\"https://github.com/AsmaChloe/traffic_counting_paris\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/asma-chloefarah/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/AsmaChloe\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nThank you for this amazing zoomcamp ! It was really fun !\n</details></td>\n</tr>\n<tr>\n  <td>Happy Feistel</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Luca Pugliese</td>\n  <td><a href=\"https://github.com/lucapug/nyc-bike-analytics\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/lucapug/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/lucapug\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nit has been a crowdlearning experience! starting in thousands of us. 359 graduated in the end. Proud to have classified 59th. Thanks to all.\n</details></td>\n</tr>\n<tr>\n  <td>Jake Maund</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Aditya Phulallwar</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Dave Wilson</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Haitham Hussein Hamad</td>\n  <td><a href=\"https://github.com/haithamhamad2/kaggle-survey\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/haitham-hamad-8926b415/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/haithamhamad2\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Alexandre Bergere aka Rocket</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>TOGBAN COKOUVI Joyce Elvis Mahoutondji</td>\n  <td><a href=\"https://github.com/lvsuno/Github_data_analysis\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/elvistogban/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/lvsuno/Github_data_analysis\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Sad Robinson</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Tetiana Omelchenko</td>\n  <td><a href=\"https://github.com/TOmelchenko/LifeExpectancyProject\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"www.linkedin.com/in/tetiana-omelchenko-35177379\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/TOmelchenko/LifeExpectancyProject\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Amanda Kershaw</td>\n  <td><a href=\"https://github.com/ANKershaw/youtube_video_ranks\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/amandalnkershaw\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/ANKershaw/youtube_video_ranks\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nThis course was incredibly rewarding and absolutely worth the effort.\n</details></td>\n</tr>\n<tr>\n  <td>Kristjan Sert</td>\n  <td><a href=\"https://github.com/KrisSert/cadaster-ee\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/kristjan-sert-043396131/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/KrisSert\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Murad Arfanyan</td>\n  <td><a href=\"https://github.com/murkenson/movies_tv_shows_data_pipeline\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/murad-arfanyan-846786176/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/murkenson\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Ecstatic Hofstadter</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Chung Huu Tin</td>\n  <td><a href=\"https://github.com/TinChung41/US-Accidents-Analysis-zoomcamp-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"linkedin.com/in/huu-tin-chung\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/TinChung41\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Zen Mayer</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Zhastay Yeltay</td>\n  <td><a href=\"https://github.com/yelzha/tengrinews-open-project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/yelzha/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/yelzha\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\n;)\n</details></td>\n</tr>\n<tr>\n  <td>AV3NII</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Sebastian Alejandro Peralta Casafranca</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Relaxed Williams</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>George Mouratos</td>\n  <td><a href=\"https://github.com/Gimour/Datatalks_final_project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/gmouratos/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/Gimour/DataTalks, https://github.com/Gimour/Datatalks_final_project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\n-\n</details></td>\n</tr>\n<tr>\n  <td>mhmed ahmed rjb</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Frosty Jackson</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>WANJOHI</td>\n  <td><a href=\"https://github.com/DE-ZoomCamp/Flood-Monitoring\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://github.com/DE-ZoomCamp/Flood-Monitoring\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Ighorr Holstrom</td>\n  <td><a href=\"https://github.com/askeladden31/air_raids_data/\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/ighorr-holstrom/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/askeladden31\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Jesse Delzio</td>\n  <td></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/delzioj\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/delzio\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Khalil El Daou</td>\n  <td><a href=\"https://github.com/khalileldoau/global-news-engagement-on-social-media\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/khalil-el-daou-177a8b114?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/khalileldoau\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td><details>\n<summary>comment</summary>\nAlready made a post about the zoomcamp\n</details></td>\n</tr>\n<tr>\n  <td>Juan Rojas</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Gon\u00e7alo</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Muhamad Farikhin</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Bold Lederberg</td>\n  <td></a></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>Taras Shalaiko</td>\n  <td><a href=\"https://github.com/tarasenya/dezoomcamp_final_project\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></a></td>\n  <td> <a href=\"https://www.linkedin.com/in/taras-shalaiko-30114a107/\"><img src=\"https://user-images.githubusercontent.com/875246/192300614-2ce22ed5-bbc4-4684-8098-d8128d71aac5.png\" height=\"16em\" /></a> <a href=\"https://github.com/tarasenya\"><img src=\"https://user-images.githubusercontent.com/875246/192300611-a606521b-cb76-4090-be8e-7cc21752b996.png\" height=\"16em\" /></a></td>\n  <td></td>\n</tr>\n</table>",
    "filename": "cohorts/2024/leaderboard.md"
  },
  {
    "content": "## Course Project\n\nThe goal of this project is to apply everything we learned\nin this course and build an end-to-end data pipeline.\n\nYou will have two attempts to submit your project. If you don't have \ntime to submit your project by the end of attempt #1 (you started the \ncourse late, you have vacation plans, life/work got in the way, etc.)\nor you fail your first attempt, \nthen you will have a second chance to submit your project as attempt\n#2. \n\nThere are only two attempts.\n\nRemember that to pass the project, you must evaluate 3 peers. If you don't do that,\nyour project can't be considered complete.\n\nTo find the projects assigned to you, use the peer review assignments link \nand find your hash in the first column. You will see three rows: you need to evaluate \neach of these projects. For each project, you need to submit the form once,\nso in total, you will make three submissions. \n\n\n### Submitting\n\n#### Project Attempt #1\n\n* Project: https://courses.datatalks.club/de-zoomcamp-2024/project/project1\n* Review: https://courses.datatalks.club/de-zoomcamp-2024/project/project1/eval\n\n#### Project Attempt #2\n\n* Project: https://courses.datatalks.club/de-zoomcamp-2024/project/project2\n* Review: https://courses.datatalks.club/de-zoomcamp-2024/project/project2/eval\n\n> **Important**: update your \"Certificate name\" here: https://courses.datatalks.club/de-zoomcamp-2024/enrollment -\nthis is what we will use when generating certificates for you.\n\n### Evaluation criteria\n\nSee [here](../../week_7_project/README.md)",
    "filename": "cohorts/2024/project.md"
  },
  {
    "content": "# Data ingestion with dlt\n\n\u200bIn this hands-on workshop, we\u2019ll learn how to build data ingestion pipelines.\n\n\u200bWe\u2019ll cover the following steps:\n\n* \u200bExtracting data from APIs, or files.\n* \u200bNormalizing and loading data\n* \u200bIncremental loading\n\n\u200bBy the end of this workshop, you\u2019ll be able to write data pipelines like a senior data engineer: Quickly, concisely, scalable, and self-maintaining.\n\nVideo: https://www.youtube.com/live/oLXhBM7nf2Q\n\n--- \n\n# Navigation\n\n* [Workshop content](dlt_resources/data_ingestion_workshop.md)\n* [Workshop notebook](dlt_resources/workshop.ipynb)\n* [Homework starter notebook](dlt_resources/homework_starter.ipynb)\n\n# Resources\n\n- Website and community: Visit our [docs](https://dlthub.com/docs/intro), discuss on our slack (Link at top of docs).\n- Course colab: [Notebook](https://colab.research.google.com/drive/1kLyD3AL-tYf_HqCXYnA3ZLwHGpzbLmoj#scrollTo=5aPjk0O3S_Ag&forceEdit=true&sandboxMode=true).\n- dlthub [community Slack](https://dlthub.com/community).\n\n---\n\n# Teacher\n\nWelcome to the data talks club data engineering zoomcamp, the data ingestion workshop.\n\n- My name is [Adrian](https://www.linkedin.com/in/data-team/), and I work in the data field since 2012\n    - I built many data warehouses some lakes, and a few data teams\n    - 10 years into my career I started working on dlt \u201cdata load tool\u201d, which is an open source library to enable data engineers to build faster and better.\n    - I started working on dlt because data engineering is one of the few areas of software engineering where we do not have developer tools to do our work.\n    - Building better pipelines would require more code re-use - we cannot all just build perfect pipelines from scratch every time.\n    - And so dlt was born, a library that automates the tedious part of data ingestion: Loading, schema management, data type detection, scalability, self healing, scalable extraction\u2026 you get the idea - essentially a data engineer\u2019s \u201cone stop shop\u201d for best practice data pipelining.\n    - Due to its **simplicity** of use, dlt enables **laymen** to\n        - Build pipelines 5-10x faster than without it\n        - Build self healing, self maintaining pipelines with all the best practices of data engineers. Automating schema changes removes the bulk of maintenance efforts.\n        - Govern your pipelines with schema evolution alerts and data contracts.\n        - and generally develop pipelines like a senior, commercial data engineer.\n\n--- \n\n# Course\nYou can find the course file [here](./dlt_resources/data_ingestion_workshop.md)\nThe course has 3 parts\n- [Extraction Section](./dlt_resources/data_ingestion_workshop.md#extracting-data): In this section we will learn about scalable extraction\n- [Normalisation Section](./dlt_resources/data_ingestion_workshop.md#normalisation): In this section we will learn to prepare data for loading\n- [Loading Section](./dlt_resources/data_ingestion_workshop.md#incremental-loading)): Here we will learn about incremental loading modes\n\n---\n\n# Homework\n\nThe [linked colab notebook](https://colab.research.google.com/drive/1Te-AT0lfh0GpChg1Rbd0ByEKOHYtWXfm#scrollTo=wLF4iXf-NR7t&forceEdit=true&sandboxMode=true) offers a few exercises to practice what you learned today.\n\n\n#### Question 1: What is the sum of the outputs of the generator for limit = 5?\n- **A**: 10.23433234744176\n- **B**: 7.892332347441762\n- **C**: 8.382332347441762\n- **D**: 9.123332347441762\n\n#### Question 2: What is the 13th number yielded by the generator?\n- **A**: 4.236551275463989\n- **B**: 3.605551275463989\n- **C**: 2.345551275463989\n- **D**: 5.678551275463989\n\n#### Question 3: Append the 2 generators. After correctly appending the data, calculate the sum of all ages of people.\n- **A**: 353\n- **B**: 365\n- **C**: 378\n- **D**: 390\n\n#### Question 4: Merge the 2 generators using the ID column. Calculate the sum of ages of all the people loaded as described above.\n- **A**: 215\n- **B**: 266\n- **C**: 241\n- **D**: 258\n\nSubmit the solution here: https://courses.datatalks.club/de-zoomcamp-2024/homework/workshop1\n\n--- \n# Next steps\n\nAs you are learning the various concepts of data engineering, \nconsider creating a portfolio project that will further your own knowledge.\n\nBy demonstrating the ability to deliver end to end, you will have an easier time finding your first role. \nThis will help regardless of whether your hiring manager reviews your project, largely because you will have a better \nunderstanding and will be able to talk the talk.\n\nHere are some example projects that others did with dlt:\n- Serverless dlt-dbt on cloud functions: [Article](https://docs.getdbt.com/blog/serverless-dlt-dbt-stack)\n- Bird finder: [Part 1](https://publish.obsidian.md/lough-on-data/blogs/bird-finder-via-dlt-i), [Part 2](https://publish.obsidian.md/lough-on-data/blogs/bird-finder-via-dlt-ii)\n- Event ingestion on GCP: [Article and repo](https://dlthub.com/docs/blog/streaming-pubsub-json-gcp)\n- Event ingestion on AWS: [Article and repo](https://dlthub.com/docs/blog/dlt-aws-taktile-blog)\n- Or see one of the many demos created by our working students: [Hacker news](https://dlthub.com/docs/blog/hacker-news-gpt-4-dashboard-demo), \n[GA4 events](https://dlthub.com/docs/blog/ga4-internal-dashboard-demo), \n[an E-Commerce](https://dlthub.com/docs/blog/postgresql-bigquery-metabase-demo), \n[google sheets](https://dlthub.com/docs/blog/google-sheets-to-data-warehouse-pipeline), \n[Motherduck](https://dlthub.com/docs/blog/dlt-motherduck-demo), \n[MongoDB + Holistics](https://dlthub.com/docs/blog/MongoDB-dlt-Holistics), \n[Deepnote](https://dlthub.com/docs/blog/deepnote-women-wellness-violence-tends), \n[Prefect](https://dlthub.com/docs/blog/dlt-prefect),\n[PowerBI vs GoodData vs Metabase](https://dlthub.com/docs/blog/semantic-modeling-tools-comparison),\n[Dagster](https://dlthub.com/docs/blog/dlt-dagster),\n[Ingesting events via gcp webhooks](https://dlthub.com/docs/blog/dlt-webhooks-on-cloud-functions-for-event-capture),\n[SAP to snowflake replication](https://dlthub.com/docs/blog/sap-hana-to-snowflake-demo-blog),\n[Read emails and send sumamry to slack with AI and Kestra](https://dlthub.com/docs/blog/dlt-kestra-demo-blog),\n[Mode +dlt capabilities](https://dlthub.com/docs/blog/dlt-mode-blog),\n[dbt on cloud functions](https://dlthub.com/docs/blog/dlt-dbt-runner-on-cloud-functions)\n- If you want to use dlt in your project, [check this list of public APIs](https://dlthub.com/docs/blog/practice-api-sources)\n\n\nIf you create a personal project, consider submitting it to our blog - we will be happy to showcase it. Just drop us a line in the dlt slack.\n\n\n\n**And don't forget, if you like dlt**\n- **Give us a [GitHub Star!](https://github.com/dlt-hub/dlt)**\n- **Join our [Slack community](https://dlthub.com/community)**\n\n\n# Notes\n\n* Add your notes here",
    "filename": "cohorts/2024/workshops/dlt.md"
  },
  {
    "content": "# Intro\n\nWhat is data loading, or data ingestion?\n\nData ingestion is the process of extracting data from a producer, transporting it to a convenient environment, and preparing it for usage by normalising it, sometimes cleaning, and adding metadata.\n\n### \u201cA wild dataset magically appears!\u201d\n\nIn many data science teams, data magically appears - because the engineer loads it.\n\n- Sometimes the format in which it appears is structured, and with explicit schema\n    - In that case, they can go straight to using it; Examples: Parquet, Avro, or table in a db,\n- Sometimes the format is weakly typed and without explicit schema, such as csv, json\n    - in which case some extra normalisation or cleaning might be needed before usage\n\n> \ud83d\udca1 **What is a schema?** The schema specifies the expected format and structure of data within a document or data store, defining the allowed keys, their data types, and any constraints or relationships.\n\n\n### Be the magician! \ud83d\ude0e\n\nSince you are here to learn about data engineering, you will be the one making datasets magically appear. \n\nHere\u2019s what you need to learn to build pipelines\n\n- Extracting data\n- Normalising, cleaning, adding metadata such as schema and types\n- and Incremental loading, which is vital for fast, cost effective data refreshes.\n\n### What else does a data engineer do? What are we not learning, and what are we learning?\n\n- It might seem simplistic, but in fact a data engineer\u2019s main goal is to ensure data flows from source systems to analytical destinations.\n- So besides building pipelines, running pipelines and fixing pipelines, a data engineer may also focus on optimising data storage, ensuring data quality and integrity, implementing effective data governance practices, and continuously refining data architecture to meet the evolving needs of the organisation.\n- Ultimately, a data engineer's role extends beyond the mechanical aspects of pipeline development, encompassing the strategic management and enhancement of the entire data lifecycle.\n- This workshop focuses on building robust, scalable, self maintaining pipelines, with built in governance - in other words, best practices applied.\n\n# Extracting data\n\n### The considerations of extracting data\n\nIn this section we will learn about extracting data from source systems, and what to care about when doing so.\n\nMost data is stored behind an API \n\n- Sometimes that\u2019s a RESTful api for some business application, returning records of data.\n- Sometimes the API returns a secure file path to something like a json or parquet file in a bucket that enables you to grab the data in bulk,\n- Sometimes the API is something else (mongo, sql, other databases or applications) and will generally return records as JSON - the most common interchange format.\n\nAs an engineer, you will need to build pipelines that \u201cjust work\u201d. \n\nSo here\u2019s what you need to consider on extraction, to prevent the pipelines from breaking, and to keep them running smoothly.\n\n- Hardware limits: During this course we will cover how to navigate the challenges of managing memory.\n- Network limits: Sometimes networks can fail. We can\u2019t fix what could go wrong but we can retry network jobs until they succeed. For example, dlt library offers a requests \u201creplacement\u201d that has built in retries. [Docs](https://dlthub.com/docs/reference/performance#using-the-built-in-requests-client). We won\u2019t focus on this during the course but you can read the docs on your own.\n- Source api limits: Each source might have some limits such as how many requests you can do per second. We would call these \u201crate limits\u201d. Read each source\u2019s docs carefully to understand how to navigate these obstacles. You can find some examples of how to wait for rate limits in our verified sources repositories\n    - examples: [Zendesk](https://developer.zendesk.com/api-reference/introduction/rate-limits/), [Shopify](https://shopify.dev/docs/api/usage/rate-limits)\n\n### Extracting data without hitting hardware limits\n\nWhat kind of limits could you hit on your machine? In the case of data extraction, the only limits are memory and storage. This refers to the RAM or virtual memory, and the disk, or physical storage.\n\n### **Managing memory.**\n\n- Many data pipelines run on serverless functions or on orchestrators that delegate the workloads to clusters of small workers.\n- These systems have a small memory or share it between multiple workers - so filling the memory is BAAAD: It might lead to not only your pipeline crashing, but crashing the entire container or machine that might be shared with other worker processes, taking them down too.\n- The same can be said about disk - in most cases your disk is sufficient, but in some cases it\u2019s not. For those cases, mounting an external drive mapped to a storage bucket is the way to go. Airflow for example supports a \u201cdata\u201d folder that is used just like a local folder but can be mapped to a bucket for unlimited capacity.\n\n### So how do we avoid filling the memory?\n\n- We often do not know the volume of data upfront\n- And we cannot scale dynamically or infinitely on hardware during runtime\n- So the answer is: Control the max memory you use\n\n### Control the max memory used by streaming the data\n\nStreaming here refers to processing the data event by event or chunk by chunk instead of doing bulk operations. \n\nLet\u2019s look at some classic examples of streaming where data is transferred chunk by chunk or event by event\n\n- Between an audio broadcaster and an in-browser audio player\n- Between a server and a local video player\n- Between a smart home device or IoT device and your phone\n- between google maps and your navigation app\n- Between instagram live and your followers\n\nWhat do data engineers do? We usually stream the data between buffers, such as \n\n- from API to local file\n- from webhooks to event queues\n- from event queue (Kafka, SQS) to Bucket\n\n### Streaming in python via generators\n\nLet\u2019s focus on how we build most data pipelines:\n\n- To process data in a stream in python, we use generators, which are functions that can return multiple times - by allowing multiple returns, the data can be released as it\u2019s produced, as stream, instead of returning it all at once as a batch.\n\nTake the following theoretical example: \n\n- We search twitter for \u201ccat pictures\u201d. We do not know how many pictures will be returned - maybe 10, maybe 10.000.000. Will they fit in memory? Who knows.\n- So to grab this data without running out of memory, we would use a python generator.\n- What\u2019s a generator? In simple words, it\u2019s a function that can return multiple times. Here\u2019s an example of a regular function, and how that function looks if written as a generator.\n\n### Generator examples:\n\nLet\u2019s look at a regular returning function, and how we can re-write it as a generator.\n\n**Regular function collects data in memory.** Here you can see how data is collected row by row in a list called `data`before it is returned. This will break if we have more data than memory.\n\n```python\ndef search_twitter(query):\n\tdata = []\n\tfor row in paginated_get(query):\n\t\tdata.append(row)\n\treturn data\n\n# Collect all the cat picture data\nfor row in search_twitter(\"cat pictures\"):\n  # Once collected, \n  # print row by row\n\tprint(row)\n```\n\nWhen calling `for row in search_twitter(\"cat pictures\"):` all the data must first be downloaded before the first record is returned\n\nLet\u2019s see how we could rewrite this as a generator.\n\n**Generator for streaming the data.** The memory usage here is minimal.\n\nAs you can see, in the modified function, we yield each row as we get the data, without collecting it into memory. We can then run this generator and handle the data item by item.\n\n```python\ndef search_twitter(query):\n\tfor row in paginated_get(query):\n\t\tyield row\n\n# Get one row at a time\nfor row in extract_data(\"cat pictures\"):\n\t# print the row\n\tprint(row)\n  # do something with the row such as cleaning it and writing it to a buffer\n\t# continue requesting and printing data\n```\n\nWhen calling `for row in extract_data(\"cat pictures\"):` the function only runs until the first data item is yielded, before printing - so we do not need to wait long for the first value. It will then continue until there is no more data to get.\n\nIf we wanted to get all the values at once from a generator instead of one by one, we would need to first \u201crun\u201d the generator and collect the data. For example, if we wanted to get all the data in memory we could do `data = list(extract_data(\"cat pictures\"))` which would run the generator and collect all the data in a list before continuing.\n\n## 3 Extraction examples:\n\n### Example 1: Grabbing data from an api\n\n> \ud83d\udca1 This is the bread and butter of data engineers pulling data, so follow along in the colab or in your local setup.\n\n\nFor these purposes we created an api that can serve the data you are already familiar with, the NYC taxi dataset.\n\nThe api documentation is as follows:\n\n- There are a limited nr of records behind the api\n- The data can be requested page by page, each page containing 1000 records\n- If we request a page with no data, we will get a successful response with no data\n- so this means that when we get an empty page, we know there is no more data and we can stop requesting pages - this is a common way to paginate but not the only one - each api may be different.\n- details:\n    - method: get\n    - url: `https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api`\n    - parameters: `page`  integer. Represents the page number you are requesting. Defaults to 1.\n    \n\nSo how do we design our requester? \n\n- We need to request page by page until we get no more data. At this point, we do not know how much data is behind the api.\n- It could be 1000 records or it could be 10GB of records. So let\u2019s grab the data with a generator to avoid having to fit an undetermined amount of data into ram.\n\nIn this approach to grabbing data from apis, we have pros and cons:\n\n- Pros: **Easy memory management** thanks to api returning events/pages\n- Cons: **Low throughput**, due to the data transfer being constrained via an API.\n\n```bash\nimport requests\n\nBASE_API_URL = \"https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api\"\n\n# I call this a paginated getter\n# as it's a function that gets data\n# and also paginates until there is no more data\n# by yielding pages, we \"microbatch\", which speeds up downstream processing\n\ndef paginated_getter():\n    page_number = 1\n\n    while True:\n        # Set the query parameters\n        params = {'page': page_number}\n\n        # Make the GET request to the API\n        response = requests.get(BASE_API_URL, params=params)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n        page_json = response.json()\n        print(f'got page number {page_number} with {len(page_json)} records')\n\n        # if the page has no records, stop iterating\n        if page_json:\n            yield page_json\n            page_number += 1\n        else:\n            # No more data, break the loop\n            break\n\nif __name__ == '__main__':\n    # Use the generator to iterate over pages\n    for page_data in paginated_getter():\n        # Process each page as needed\n        print(page_data)\n```\n\n### Example 2: Grabbing the same data from file - simple download\n\n\n> \ud83d\udca1 This part is demonstrative, so you do not need to follow along; just pay attention.\n\n\n- Why am I showing you this? so when you do this in the future, you will remember there is a best practice you can apply for scalability.\n\nSome apis respond with files instead of pages of data. The reason for this is simple: Throughput and cost. A restful api that returns data has to read the data from storage and process and return it to you by some logic - If this data is large, this costs time, money and creates a bottleneck. \n\nA better way is to offer the data as files that someone can download from storage directly, without going through the restful api layer. This is common for apis that offer large volumes of data, such as ad impressions data.\n\nIn this example, we grab exactly the same data as we did in the API example above, but now we get it from the underlying file instead of going through the API.\n\n- Pros: **High throughput**\n- Cons: **Memory** is used to hold all the data\n\nThis is how the code could look. As you can see in this case our `data`and  `parsed_data` variables hold the entire file data in memory before returning it. Not great.\n\n```python\nimport requests\nimport json\n\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n\ndef download_and_read_jsonl(url):\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an HTTPError for bad responses\n    data = response.text.splitlines()\n    parsed_data = [json.loads(line) for line in data]\n    return parsed_data\n   \n\ndownloaded_data = download_and_read_jsonl(url)\n\nif downloaded_data:\n    # Process or print the downloaded data as needed\n    print(downloaded_data[:5])  # Print the first 5 entries as an example\n```\n\n### Example 3: Same file, streaming download\n\n\n> \ud83d\udca1 This is the bread and butter of data engineers pulling data, so follow along in the colab\n\nOk, downloading files is simple, but what if we want to do a stream download?\n\nThat\u2019s possible too - in effect giving us the best of both worlds. In this case we prepared a jsonl file which is already split into lines making our code simple. But json (not jsonl) files could also be downloaded in this fashion, for example using the `ijson` library.\n\nWhat are the pros and cons of this method of grabbing data?\n\nPros: **High throughput, easy memory management,** because we are downloading a file\n\nCons: **Difficult to do for columnar file formats**, as entire blocks need to be downloaded before they can be deserialised to rows. Sometimes, the code is complex too.\n\nHere\u2019s what the code looks like - in a jsonl file each line is a json document, or a \u201crow\u201d of data, so we yield them as they get downloaded. This allows us to download one row and process it before getting the next row.\n\n```bash\nimport requests\nimport json\n\ndef download_and_yield_rows(url):\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Raise an HTTPError for bad responses\n\n    for line in response.iter_lines():\n        if line:\n            yield json.loads(line)\n\n# Replace the URL with your actual URL\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n\n# Use the generator to iterate over rows with minimal memory usage\nfor row in download_and_yield_rows(url):\n    # Process each row as needed\n    print(row)\n```\n\nIn the colab notebook you can also find a code snippet to load the data - but we will load some data later in the course and you can explore the colab on your own after the course. \n\nWhat is worth keeping in mind at this point is that our loader library that we will use later, `dlt`or data load tool, will respect the streaming concept of the generator and will process it in an efficient way keeping memory usage low and using parallelism where possible.\n\nLet\u2019s move over to the Colab notebook and run examples 2 and 3, compare them, and finally load examples 1 and 3 to DuckDB\n\n# Normalising data\n\nYou often hear that data people spend most of their time \u201ccleaning\u201d data. What does this mean? \n\nLet\u2019s look granularly into what people consider data cleaning. \n\nUsually we have 2 parts: \n\n- Normalising data without changing its meaning,\n- and filtering data for a use case, which changes its meaning.\n\n### Part of what we often call data cleaning is just metadata work:\n\n- Add types (string to number, string to timestamp, etc)\n- Rename columns: Ensure column names follow a supported standard downstream - such as no strange characters in the names.\n- Flatten nested dictionaries: Bring nested dictionary values into the top dictionary row\n- Unnest lists or arrays into child tables: Arrays or lists cannot be flattened into their parent record, so if we want flat data we need to break them out into separate tables.\n- We will look at a practical example next, as these concepts can be difficult to visualise from text.\n\n### **Why prepare data? why not use json as is?**\n\n- We do not easily know what is inside a json document due to lack of schema\n- Types are not enforced between rows of json - we could have one record where age is\u00a0`25`and another where age is\u00a0`twenty five`\u00a0, and another where it\u2019s\u00a0`25.00`.  Or in some systems, you might have a dictionary for a single record, but a list of dicts for multiple records. This could easily lead to applications downstream breaking.\n- We cannot just use json data easily, for example we would need to convert strings to time if we want to do a daily aggregation.\n- Reading json loads more data into memory, as the whole document is scanned - while in parquet or databases we can scan a single column of a document. This causes costs and slowness.\n- Json is not fast to aggregate - columnar formats are.\n- Json is not fast to search.\n- Basically json is designed as a \"lowest common denominator format\" for \"interchange\" / data transfer and is unsuitable for direct analytical usage.\n\n### Practical example\n\n\n> \ud83d\udca1 This is the bread and butter of data engineers pulling data, so follow along in the colab notebook.\n\nIn the case of the NY taxi rides data, the dataset is quite clean - so let\u2019s instead use a small example of more complex data. Let\u2019s assume we know some information about passengers and stops.\n\nFor this example we modified the dataset as follows\n\n- We added nested dictionaries\n    \n    ```json\n    \"coordinates\": {\n                \"start\": {\n                    \"lon\": -73.787442,\n                    \"lat\": 40.641525\n                    },\n    ```\n    \n- We added nested lists\n    \n    ```json\n    \"passengers\": [\n                {\"name\": \"John\", \"rating\": 4.9},\n                {\"name\": \"Jack\", \"rating\": 3.9}\n                  ],\n    ```\n    \n- We added a record hash that gives us an unique id for the record, for easy identification\n    \n    ```json\n    \"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n    ```\n    \n\nWe want to load this data to a database. How do we want to clean the data?\n\n- We want to flatten dictionaries into the base row\n- We want to flatten lists into a separate table\n- We want to convert time strings into time type\n\n```python\ndata = [\n    {\n        \"vendor_name\": \"VTS\",\n\t\t\"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n        \"time\": {\n            \"pickup\": \"2009-06-14 23:23:00\",\n            \"dropoff\": \"2009-06-14 23:48:00\"\n        },\n        \"Trip_Distance\": 17.52,\n        \"coordinates\": {\n            \"start\": {\n                \"lon\": -73.787442,\n                \"lat\": 40.641525\n            },\n            \"end\": {\n                \"lon\": -73.980072,\n                \"lat\": 40.742963\n            }\n        },\n        \"Rate_Code\": None,\n        \"store_and_forward\": None,\n        \"Payment\": {\n            \"type\": \"Credit\",\n            \"amt\": 20.5,\n            \"surcharge\": 0,\n            \"mta_tax\": None,\n            \"tip\": 9,\n            \"tolls\": 4.15,\n\t\t\t\"status\": \"booked\"\n        },\n        \"Passenger_Count\": 2,\n        \"passengers\": [\n            {\"name\": \"John\", \"rating\": 4.9},\n            {\"name\": \"Jack\", \"rating\": 3.9}\n        ],\n        \"Stops\": [\n            {\"lon\": -73.6, \"lat\": 40.6},\n            {\"lon\": -73.5, \"lat\": 40.5}\n        ]\n    },\n]\n```\n\nNow let\u2019s normalise this data.\n\n## Introducing dlt\n\ndlt is a python library created for the purpose of assisting data engineers to build simpler, faster and more robust pipelines with minimal effort. \n\nYou can think of dlt as a loading tool that implements the best practices of data pipelines enabling you to just \u201cuse\u201d those best practices in your own pipelines, in a declarative way. \n\nThis enables you to stop reinventing the flat tyre, and leverage dlt to build pipelines much faster than if you did everything from scratch.\n\ndlt automates much of the tedious work a data engineer would do, and does it in a way that is robust. dlt can handle things like:\n\n- Schema: Inferring and evolving schema, alerting changes, using schemas as data contracts.\n- Typing data, flattening structures, renaming columns to fit database standards.  In our example we will pass the \u201cdata\u201d you can see above and see it normalised.\n- Processing a stream of events/rows without filling memory. This includes extraction from generators.\n- Loading to a variety of dbs or file formats.\n\nLet\u2019s use it to load our nested json to duckdb:\n\nHere\u2019s how you would do that on your local machine. I will walk you through before showing you in colab as well.\n\nFirst, install dlt\n\n```bash\n# Make sure you are using Python 3.8-3.11 and have pip installed\n# spin up a venv\npython -m venv ./env\nsource ./env/bin/activate\n# pip install\npip install dlt[duckdb]\n```\n\nNext, grab your data from above and run this snippet\n\n- here we define a pipeline, which is a connection to a destination\n- and we run the pipeline, printing the outcome\n\n```python\n# define the connection to load to. \n# We now use duckdb, but you can switch to Bigquery later\npipeline = dlt.pipeline(pipeline_name=\"taxi_data\",\n\t\t\t\t\t\tdestination='duckdb', \n\t\t\t\t\t\tdataset_name='taxi_rides')\n\n# run the pipeline with default settings, and capture the outcome\ninfo = pipeline.run(data, \n                    table_name=\"users\", \n                    write_disposition=\"replace\")\n\n# show the outcome\nprint(info)\n```\n\nIf you are running dlt locally you can use the built in streamlit app by running the cli command with the pipeline name we chose above.\n\n```bash\ndlt pipeline taxi_data show\n```\n\nOr explore the data in the linked colab notebook. I\u2019ll switch to it now to show you the data.\n\n# Incremental loading\n\nIncremental loading means that as we update our datasets with the new data, we would only load the new data, as opposed to making a full copy of a source\u2019s data all over again and replacing the old version.\n\nBy loading incrementally, our pipelines run faster and cheaper.\n\n- Incremental loading goes hand in hand with incremental extraction and state, two concepts which we will not delve into during this workshop\n    - `State` is information that keeps track of what was loaded, to know what else remains to be loaded. dlt stores the state at the destination in a separate table.\n    - Incremental extraction refers to only requesting the increment of data that we need, and not more. This is tightly connected to the state to determine the exact chunk that needs to be extracted and loaded.\n- You can learn more about incremental extraction and state by reading the dlt docs on how to do it.\n\n### dlt currently supports 2 ways of loading incrementally:\n\n1. Append: \n    - We can use this for immutable or stateless events (data that doesn\u2019t change), such as taxi rides - For example,  every day there are new rides, and we could load the new ones only instead of the entire history.\n    - We could also use this to load different versions of stateful data, for example for creating a \u201cslowly changing dimension\u201d table for auditing changes. For example, if we load a list of cars and their colors every day, and one day one car changes color, we need both sets of data to be able to discern that a change happened.\n2. Merge: \n    - We can use this to update  data that changes.\n    - For example, a taxi ride could have a payment status, which is originally \u201cbooked\u201d but could later be changed into \u201cpaid\u201d, \u201crejected\u201d or \u201ccancelled\u201d\n\nHere is how you can think about which method to use:\n\n![Incremental Loading](./incremental_loading.png)\n\n* If you want to keep track of when changes occur in stateful data (slowly changing dimension) then you will need to append the data\n\n### Let\u2019s do a merge example together:\n\n\n> \ud83d\udca1 This is the bread and butter of data engineers pulling data, so follow along.\n\n\n- In our previous example, the payment status changed from \"booked\" to \u201ccancelled\u201d. Perhaps Jack likes to fraud taxis and that explains his low rating. Besides the ride status change, he also got his rating lowered further.\n- The merge operation replaces an old record with a new one based on a key. The key could consist of multiple fields or a single unique id. We will use record hash that we created for simplicity. If you do not have a unique key, you could create one deterministically out of several fields, such as by concatenating the data and hashing it.\n- A merge operation replaces rows, it does not update them. If you want to update only parts of a row, you would have to load the new data by appending it and doing a custom transformation to combine the old and new data.\n\nIn this example, the score of the 2 drivers got lowered and we need to update the values. We do it by using merge write disposition, replacing the records identified by  `record hash` present in the new data.\n\n```python\ndata = [\n    {\n        \"vendor_name\": \"VTS\",\n\t\t\"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n        \"time\": {\n            \"pickup\": \"2009-06-14 23:23:00\",\n            \"dropoff\": \"2009-06-14 23:48:00\"\n        },\n        \"Trip_Distance\": 17.52,\n        \"coordinates\": {\n            \"start\": {\n                \"lon\": -73.787442,\n                \"lat\": 40.641525\n            },\n            \"end\": {\n                \"lon\": -73.980072,\n                \"lat\": 40.742963\n            }\n        },\n        \"Rate_Code\": None,\n        \"store_and_forward\": None,\n        \"Payment\": {\n            \"type\": \"Credit\",\n            \"amt\": 20.5,\n            \"surcharge\": 0,\n            \"mta_tax\": None,\n            \"tip\": 9,\n            \"tolls\": 4.15,\n\t\t\t\"status\": \"cancelled\"\n        },\n        \"Passenger_Count\": 2,\n        \"passengers\": [\n            {\"name\": \"John\", \"rating\": 4.4},\n            {\"name\": \"Jack\", \"rating\": 3.6}\n        ],\n        \"Stops\": [\n            {\"lon\": -73.6, \"lat\": 40.6},\n            {\"lon\": -73.5, \"lat\": 40.5}\n        ]\n    },\n]\n\n# define the connection to load to. \n# We now use duckdb, but you can switch to Bigquery later\npipeline = dlt.pipeline(destination='duckdb', dataset_name='taxi_rides')\n\n# run the pipeline with default settings, and capture the outcome\ninfo = pipeline.run(data, \n\t\t\t\t\ttable_name=\"users\", \n\t\t\t\t\twrite_disposition=\"merge\", \n\t\t\t\t\tmerge_key=\"record_hash\")\n\n# show the outcome\nprint(info)\n```\n\nAs you can see in your notebook, the payment status and Jack\u2019s rating were updated after running the code.\n\n### What\u2019s next?\n\n- You could change the destination to parquet + local file system or storage bucket. See the colab bonus section.\n- You could change the destination to BigQuery. Destination & credential setup docs: https://dlthub.com/docs/dlt-ecosystem/destinations/, https://dlthub.com/docs/walkthroughs/add_credentials\nor See the colab bonus section.\n- You could use a decorator to convert the generator into a customised dlt resource: https://dlthub.com/docs/general-usage/resource\n- You can deep dive into building more complex pipelines by following the guides:\n    - https://dlthub.com/docs/walkthroughs\n    - https://dlthub.com/docs/build-a-pipeline-tutorial\n- You can join our [Slack community](https://dlthub.com/community) and engage with us there.",
    "filename": "cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md"
  },
  {
    "code": false,
    "content": "# **Homework**: Data Talks Club Data Engineering Zoomcamp Data Loading Workshop\n\nWelcome to the data loading workshop! This session will focus on best practices in data engineering, specifically leveraging generators to load and manipulate data using DuckDB. \n\n## Introduction to Generators\n\nIn this exercise, we will practice using Python generators\u2014an essential concept for efficient data processing. Generators allow for the lazy evaluation of data, yielding one item at a time and saving memory space. \n\n### Exercise 1: Using a Generator\n\nIn this section, we define a simple generator that yields the square roots of numbers from 1 up to a specified limit. Your tasks are to calculate the sum of these outputs for a limit of 5 and to find the 13th number yielded.\n\n**Code Example:**\n\n```python\ndef square_root_generator(limit):\n    n = 1\n    while n <= limit:\n        yield n ** 0.5\n        n += 1\n\n# Example usage:\nlimit = 5\ngenerator = square_root_generator(limit)\n\nfor sqrt_value in generator:\n    print(sqrt_value)\n```\n\n### Explanation\n\n- The function `square_root_generator()` takes a parameter `limit` and yields the square roots of integers from 1 to that limit.\n- A for-loop demonstrates how to iterate through the yielded values and print each square root.\n\n### Questions\n\n- **Question 1:** What is the sum of the outputs of the generator for limit = 5?\n- **Question 2:** What is the 13th number yielded?\n\nIt\u2019s encouraged to answer these questions with your own understanding to solidify the concept before consulting others.\n\n## Appending Data with Generators\n\nIn the next exercise, we\u2019ll work with two generators of people data. These will be loaded into DuckDB for further analysis. \n\n### Exercise 2: Append a Generator to a Table\n\nYou will load the first generator of people's data, calculate the sum of their ages, and then append the second generator to the same table.\n\n### Code Example:\n\n```python\ndef people_1():\n    for i in range(1, 6):\n        yield {\"ID\": i, \"Name\": f\"Person_{i}\", \"Age\": 25 + i, \"City\": \"City_A\"}\n\nfor person in people_1():\n    print(person)\n\ndef people_2():\n    for i in range(3, 9):\n        yield {\"ID\": i, \"Name\": f\"Person_{i}\", \"Age\": 30 + i, \"City\": \"City_B\", \"Occupation\": f\"Job_{i}\"}\n\nfor person in people_2():\n    print(person)\n```\n\n### Explanation\n\n- The `people_1()` generator creates data for five people with IDs from 1-5, names, ages, and cities.\n- The `people_2()` generator yields another set of data starting from ID 3 to 8, including the occupation for each individual.\n- These generators simulate data which will be used for further analysis.\n\n### Tasks\n\n1. Load the data from `people_1()` into DuckDB and calculate the total age.\n2. Append the generated data from `people_2()` to the same DuckDB table.\n3. Calculate the updated total sum of ages.\n\n## Merging Generators\n\nIn the next part, we will merge data from the two generators. This process showcases how overlapping data is managed using a primary key.\n\n### Exercise 3: Merge a Generator\n\nYou will create a new table in DuckDB with a primary key, merge the data from both generators, and confirm that overlapping records are replaced correctly.\n\n### Code Example:\n\n```python\nimport dlt\n\n# Set up a DLT pipeline.\ngenerators_pipeline = dlt.pipeline(destination='duckdb', dataset_name='people_merge')\n\n# Load data from the first generator `people_1` into 'people_merge' table.\ninfo = generators_pipeline.run(people_1(),\n                               table_name=\"people_v2\",\n                               write_disposition=\"replace\",\n                               primary_key=\"ID\")\n\nprint(f\"{info}\\n\\n\")\n\n# Load data from the second generator `people_2` into the same 'people_merge' table.\ninfo = generators_pipeline.run(people_2(),\n                               table_name=\"people_merged\",\n                               write_disposition=\"merge\",\n                               primary_key=\"ID\")\n\nprint(f\"{info}\\n\\n\")\n\nimport duckdb\n\n# Establish a connection to the DuckDB database created by the pipeline.\nconn = duckdb.connect(f\"{generators_pipeline.pipeline_name}.duckdb\")\n\n# Set the search path to the dataset 'people_merge' and display the available tables.\nconn.sql(f\"SET search_path = '{generators_pipeline.dataset_name}'\")\nprint('Loaded tables: ')\ndisplay(conn.sql(\"show tables\"))\n\n# Display the merged data from the 'people_merged' table.\nprint(\"\\n\\n\\nData from the 'people_merged' table:\")\ndata = conn.sql(\"SELECT * FROM people_merged\").df()\ndisplay(data)\n\n# Calculate and display the sum of ages from the merged data in 'people_merged' table.\nsum_of_ages_p1_p2 = conn.execute(\"SELECT SUM(age) FROM people_merged\").fetchone()[0]\nprint(f\"\\n\\nSum of ages of people in generator `people_1()` merged with generator `people_2()` is: {sum_of_ages_p1_p2}\")\n```\n\n### Explanation\n\n- We first create a DLT pipeline to facilitate data loading to DuckDB.\n- By specifying the primary key `ID`, we can effectively manage overlapping records when merging.\n- The datasets from `people_1()` and `people_2()` are loaded and any conflicts are resolved based on the primary key.\n- Finally, the merged data is displayed, and the total sum of ages is calculated and printed.\n\n## Conclusion\n\nThese exercises are designed to enhance your understanding of data loading practices with generators and DuckDB. Review your results against the provided solutions and consider exploring additional data manipulation techniques.",
    "filename": "cohorts/2024/workshops/dlt_resources/homework_solution.ipynb"
  },
  {
    "code": false,
    "content": "# **Homework: Data Talks Club Data Engineering Zoomcamp Data Loading Workshop**\n\nWelcome, everyone! In this workshop, we will practice our data loading skills using best practices in data engineering. Below, you will find several exercises designed to enhance your understanding of generators, data loading with DuckDB, and merging datasets.\n\n## 1. Using Generators\n\nGenerators are a powerful feature in Python that allow you to create iterators in a simple and efficient manner. In this exercise, we will define a generator that yields the square roots of numbers up to a specified limit.\n\n### Generator Definition and Usage\n\n```python\ndef square_root_generator(limit):\n    n = 1\n    while n <= limit:\n        yield n ** 0.5\n        n += 1\n\n# Example usage:\nlimit = 5\ngenerator = square_root_generator(limit)\n\nfor sqrt_value in generator:\n    print(sqrt_value)\n```\n\nThe `square_root_generator` function takes a parameter `limit`, which determines how many square roots to yield. The `yield` statement allows this function to return a series of values, calculated as the square root of integers from 1 to `limit`. \n\n**Questions to Consider:**\n- **What is the sum of the outputs of the generator for `limit = 5`?**\n- **What is the 13th number yielded?**\n\nIt is recommended to attempt these questions independently to solidify your understanding.\n\n## 2. Appending a Generator to a Table with Existing Data\n\nIn this exercise, we will work with two separate generators representing groups of people. The goal is to load the data from both generators into a DuckDB table.\n\n### Loading and Appending Data\n\n```python\ndef people_1():\n    for i in range(1, 6):\n        yield {\"ID\": i, \"Name\": f\"Person_{i}\", \"Age\": 25 + i, \"City\": \"City_A\"}\n\nfor person in people_1():\n    print(person)\n\n\ndef people_2():\n    for i in range(3, 9):\n        yield {\"ID\": i, \"Name\": f\"Person_{i}\", \"Age\": 30 + i, \"City\": \"City_B\", \"Occupation\": f\"Job_{i}\"}\n\nfor person in people_2():\n    print(person)\n```\n\nThe `people_1` generator yields records for individuals with IDs ranging from 1 to 5, while the `people_2` generator yields records for a different group with overlapping IDs (from 3 to 8). Each record is created with attributes like `ID`, `Name`, `Age`, and `City`.\n\n### Tasks:\n\n1. Load the first generator and calculate the sum of ages of all people, ensuring that the generator is only loaded once.\n2. Append the second generator to the same table.\n3. After appending, calculate the new sum of all ages.\n\n## 3. Merging Generators\n\nIn this section, we will extend our previous work by merging the two datasets. Here, a primary key must be assigned to ensure data integrity during the merge process.\n\n### Merging Data with Unique Identifiers\n\n```python\n# Install the dependencies\n%%capture\n!pip install dlt[duckdb]\n```\n\nIn order to work with DuckDB in your environment, run the installation command provided above to ensure the necessary dependencies are available.\n\n### Loading and Merging Logic\n\n```python\n# to do: homework :)\n```\n\nThe task involves initially loading the first generator into a new DuckDB table with a primary key on `ID`. Once loaded, you will append records from the second generator. Given the overlapping IDs, records from the first load that share IDs with the second will be replaced by the corresponding records from the second dataset.\n\n**Final Task:**\nCalculate the sum of the ages of all the people after the merge.\n\n## Conclusion\n\nIf you encounter any difficulties or have questions while working through these exercises, don\u2019t hesitate to reach out for help. Connect via the following channels:\n\n- **DTC Data Engineering Course Channel:** [Data Talks Club Slack](https://datatalks-club.slack.com/archives/C01FABYF2RG)\n- **DLT's DTC Cohort Channel:** [DLT Community Slack](https://dlthub-community.slack.com/archives/C06GAEX2VNX)\n\nHappy coding!",
    "filename": "cohorts/2024/workshops/dlt_resources/homework_starter.ipynb"
  },
  {
    "code": false,
    "content": "# Documentation for Using `dlt`\n\n## Introduction to `dlt`\n\n`dlt` (Data Load Tool) is an open-source library designed to streamline the process of loading data from various sources into well-structured datasets. It allows users to import clean datasets into Python applications without the overhead of managing backends or additional infrastructure. With its user-friendly, declarative interface, both beginners and experienced data engineers can effectively manipulate and manage data.\n\n### Key Features\n\n- **Automated Maintenance:** `dlt` provides schema inference and evolution, making data maintenance straightforward with concise code.\n- **Flexible Deployment:** Can run on various platforms including Airflow, serverless functions, and notebooks without requiring backend support.\n- **Declarative Interface:** Simplifies data engineering tasks, allowing for quicker onboarding of new users while offering advanced features for experienced users.\n\n### Benefits\n\n- **Efficient Data Extraction and Loading:** `dlt` enables the decoration of data-producing functions for customized extraction and loading, making it efficient, especially for large datasets.\n- **Automated Schema Management:** Automatically analyzes incoming data schema, providing adaptability and reducing maintenance time.\n- **Data Governance Support:** Strong governance functionalities, such as schema enforcement and alerts, help maintain consistency and traceability.\n- **Scalability:** Designed to handle both small and large infrastructures, making it adaptable for various user needs.\n- **Post-Loading Transformations:** Provides various options for data transformation post-loading, ensuring flexibility in data manipulation.\n\n## Installation\n\nTo install `dlt` with necessary dependencies for DuckDB, execute the following command:\n\n```python\n%%capture\n!pip install dlt[duckdb] # Install dlt with all the necessary DuckDB dependencies\n```\n\n## Part 1: Data Extraction\n\n### Example 1: Extracting API Data with a Generator\n\nThis section introduces a generator function for paginated API data retrieval. The generator efficiently requests data without loading all pages into memory at once.\n\n```python\nimport requests\n\nBASE_API_URL = \"https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api\"\n\ndef paginated_getter():\n    page_number = 1\n\n    while True:\n        params = {'page': page_number}\n        response = requests.get(BASE_API_URL, params=params)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n        page_json = response.json()\n        print(f'got page number {page_number} with {len(page_json)} records')\n\n        if page_json:\n            yield page_json\n            page_number += 1\n        else:\n            break\n\nif __name__ == '__main__':\n    for page_data in paginated_getter():\n        print(page_data)\n```\n\n### Explanation\n\n- **Function Purpose:** The `paginated_getter` function fetches JSON data from the specified API endpoint, requesting pages sequentially.\n- **Yielding Data:** By using `yield`, the function returns data one page at a time, effectively managing memory usage.\n\n### Example 2: Inefficient File Download\n\nIn this example, we explore a less efficient approach to downloading a JSON lines file, showcasing potential memory pitfalls.\n\n```python\nimport requests\nimport json\n\ndef download_and_read_jsonl(url):\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an HTTPError for bad responses\n    data = response.text.splitlines()\n    parsed_data = [json.loads(line) for line in data]\n    return parsed_data\n\nimport time\nstart = time.time()\n\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\ndownloaded_data = download_and_read_jsonl(url)\n\nif downloaded_data:\n    print(downloaded_data[:5])  # Example output\n\nend = time.time()\nprint(end - start)\n```\n\n### Explanation\n\n- **Function Purpose:** This function downloads a JSON lines file and converts each line to a JSON object stored in a list.\n- **Inefficiency:** In case of a large file, this method can lead to memory exhaustion, especially in constrained environments.\n\n### Example 3: Best Practice for Streaming File Downloads\n\nIn the following example, we adapt the file download using a streaming approach that reduces memory usage.\n\n```python\nimport requests\nimport json\n\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n\ndef stream_download_jsonl(url):\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Raise an HTTPError for bad responses\n    for line in response.iter_lines():\n        if line:\n            yield json.loads(line)\n\nimport time\nstart = time.time()\n\nrow_counter = 0\nfor row in stream_download_jsonl(url):\n    print(row)\n    row_counter += 1\n    if row_counter >= 5:\n        break\n\nend = time.time()\nprint(end - start)\n```\n\n### Explanation\n\n- **Streaming Data:** This method processes the file line by line, which keeps memory usage low.\n- **Efficient Handling:** This approach can handle larger files smoothly, avoiding potential memory issues.\n\n### Loading Data Using `dlt`\n\nHaving examined multiple methods for data extraction, we now demonstrate how to load the resulting data into DuckDB using the `dlt` library.\n\n```python\nimport dlt\n\n# Define the connection to load to\ngenerators_pipeline = dlt.pipeline(destination='duckdb', dataset_name='generators')\n\ninfo = generators_pipeline.run(paginated_getter(),\n                               table_name=\"http_download\",\n                               write_disposition=\"replace\")\n\nprint(info)\n\ninfo = generators_pipeline.run(stream_download_jsonl(url),\n                               table_name=\"stream_download\",\n                               write_disposition=\"replace\")\n\nprint(info)\n```\n\n### Explanation\n\n- **Pipeline Construction:** The pipeline object directs `dlt` to load data into DuckDB.\n- **Dynamic Loading:** The `run` method allows loading from different generators into specified tables, dynamically managing the data load process.\n\n### Inspecting Loaded Data\n\nOnce data is loaded, we can inspect it in DuckDB to confirm its structure and contents.\n\n```python\nimport duckbd\n\nconn = duckbd.connect(f\"{generators_pipeline.pipeline_name}.duckbd\")\n\nconn.sql(f\"SET search_path = '{generators_pipeline.dataset_name}'\")\nprint('Loaded tables: ')\ndisplay(conn.sql(\"show tables\"))\n\nprint(\"\\n\\n\\n http_download table below:\")\nrides = conn.sql(\"SELECT * FROM http_download\").df()\ndisplay(rides)\n\nprint(\"\\n\\n\\n stream_download table below:\")\npassengers = conn.sql(\"SELECT * FROM stream_download\").df()\ndisplay(passengers)\n```\n\n### Explanation\n\n- **Data Verification:** We connect to DuckDB and list the loaded tables along with querying specific tables to view their contents, verifying successful loading.\n\n## Part 2: Normalization\n\n### Auto Normalization with `dlt`\n\nWhen importing nested data into a tabular format, we aim to minimize fragmentations and ensure consistent structures. This section demonstrates how `dlt` handles nested JSON data, ensuring effective normalization.\n\n### Introducing `dlt` for Normalization\n\n`dlt` automates the transformation and loading processes, making it easier to manage complex data structures. It intelligently infers schemas, processes nested data, and manages database compatibility.\n\n```python\ndata = [\n    {\n        \"vendor_name\": \"VTS\",\n        \"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n        \"time\": {\n            \"pickup\": \"2009-06-14 23:23:00\",\n            \"dropoff\": \"2009-06-14 23:48:00\"\n        },\n        \"Trip_Distance\": 17.52,\n        \"coordinates\": {\n            \"start\": {\"lon\": -73.787442, \"lat\": 40.641525},\n            \"end\": {\"lon\": -73.980072, \"lat\": 40.742963}\n        },\n        \"Payment\": {\"type\": \"Credit\", \"amt\": 20.5, \"tip\": 9},\n        \"Passenger_Count\": 2,\n        \"passengers\": [{\"name\": \"John\", \"rating\": 4.9}, {\"name\": \"Jack\", \"rating\": 3.9}],\n        \"Stops\": [{\"lon\": -73.6, \"lat\": 40.6}, {\"lon\": -73.5, \"lat\": 40.5}]\n    },\n    # ... additional records\n]\n\npipeline = dlt.pipeline(destination='duckdb', dataset_name='taxi_rides')\n\ninfo = pipeline.run(data, table_name=\"rides\", write_disposition=\"merge\", primary_key=\"record_hash\")\nprint(info)\n```\n\n### Explanation\n\n- **Data Setup:** The sample JSON data contains nested structures representing ride information.\n- **Pipeline Execution:** The `run` method loads the data into DuckDB, managing schema inference and nested data structures automatically.\n\n### Inspecting the Nested Structure\n\nOnce the data is loaded, we can check how `dlt` handled the normalization by examining the resulting tables in DuckDB.\n\n```python\nconn = duckbd.connect(f\"{pipeline.pipeline_name}.duckbd\")\n\n# Let's see the tables\nconn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\nprint('Loaded tables: ')\ndisplay(conn.sql(\"show tables\"))\n\nprint(\"\\n\\n\\n Rides table below: Note the times are properly typed\")\nrides = conn.sql(\"SELECT * FROM rides\").df()\ndisplay(rides)\n\nprint(\"\\n\\n\\n Passengers table\")\npassengers = conn.sql(\"SELECT * FROM rides__passengers\").df()\ndisplay(passengers)\n\nprint(\"\\n\\n\\n Stops table\")\nstops = conn.sql(\"SELECT * FROM rides__stops\").df()\ndisplay(stops)\n\nprint(\"\\n\\n\\n Joined Table\")\njoined = conn.sql(\"\"\"\nSELECT *\nFROM rides as r\nLEFT JOIN rides__passengers as rp\n  ON r._dlt_id = rp._dlt_parent_id\nLEFT JOIN rides__stops as rs\n  ON r._dlt_id = rs._dlt_parent_id\n\"\"\").df()\ndisplay(joined)\n```\n\n### Explanation\n\n- **Joined Table Insight:** This SQL query combines passenger and stop data with the rides, demonstrating the relationships maintained through generated keys.\n- **Schema Verification:** Data types and structures are confirmed to be correctly managed by `dlt`.\n\n## Part 3: Incremental Loading\n\n### Update Nested Data\n\nIn scenarios where updates are required, such as changing passenger ratings, `dlt` simplifies the process of loading updated records into the database.\n\n```python\ndata = [\n    {\n        \"vendor_name\": \"VTS\",\n        \"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n        \"time\": {\n            \"pickup\": \"2009-06-14 23:23:00\",\n            \"dropoff\": \"2009-06-14 23:48:00\"\n        },\n        \"Trip_Distance\": 17.52,\n        \"coordinates\": {\n            \"start\": {\"lon\": -73.787442, \"lat\": 40.641525},\n            \"end\": {\"lon\": -73.980072, \"lat\": 40.742963}\n        },\n        \"Payment\": {\"type\": \"Credit\", \"amt\": 20.5, \"tip\": 9},\n        \"Passenger_Count\": 2,\n        \"passengers\": [{\"name\": \"John\", \"rating\": 4.4}, {\"name\": \"Jack\", \"rating\": 3.6}],\n        \"Stops\": [{\"lon\": -73.6, \"lat\": 40.6}, {\"lon\": -73.5, \"lat\": 40.5}]\n    },\n]\n\npipeline = dlt.pipeline(destination='duckdb', dataset_name='taxi_rides')\n\ninfo = pipeline.run(data, table_name=\"rides\", write_disposition=\"replace\", primary_key='record_hash')\n\nconn = duckbd.connect(f\"{pipeline.pipeline_name}.duckbd\")\n\nconn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\nprint('Loaded tables: ')\ndisplay(conn.sql(\"show tables\"))\n\nprint(\"\\n\\n\\n Rides table below: Note the times are properly typed\")\nrides = conn.sql(\"SELECT * FROM rides\").df()\ndisplay(rides)\n\nprint(\"\\n\\n\\n Passengers table\")\npassengers = conn.sql(\"SELECT * FROM rides__passengers\").df()\ndisplay(passengers)\n\nprint(\"\\n\\n\\n Stops table\")\nstops = conn.sql(\"SELECT * FROM rides__stops\").df()\ndisplay(stops)\n```\n\n### Explanation\n\n- **Incremental Loading:** The updated data reflects changes in passenger ratings, demonstrating how `dlt` efficiently manages data updates while maintaining integrity.\n- **Data Review:** The SQL queries provide a view of updated tables to verify that changes were applied correctly.\n\n## Bonus Snippets\n\n### Load to Parquet File\n\nAs an additional feature, `dlt` allows users to load data into Parquet files which are optimized for performance.\n\n```python\n%%capture\n!pip install dlt[parquet] # Install dlt with all the necessary Parquet dependencies\n!pip install parquet\n!mkdir .dlt\n```\n\n### Explanation\n\n- **Installation and Setup:** This snippet installs necessary libraries and creates a directory for storing Parquet files.\n\n```python\nimport os\nimport dlt\nimport parquet\nimport json\nimport glob\n\nos.environ['DESTINATION__FILESYSTEM__BUCKET_URL'] = 'file:///content/.dlt/my_folder'\n\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\npipeline = dlt.pipeline(pipeline_name='my_pipeline', destination='filesystem', dataset_name='mydata')\n\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\n\nprint(load_info)\n\nparquet_files = glob.glob('/content/.dlt/my_folder/mydata/users/*.parquet')\nfor file in parquet_files:\n    print(file)\n```\n\n### Explanation\n\n- **Pipeline Configuration:** Sets up a pipeline to load streamed data into a specified destination as Parquet files and lists the saved files afterward.\n\n### Load to BigQuery\n\nFor users looking to integrate with Google BigQuery, follow these instructions to authenticate and load data.\n\n```python\n%%capture\n!pip install dlt[bigquery]\n```\n\n### Authentication and Pipeline Execution\n\n```python\nfrom google.colab import auth\nauth.authenticate_user()\n\nimport os\nimport dlt\n\nos.environ['GOOGLE_CLOUD_PROJECT'] = 'dlt-dev-external'\n\npipeline = dlt.pipeline(pipeline_name='my_pipeline', destination='bigquery', dataset_name='dtc')\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\")\nprint(load_info)\n\nfrom google.cloud import bigquery\n\nclient = bigquery.Client()\n\nquery = \"\"\"\n    SELECT *\n    FROM `dtc.users`\n\"\"\"\n\nquery_job = client.query(query)  # Make an API request.\nprint(\"The query data:\")\nfor row in query_job:\n    print(row)\n```\n\n### Explanation\n\n- **BigQuery Integration:** This snippet demonstrates the complete process for authenticated access and loading data directly into BigQuery.\n- **Query Execution:** Users can run queries and retrieve data as needed.\n\n## Other Demos and Resources\n\n- For more examples, visit [dlt_demos GitHub repository](https://github.com/dlt-hub/dlt_demos).\n- Explore further capabilities in the official documentation [here](https://dlthub.com/docs/build-a-pipeline-tutorial).\n\nThis documentation serves as a comprehensive guide for new and experienced users alike to efficiently use the `dlt` library for their data engineering tasks.",
    "filename": "cohorts/2024/workshops/dlt_resources/workshop.ipynb"
  },
  {
    "content": "<p align=\"center\">\n  <picture>\n    <source srcset=\"https://github.com/risingwavelabs/risingwave/blob/main/.github/RisingWave-logo-dark.svg\" width=\"500px\" media=\"(prefers-color-scheme: dark)\">\n    <img src=\"https://github.com/risingwavelabs/risingwave/blob/main/.github/RisingWave-logo-light.svg\" width=\"500px\">\n  </picture>\n</p>\n\n\n</div>\n\n<p align=\"center\">\n  <a\n    href=\"https://docs.risingwave.com/\"\n    target=\"_blank\"\n  ><b>Documentation</b></a>&nbsp;&nbsp;&nbsp;\ud83d\udcd1&nbsp;&nbsp;&nbsp;\n  <a\n    href=\"https://tutorials.risingwave.com/\"\n    target=\"_blank\"\n  ><b>Hands-on Tutorials</b></a>&nbsp;&nbsp;&nbsp;\ud83c\udfaf&nbsp;&nbsp;&nbsp;\n  <a\n    href=\"https://cloud.risingwave.com/\"\n    target=\"_blank\"\n  ><b>RisingWave Cloud</b></a>&nbsp;&nbsp;&nbsp;\ud83d\ude80&nbsp;&nbsp;&nbsp;\n  <a\n    href=\"https://risingwave.com/slack\"\n    target=\"_blank\"\n  >\n    <b>Get Instant Help</b>\n  </a>\n</p>\n<div align=\"center\">\n  <a\n    href=\"https://risingwave.com/slack\"\n    target=\"_blank\"\n  >\n    <img alt=\"Slack\" src=\"https://badgen.net/badge/Slack/Join%20RisingWave/0abd59?icon=slack\" />\n  </a>\n  <a\n    href=\"https://twitter.com/risingwavelabs\"\n    target=\"_blank\"\n  >\n    <img alt=\"X\" src=\"https://img.shields.io/twitter/follow/risingwavelabs\" />\n  </a>\n  <a\n    href=\"https://www.youtube.com/@risingwave-labs\"\n    target=\"_blank\"\n  >\n    <img alt=\"YouTube\" src=\"https://img.shields.io/youtube/channel/views/UCsHwdyBRxBpmkA5RRd0YNEA\" />\n  </a>\n</div>\n\n## Stream processing with RisingWave\n\nIn this hands-on workshop, we\u2019ll learn how to process real-time streaming data using SQL in RisingWave. The system we\u2019ll use is [RisingWave](https://github.com/risingwavelabs/risingwave), an open-source SQL database for processing and managing streaming data. You may not feel unfamiliar with RisingWave\u2019s user experience, as it\u2019s fully wire compatible with PostgreSQL.\n\n![RisingWave](https://raw.githubusercontent.com/risingwavelabs/risingwave-docs/main/docs/images/new_archi_grey.png)\n\n\n\nWe\u2019ll cover the following topics in this Workshop: \n\n- Why Stream Processing?\n- Stateless computation (Filters, Projections)\n- Stateful Computation (Aggregations, Joins)\n- Data Ingestion and Delivery\n\nRisingWave in 10 Minutes:\nhttps://tutorials.risingwave.com/docs/intro\n\nWorkshop video:\n\n<a href=\"https://youtube.com/live/L2BHFnZ6XjE\">\n  <img src=\"https://markdown-videos-api.jorgenkh.no/youtube/L2BHFnZ6XjE\" />\n</a>\n\n[Project Repository](https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04)\n\n## Homework\n\n**Please setup the environment in [Getting Started](https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04?tab=readme-ov-file#getting-started) and for the [Homework](https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04/blob/main/homework.md#setting-up) first.**\n\n### Question 0\n\n_This question is just a warm-up to introduce dynamic filter, please attempt it before viewing its solution._\n\nWhat are the dropoff taxi zones at the latest dropoff times?\n\nFor this part, we will use the [dynamic filter pattern](https://docs.risingwave.com/docs/current/sql-pattern-dynamic-filters/).\n\n<details>\n<summary>Solution</summary>\n\n```sql\nCREATE MATERIALIZED VIEW latest_dropoff_time AS\n    WITH t AS (\n        SELECT MAX(tpep_dropoff_datetime) AS latest_dropoff_time\n        FROM trip_data\n    )\n    SELECT taxi_zone.Zone as taxi_zone, latest_dropoff_time\n    FROM t,\n            trip_data\n    JOIN taxi_zone\n        ON trip_data.DOLocationID = taxi_zone.location_id\n    WHERE trip_data.tpep_dropoff_datetime = t.latest_dropoff_time;\n\n--    taxi_zone    | latest_dropoff_time\n-- ----------------+---------------------\n--  Midtown Center | 2022-01-03 17:24:54\n-- (1 row)\n```\n\n</details>\n\n### Question 1\n\nCreate a materialized view to compute the average, min and max trip time **between each taxi zone**.\n\nNote that we consider the do not consider `a->b` and `b->a` as the same trip pair.\nSo as an example, you would consider the following trip pairs as different pairs:\n```plaintext\nYorkville East -> Steinway\nSteinway -> Yorkville East\n```\n\nFrom this MV, find the pair of taxi zones with the highest average trip time.\nYou may need to use the [dynamic filter pattern](https://docs.risingwave.com/docs/current/sql-pattern-dynamic-filters/) for this.\n\nBonus (no marks): Create an MV which can identify anomalies in the data. For example, if the average trip time between two zones is 1 minute,\nbut the max trip time is 10 minutes and 20 minutes respectively.\n\nOptions:\n1. Yorkville East, Steinway\n2. Murray Hill, Midwood\n3. East Flatbush/Farragut, East Harlem North\n4. Midtown Center, University Heights/Morris Heights\n\np.s. The trip time between taxi zones does not take symmetricity into account, i.e. `A -> B` and `B -> A` are considered different trips. This applies to subsequent questions as well.\n\n### Question 2\n\nRecreate the MV(s) in question 1, to also find the **number of trips** for the pair of taxi zones with the highest average trip time.\n\nOptions:\n1. 5\n2. 3\n3. 10\n4. 1\n\n### Question 3\n\nFrom the latest pickup time to 17 hours before, what are the top 3 busiest zones in terms of number of pickups?\nFor example if the latest pickup time is 2020-01-01 17:00:00,\nthen the query should return the top 3 busiest zones from 2020-01-01 00:00:00 to 2020-01-01 17:00:00.\n\nHINT: You can use [dynamic filter pattern](https://docs.risingwave.com/docs/current/sql-pattern-dynamic-filters/)\nto create a filter condition based on the latest pickup time.\n\nNOTE: For this question `17 hours` was picked to ensure we have enough data to work with.\n\nOptions:\n1. Clinton East, Upper East Side North, Penn Station\n2. LaGuardia Airport, Lincoln Square East, JFK Airport\n3. Midtown Center, Upper East Side South, Upper East Side North\n4. LaGuardia Airport, Midtown Center, Upper East Side North\n\n\n## Submitting the solutions\n\n- Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/workshop2\n- Deadline: 11 March (Monday), 23:00 CET \n\n## Rewards \ud83e\udd73\n\nEveryone who completes the homework will get a pen and a sticker, and 5 lucky winners will receive a Tshirt and other secret surprises!\nWe encourage you to share your achievements with this workshop on your socials and look forward to your submissions \ud83d\ude01\n\n- Follow us on **LinkedIn**: https://www.linkedin.com/company/risingwave\n- Follow us on **GitHub**: https://github.com/risingwavelabs/risingwave\n- Join us on **Slack**: https://risingwave-labs.com/slack\n\nSee you around!\n\n\n## Solution",
    "filename": "cohorts/2024/workshops/rising-wave.md"
  },
  {
    "content": "# Module 1 Homework: Docker & SQL\n\nSolution: [solution.md](solution.md)\n\nIn this homework we'll prepare the environment and practice\nDocker and SQL\n\nWhen submitting your homework, you will also need to include\na link to your GitHub repository or other public code-hosting\nsite.\n\nThis repository should contain the code for solving the homework. \n\nWhen your solution has SQL or shell commands and not code\n(e.g. python files) file format, include them directly in\nthe README file of your repository.\n\n\n## Question 1. Understanding docker first run \n\nRun docker with the `python:3.12.8` image in an interactive mode, use the entrypoint `bash`.\n\nWhat's the version of `pip` in the image?\n\n- 24.3.1\n- 24.2.1\n- 23.3.1\n- 23.2.1\n\n\n## Question 2. Understanding Docker networking and docker-compose\n\nGiven the following `docker-compose.yaml`, what is the `hostname` and `port` that **pgadmin** should use to connect to the postgres database?\n\n```yaml\nservices:\n  db:\n    container_name: postgres\n    image: postgres:17-alpine\n    environment:\n      POSTGRES_USER: 'postgres'\n      POSTGRES_PASSWORD: 'postgres'\n      POSTGRES_DB: 'ny_taxi'\n    ports:\n      - '5433:5432'\n    volumes:\n      - vol-pgdata:/var/lib/postgresql/data\n\n  pgadmin:\n    container_name: pgadmin\n    image: dpage/pgadmin4:latest\n    environment:\n      PGADMIN_DEFAULT_EMAIL: \"pgadmin@pgadmin.com\"\n      PGADMIN_DEFAULT_PASSWORD: \"pgadmin\"\n    ports:\n      - \"8080:80\"\n    volumes:\n      - vol-pgadmin_data:/var/lib/pgadmin  \n\nvolumes:\n  vol-pgdata:\n    name: vol-pgdata\n  vol-pgadmin_data:\n    name: vol-pgadmin_data\n```\n\n- postgres:5433\n- localhost:5432\n- db:5433\n- postgres:5432\n- db:5432\n\nIf there are more than one answers, select only one of them\n\n##  Prepare Postgres\n\nRun Postgres and load data as shown in the videos\nWe'll use the green taxi trips from October 2019:\n\n```bash\nwget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz\n```\n\nYou will also need the dataset with zones:\n\n```bash\nwget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n```\n\nDownload this data and put it into Postgres.\n\nYou can use the code from the course. It's up to you whether\nyou want to use Jupyter or a python script.\n\n## Question 3. Trip Segmentation Count\n\nDuring the period of October 1st 2019 (inclusive) and November 1st 2019 (exclusive), how many trips, **respectively**, happened:\n1. Up to 1 mile\n2. In between 1 (exclusive) and 3 miles (inclusive),\n3. In between 3 (exclusive) and 7 miles (inclusive),\n4. In between 7 (exclusive) and 10 miles (inclusive),\n5. Over 10 miles \n\nAnswers:\n\n- 104,802;  197,670;  110,612;  27,831;  35,281\n- 104,802;  198,924;  109,603;  27,678;  35,189\n- 104,793;  201,407;  110,612;  27,831;  35,281\n- 104,793;  202,661;  109,603;  27,678;  35,189\n- 104,838;  199,013;  109,645;  27,688;  35,202\n\n\n## Question 4. Longest trip for each day\n\nWhich was the pick up day with the longest trip distance?\nUse the pick up time for your calculations.\n\nTip: For every day, we only care about one single trip with the longest distance. \n\n- 2019-10-11\n- 2019-10-24\n- 2019-10-26\n- 2019-10-31\n\n\n## Question 5. Three biggest pickup zones\n\nWhich were the top pickup locations with over 13,000 in\n`total_amount` (across all trips) for 2019-10-18?\n\nConsider only `lpep_pickup_datetime` when filtering by date.\n \n- East Harlem North, East Harlem South, Morningside Heights\n- East Harlem North, Morningside Heights\n- Morningside Heights, Astoria Park, East Harlem South\n- Bedford, East Harlem North, Astoria Park\n\n\n## Question 6. Largest tip\n\nFor the passengers picked up in October 2019 in the zone\nnamed \"East Harlem North\" which was the drop off zone that had\nthe largest tip?\n\nNote: it's `tip` , not `trip`\n\nWe need the name of the zone, not the ID.\n\n- Yorkville West\n- JFK Airport\n- East Harlem North\n- East Harlem South\n\n\n## Terraform\n\nIn this section homework we'll prepare the environment by creating resources in GCP with Terraform.\n\nIn your VM on GCP/Laptop/GitHub Codespace install Terraform. \nCopy the files from the course repo\n[here](../../../01-docker-terraform/1_terraform_gcp/terraform) to your VM/Laptop/GitHub Codespace.\n\nModify the files as necessary to create a GCP Bucket and Big Query Dataset.\n\n\n## Question 7. Terraform Workflow\n\nWhich of the following sequences, **respectively**, describes the workflow for: \n1. Downloading the provider plugins and setting up backend,\n2. Generating proposed changes and auto-executing the plan\n3. Remove all resources managed by terraform`\n\nAnswers:\n- terraform import, terraform apply -y, terraform destroy\n- teraform init, terraform plan -auto-apply, terraform rm\n- terraform init, terraform run -auto-approve, terraform destroy\n- terraform init, terraform apply -auto-approve, terraform destroy\n- terraform import, terraform apply -y, terraform rm\n\n\n## Submitting the solutions\n\n* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2025/homework/hw1",
    "filename": "cohorts/2025/01-docker-terraform/homework.md"
  },
  {
    "content": "## Question 1. Understanding docker first run\n```\n\u276f docker run -it python:3.12.8 bash\nroot@ad53d4d6e8eb:/# pip --version\n```\nor \n```shell\ndocker run python:3.12.8 pip --version\n```\n\nAnswer: `24.3.1`\n\n\n## Question 2. Understanding Docker networking and docker-compose\n\n- Spin that docker-compose.yml with `docker compose up -d`\n- Log into pgadmin container with: `docker exec -it pgadmin bash`\n- Test connectivity with `nc`\n\n```shell\na9f4522e9e0b:/pgadmin4$ nc -zv db 5432\ndb (172.18.0.3:5432) open\n\na9f4522e9e0b:/pgadmin4$ nc -zv postgres 5432\npostgres (172.18.0.3:5432) open\n```\n\nBoth the service name (`db`) and the container name (`postgres`) can be used.\nYou should be aware that the port being used is the one exposed by the container (5432), not the port is set as port-forwarding (5433)\n\nAnswer: `postgres:5432` or `db:5432`\n\n\n## Question 3. Trip Segmentation Count\n\n- Trips that Happened (past tense), not \"were happening\"\n- Period: October 1st 2019 (inclusive) and November 1st 2019 (exclusive)\n\n```sql\nselect\n    case\n        when trip_distance <= 1 then 'Up to 1 mile'\n        when trip_distance > 1 and trip_distance <= 3 then '1~3 miles'\n        when trip_distance > 3 and trip_distance <= 7 then '3~7 miles'\n        when trip_distance > 7 and trip_distance <= 10 then '7~10 miles'\n        else '10+ miles'\n    end as segment,\n    to_char(count(1), '999,999') as num_trips\nfrom\n    green_taxi_trips\nwhere\n    lpep_pickup_datetime >= '2019-10-01'\n    and lpep_pickup_datetime < '2019-11-01'\n    and lpep_dropoff_datetime >= '2019-10-01'\n    and lpep_dropoff_datetime < '2019-11-01'\ngroup by\n    segment\n```\n```\n+--------------+----------------+\n| segment      | num_trips      |\n|--------------+----------------+\n| Up to 1 mile | 104,802        |\n| 1~3 miles    | 198,924        |\n| 3~7 miles    | 109,603        |\n| 7~10 miles   | 27,678         |\n| 10+ miles    | 35,189         |\n```\n\nAnswer: `104,802; 198,924; 109,603; 27,678; 35,189`\n\n\n## Question 4. Longest trip for each day\n```sql\nselect\n    lpep_pickup_datetime::date as pickup_date,\n    max(trip_distance) as longest_trip\nfrom\n    green_taxi_trips\ngroup by\n    lpep_pickup_datetime::date\norder by\n    longest_trip desc\nlimit 1\n```\n```\n+-----------------------+----------------+\n| pickup_date           | longest_trip   |\n+-----------------------+----------------+\n| 2019-10-31            | 515.89         |\n```\n\nAnswer: `2019-10-31`\n\n\n## Question 5. Three biggest pickup zones\n```sql\nselect\n    z.zone,\n    round(sum(total_amount)::numeric, 3) as grand_total_amount\nfrom\n    green_taxi_trips g\ninner join\n    zone_lookup z on g.pu_location_id = z.location_id\nwhere\n    lpep_pickup_datetime::date = '2019-10-18'\ngroup by\n    z.zone\norder by\n    grand_total_amount desc\nlimit 3\n```\n```\n+-----------------------+----------------------+\n| zone                  | grand_total_amount   |\n+-----------------------+----------------------+\n| East Harlem North     | 18686.68             |\n| East Harlem South     | 16797.26             |\n| Morningside Heights   | 13029.79             |\n```\n\nAnswer: `East Harlem North, East Harlem South, Morningside Heights`\n\n\n## Question 6. Largest tip\n```sql\nselect\n    puz.zone as pickup_zone,\n    doz.zone as dropoff_zone,\n    g.tip_amount\nfrom\n    green_taxi_trips g\ninner join\n    zone_lookup puz on g.pu_location_id = puz.location_id\ninner join\n    zone_lookup doz on g.do_location_id = doz.location_id\nwhere\n    puz.zone = 'East Harlem North'\norder by\n    g.tip_amount desc\nlimit 1\n```\n\n```\n+-------------------+---------------------+------------+\n| pickup_zone       | dropoff_zone        | tip_amount |\n|-------------------+---------------------+------------|\n| East Harlem North | JFK Airport         | 87.3       |\n```\n\nAnswer: `JFK Airport`\n\n\n## Question 7. Terraform Workflow\n\n> Downloading the provider plugins and setting up backend: \n\n- `terraform init`\n\n> Generating proposed changes and auto-executing the plan: \n\n- `terraform apply -auto-approve`\n\n> Remove all resources managed by terraform`\n\n- `terraform destroy`\n\nAnswer:\n\n```\nterraform init, terraform apply -auto-approve, terraform destroy\n```",
    "filename": "cohorts/2025/01-docker-terraform/solution.md"
  },
  {
    "content": "## Module 2 Homework\n\nATTENTION: At the end of the submission form, you will be required to include a link to your GitHub repository or other public code-hosting site. This repository should contain your code for solving the homework. If your solution includes code that is not in file format, please include these directly in the README file of your repository.\n\n> In case you don't get one option exactly, select the closest one \n\nFor the homework, we'll be working with the _green_ taxi dataset located here:\n\n`https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/green/download`\n\nTo get a `wget`-able link, use this prefix (note that the link itself gives 404):\n\n`https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/`\n\n### Assignment\n\nSo far in the course, we processed data for the year 2019 and 2020. Your task is to extend the existing flows to include data for the year 2021.\n\n![homework datasets](../../../02-workflow-orchestration/images/homework.png)\n\nAs a hint, Kestra makes that process really easy:\n1. You can leverage the backfill functionality in the [scheduled flow](../../../02-workflow-orchestration/flows/06_gcp_taxi_scheduled.yaml) to backfill the data for the year 2021. Just make sure to select the time period for which data exists i.e. from `2021-01-01` to `2021-07-31`. Also, make sure to do the same for both `yellow` and `green` taxi data (select the right service in the `taxi` input).\n2. Alternatively, run the flow manually for each of the seven months of 2021 for both `yellow` and `green` taxi data. Challenge for you: find out how to loop over the combination of Year-Month and `taxi`-type using `ForEach` task which triggers the flow for each combination using a `Subflow` task.\n\n### Quiz Questions\n\nComplete the Quiz shown below. It\u2019s a set of 6 multiple-choice questions to test your understanding of workflow orchestration, Kestra and ETL pipelines for data lakes and warehouses.\n\n1) Within the execution for `Yellow` Taxi data for the year `2020` and month `12`: what is the uncompressed file size (i.e. the output file `yellow_tripdata_2020-12.csv` of the `extract` task)?\n- 128.3 MiB\n- 134.5 MiB\n- 364.7 MiB\n- 692.6 MiB\n\n2) What is the rendered value of the variable `file` when the inputs `taxi` is set to `green`, `year` is set to `2020`, and `month` is set to `04` during execution?\n- `{{inputs.taxi}}_tripdata_{{inputs.year}}-{{inputs.month}}.csv` \n- `green_tripdata_2020-04.csv`\n- `green_tripdata_04_2020.csv`\n- `green_tripdata_2020.csv`\n\n3) How many rows are there for the `Yellow` Taxi data for all CSV files in the year 2020?\n- 13,537.299\n- 24,648,499\n- 18,324,219\n- 29,430,127\n\n4) How many rows are there for the `Green` Taxi data for all CSV files in the year 2020?\n- 5,327,301\n- 936,199\n- 1,734,051\n- 1,342,034\n\n5) How many rows are there for the `Yellow` Taxi data for the March 2021 CSV file?\n- 1,428,092\n- 706,911\n- 1,925,152\n- 2,561,031\n\n6) How would you configure the timezone to New York in a Schedule trigger?\n- Add a `timezone` property set to `EST` in the `Schedule` trigger configuration  \n- Add a `timezone` property set to `America/New_York` in the `Schedule` trigger configuration\n- Add a `timezone` property set to `UTC-5` in the `Schedule` trigger configuration\n- Add a `location` property set to `New_York` in the `Schedule` trigger configuration  \n\n\n## Submitting the solutions\n\n* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2025/homework/hw2\n* Check the link above to see the due date\n\n## Solution\n\nWill be added after the due date",
    "filename": "cohorts/2025/02-workflow-orchestration/homework.md"
  },
  {
    "content": "## Question 1\n\n```\nWithin the execution for Yellow Taxi data for the year 2020 and month 12: what is the uncompressed file size (i.e. the output file yellow_tripdata_2020-12.csv of the extract task)?\n```\n\nTo get this answer, you need to go to the Outputs tab in Kestra and select the file. The size will be next to the preview and download button.\nAnswer: `128.3 MB`\n\n## Question 2\n\n```\nWhat is the rendered value of the variable file when the inputs taxi is set to green, year is set to 2020, and month is set to 04 during execution?\n```\n\nTo get this answer, you can run the expression in [Debug Outputs](https://youtu.be/SPGmXSJN3VE) to see it rendered.\n\nAnswer: `green_tripdata_2020-04.csv`\n\n## Question 3\n\n```\nHow many rows are there for the Yellow Taxi data for all CSV files in the year 2020?\n```\n\nAnswer: `24,648,499`\n\n## Question 4\n\n```\nHow many rows are there for the Green Taxi data for all CSV files in the year 2020?\n```\n\nAnswer: `1,734,051`\n\n## Question 5\n\n```\nHow many rows are there for the Yellow Taxi data for the March 2021 CSV file?\n```\n\nAnswer: `1,925,152`\n\n## Question 6\n\n```\nHow would you configure the timezone to New York in a Schedule trigger?\n```\n\nAnswer: `Add a timezone property set to America/New_York in the Schedule trigger configuration`",
    "filename": "cohorts/2025/02-workflow-orchestration/solution.md"
  },
  {
    "code": false,
    "content": "# Data Ingestion with dlt and Google Cloud Storage\n\nThis documentation provides a step-by-step guide on how to set up and execute data ingestion pipelines using the `dlt` library. We'll cover initializing the Google Cloud credentials, setting up the necessary packages, and processing Parquet files to either local storage or a database.\n\n## Setting Up Credentials\n\nBefore starting with the ingestion process, it's essential to set up the Google Cloud credentials. The credentials JSON file should be stored as a secret under the key `GCP_CREDENTIALS`. The following code snippet retrieves these credentials and sets up the environment variables required for your pipeline.\n\n```python\nimport os\nfrom google.colab import userdata\n\nos.environ[\"DESTINATION__CREDENTIALS\"] = userdata.get('GCP_CREDENTIALS')\nos.environ[\"BUCKET_URL\"] = \"gs://your_bucket_url\"\n```\n\n### Explanation\nThis code imports the necessary libraries and sets environment variables needed by the `dlt` library for authenticating and accessing Google Cloud Storage (GCS). Replace `\"gs://your_bucket_url\"` with your actual bucket URL for it to work properly.\n\n## Installing Packages\n\nTo ensure that you have the necessary libraries installed for both production and testing environments, use the following installation commands.\n\n### Production Installation\n\n```python\n# Install for production\n%%capture\n!pip install dlt[bigquery, gs]\n```\n\n### Testing Installation\n\n```python\n# Install for testing\n%%capture\n!pip install dlt[duckdb]\n```\n\n### Explanation\nThe first command installs the dependencies required for cloud integration with BigQuery and Google Cloud Storage, while the second command installs DuckDB, a lightweight database that is often used for local testing. The `%%capture` command captures the output of the installation process and prevents it from cluttering your notebook.\n\n## Importing Necessary Libraries\n\nOnce the packages are installed, import the libraries required for data ingestion and processing.\n\n```python\nimport dlt\nimport requests\nimport pandas as pd\nfrom dlt.destinations import filesystem\nfrom io import BytesIO\n```\n\n### Explanation\nThis code imports essential libraries including `dlt` for data pipeline creation, `requests` for HTTP requests, and `pandas` for data manipulation. The `BytesIO` class is used to handle byte data from HTTP responses.\n\n## Ingesting Parquet Files to Google Cloud Storage\n\nThe following code defines a `dlt` source to download Parquet files from a URL, process them, and prepare for ingestion into Google Cloud Storage.\n\n```python\n# Define a dlt source to download and process Parquet files as resources\n@dlt.source(name=\"rides\")\ndef download_parquet():\n     for month in range(1, 7):\n         file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n         url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-0{month}.parquet\"\n         response = requests.get(url)\n         df = pd.read_parquet(BytesIO(response.content))\n         # Return the dataframe as a dlt resource for ingestion\n         yield dlt.resource(df, name=file_name)\n\n# Initialize the pipeline\npipeline = dlt.pipeline(\n    pipeline_name=\"rides_pipeline\",\n    destination=filesystem(\n        layout=\"{schema_name}/{table_name}.{ext}\"\n    ),\n    dataset_name=\"rides_dataset\"\n)\n\n# Run the pipeline to load Parquet data into DuckDB\nload_info = pipeline.run(\n    download_parquet(),\n    loader_file_format=\"parquet\"\n)\n\n# Print the results\nprint(load_info)\n```\n\n### Explanation\nThis block defines a `dlt` source, which downloads and processes Parquet files using a loop. Each month's file is downloaded, transformed into a DataFrame, and yielded back for ingestion. The pipeline is then initialized to store the ingested data in a specified directory structure within a local filesystem. Finally, the `run()` method executes the pipeline, and the load information is printed.\n\n## Ingesting Data to a Database\n\nIf you want to load data directly into a database instead of local storage, you can modify the resource definition and utilization.\n\n```python\n# Define a dlt resource to download and process Parquet files as a single table\n@dlt.resource(name=\"rides\", write_disposition=\"replace\")\ndef download_parquet():\n     for month in range(1, 7):\n         url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-0{month}.parquet\"\n         response = requests.get(url)\n         df = pd.read_parquet(BytesIO(response.content))\n         # Return the dataframe as a dlt resource for ingestion\n         yield df\n\n# Initialize the pipeline\npipeline = dlt.pipeline(\n    pipeline_name=\"rides_pipeline\",\n    destination=\"duckdb\",  # Use DuckDB for testing\n    # destination=\"bigquery\",  # Use BigQuery for production\n    dataset_name=\"rides_dataset\"\n)\n\n# Run the pipeline to load Parquet data into DuckDB\ninfo = pipeline.run(download_parquet)\n\n# Print the results\nprint(info)\n```\n\n### Explanation\nHere, we define another `dlt` resource, which also processes Parquet files but is designed to yield a single DataFrame instead of multiple resources. The pipeline is initialized with the destination set to DuckDB for testing. You can comment in the BigQuery line to switch to production mode. The run command, followed by printing the information, confirms the ingestion process.\n\n## Analyzing Data with DuckDB\n\nOnce the data has been loaded into DuckDB, you might want to inspect the dataset and perform basic queries.\n\n```python\nimport duckdb\nconn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n\n# Set search path to the dataset\nconn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n\n# Describe the dataset to see loaded tables\nres = conn.sql(\"DESCRIBE\").df()\nprint(res)\n```\n\n### Explanation\nThis section establishes a connection to the DuckDB instance created during the pipeline process. It sets the search path to the dataset name you've provided and executes a SQL `DESCRIBE` command to list the tables loaded into the database. Finally, it prints out the result set, which gives you a clear understanding of the internal structure of your data.\n\n## Querying the Data\n\nTo extract specific metrics or insights from the dataset, you can perform SQL queries within your DuckDB connection.\n\n```python\n# Provide a resource name to query a table of that name\nwith pipeline.sql_client() as client:\n    with client.execute_query(f\"SELECT count(1) FROM rides\") as cursor:\n        data = cursor.df()\nprint(data)\n```\n\n### Explanation\nIn this final block, a context manager is used to interact with the SQL client associated with the pipeline. A SQL command is executed to count the number of records in the `rides` table, and this count is printed out. This is a simple yet effective way to validate the success of your data ingestion pipeline.",
    "filename": "cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb"
  },
  {
    "content": "## Module 3 Homework\n\nATTENTION: At the end of the submission form, you will be required to include a link to your GitHub repository or other public code-hosting site. \nThis repository should contain your code for solving the homework. If your solution includes code that is not in file format (such as SQL queries or \nshell commands), please include these directly in the README file of your repository.\n\n<b><u>Important Note:</b></u> <p> For this homework we will be using the Yellow Taxi Trip Records for **January 2024 - June 2024 NOT the entire year of data** \nParquet Files from the New York\nCity Taxi Data found here: </br> https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page </br>\nIf you are using orchestration such as Kestra, Mage, Airflow or Prefect etc. do not load the data into Big Query using the orchestrator.</br> \nStop with loading the files into a bucket. </br></br>\n\n**Load Script:** You can manually download the parquet files and upload them to your GCS Bucket or you can use the linked script [here](./load_yellow_taxi_data.py):<br>\nYou will simply need to generate a Service Account with GCS Admin Priveleges or be authenticated with the Google SDK and update the bucket name in the script to the name of your bucket<br>\nNothing is fool proof so make sure that all 6 files show in your GCS Bucket before begining.</br><br>\n\n<u>NOTE:</u> You will need to use the PARQUET option files when creating an External Table</br>\n\n<b>BIG QUERY SETUP:</b></br>\nCreate an external table using the Yellow Taxi Trip Records. </br>\nCreate a (regular/materialized) table in BQ using the Yellow Taxi Trip Records (do not partition or cluster this table). </br>\n</p>\n\n## Question 1:\nQuestion 1: What is count of records for the 2024 Yellow Taxi Data?\n- 65,623\n- 840,402\n- 20,332,093\n- 85,431,289\n\n\n## Question 2:\nWrite a query to count the distinct number of PULocationIDs for the entire dataset on both the tables.</br> \nWhat is the **estimated amount** of data that will be read when this query is executed on the External Table and the Table?\n\n- 18.82 MB for the External Table and 47.60 MB for the Materialized Table\n- 0 MB for the External Table and 155.12 MB for the Materialized Table\n- 2.14 GB for the External Table and 0MB for the Materialized Table\n- 0 MB for the External Table and 0MB for the Materialized Table\n\n## Question 3:\nWrite a query to retrieve the PULocationID from the table (not the external table) in BigQuery. Now write a query to retrieve the PULocationID and DOLocationID on the same table. Why are the estimated number of Bytes different?\n- BigQuery is a columnar database, and it only scans the specific columns requested in the query. Querying two columns (PULocationID, DOLocationID) requires \nreading more data than querying one column (PULocationID), leading to a higher estimated number of bytes processed.\n- BigQuery duplicates data across multiple storage partitions, so selecting two columns instead of one requires scanning the table twice, \ndoubling the estimated bytes processed.\n- BigQuery automatically caches the first queried column, so adding a second column increases processing time but does not affect the estimated bytes scanned.\n- When selecting multiple columns, BigQuery performs an implicit join operation between them, increasing the estimated bytes processed\n\n## Question 4:\nHow many records have a fare_amount of 0?\n- 128,210\n- 546,578\n- 20,188,016\n- 8,333\n\n## Question 5:\nWhat is the best strategy to make an optimized table in Big Query if your query will always filter based on tpep_dropoff_datetime and order the results by VendorID (Create a new table with this strategy)\n- Partition by tpep_dropoff_datetime and Cluster on VendorID\n- Cluster on by tpep_dropoff_datetime and Cluster on VendorID\n- Cluster on tpep_dropoff_datetime Partition by VendorID\n- Partition by tpep_dropoff_datetime and Partition by VendorID\n\n\n## Question 6:\nWrite a query to retrieve the distinct VendorIDs between tpep_dropoff_datetime\n2024-03-01 and 2024-03-15 (inclusive)</br>\n\nUse the materialized table you created earlier in your from clause and note the estimated bytes. Now change the table in the from clause to the partitioned table you created for question 5 and note the estimated bytes processed. What are these values? </br>\n\nChoose the answer which most closely matches.</br> \n\n- 12.47 MB for non-partitioned table and 326.42 MB for the partitioned table\n- 310.24 MB for non-partitioned table and 26.84 MB for the partitioned table\n- 5.87 MB for non-partitioned table and 0 MB for the partitioned table\n- 310.31 MB for non-partitioned table and 285.64 MB for the partitioned table\n\n\n## Question 7: \nWhere is the data stored in the External Table you created?\n\n- Big Query\n- Container Registry\n- GCP Bucket\n- Big Table\n\n## Question 8:\nIt is best practice in Big Query to always cluster your data:\n- True\n- False\n\n\n## (Bonus: Not worth points) Question 9:\nNo Points: Write a `SELECT count(*)` query FROM the materialized table you created. How many bytes does it estimate will be read? Why?\n\n\n## Submitting the solutions\n\nForm for submitting: https://courses.datatalks.club/de-zoomcamp-2025/homework/hw3\n\n## Solution\n\nSolution: https://www.youtube.com/watch?v=wpLmImIUlPg",
    "filename": "cohorts/2025/03-data-warehouse/homework.md"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThis script is designed to automate the process of downloading yellow taxi trip data files for the year 2024 from a specified URL, and then uploading those files to a Google Cloud Storage (GCS) bucket. By leveraging concurrent execution, it optimizes both downloading and uploading processes, allowing for efficient handling of multiple files simultaneously.\n\n## Imports\nThe script utilizes several libraries:\n- `os` and `sys`: For handling file paths and system-related tasks.\n- `urllib.request`: To perform the downloading of files via HTTP requests.\n- `concurrent.futures.ThreadPoolExecutor`: For concurrent execution of download and upload tasks.\n- `google.cloud.storage`: To interact with Google Cloud Storage.\n- `google.api_core.exceptions`: To handle exceptions related to GCS operations.\n- `time`: To incorporate delays during retries for uploads.\n\n## Configuration\nThe following configurations are set at the top of the script:\n- **BUCKET_NAME**: Defines the name of the GCS bucket where files will be uploaded.\n- **CREDENTIALS_FILE**: Sets the path to the JSON file containing service account credentials necessary for authentications.\n- **BASE_URL**: Specifies the base URL from which the trip data files will be downloaded.\n- **MONTHS**: A list comprehension that generates a list of months in the format \"MM\" from January (01) to June (06).\n- **DOWNLOAD_DIR**: Indicates the directory where the downloaded files will be stored.\n- **CHUNK_SIZE**: Sets the chunk size for file uploads to GCS.\n\n## Directory Preparation\nThe script attempts to create the specified download directory using `os.makedirs`, ensuring that it exists before any downloading takes place.\n\n```python\nos.makedirs(DOWNLOAD_DIR, exist_ok=True)\n```\n\nThis prevents errors if the directory does not exist and enables smooth file management.\n\n## Functions Defined\n\n### download_file(month)\nThis function handles the downloading of a single data file for a given month.\n- **Parameters**: Accepts a single month represented as a string.\n- **Operation**: Constructs the file URL and the local file path, attempts to download the file, and logs the outcome (success/failure).\n- **Returns**: The path of the downloaded file or `None` if the download fails.\n\n### create_bucket(bucket_name)\nThis function verifies the existence of a GCS bucket.\n- **Parameters**: Takes the name of the bucket to create or verify.\n- **Operation**: \n  - Attempts to access the specified bucket. \n  - If it exists and belongs to the current project, it continues. \n  - If it doesn't, it creates a new bucket or handles access-related errors.\n- **Outcome**: Provides feedback regarding the bucket's creation or the reasons for any challenges encountered.\n\n### verify_gcs_upload(blob_name)\nThis function checks whether a specific blob (file) exists in the GCS bucket.\n- **Parameters**: Takes the name of the blob as a string.\n- **Returns**: A boolean indicating whether the blob exists in GCS.\n\n### upload_to_gcs(file_path, max_retries=3)\nThis function uploads a file to GCS.\n- **Parameters**: \n  - `file_path`: The path of the file to upload.\n  - `max_retries`: Optional; specifies the maximum number of retries on upload failure.\n- **Operation**: \n  - Set up the corresponding blob in the GCS bucket.\n  - Initiates the upload process, verifying it afterward to ensure it succeeded.\n  - If unsuccessful, it retries the upload up to the specified maximum, with a delay.\n- **Reporting**: Logs success or failure messages throughout the upload process.\n\n## Main Execution Flow\nThe script's main execution block is initiated only when the script is run directly (not imported).\n1. **Bucket Creation**: The script verifies the specified bucket's existence through `create_bucket`.\n2. **Downloading Files**: The script uses `ThreadPoolExecutor` to manage concurrent downloads of all monthly files. The `download_file` function is mapped over the `MONTHS` list.\n3. **Uploading Files**: Another `ThreadPoolExecutor` is employed to upload the files concurrently, filtering out any `None` values that indicate download failures.\n\n## Conclusion\nAt the end of the execution, the script prints a concise message confirming that all files have been processed and their statuses have been verified. This script is a practical utility for efficiently managing data file transfers to cloud storage in a scalable way.",
    "filename": "cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py"
  },
  {
    "content": "## Module 4 Homework\n\nFor this homework, you will need the following datasets:\n* [Green Taxi dataset (2019 and 2020)](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/green)\n* [Yellow Taxi dataset (2019 and 2020)](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/yellow)\n* [For Hire Vehicle dataset (2019)](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/fhv)\n\n### Before you start\n\n1. Make sure you, **at least**, have them in GCS with a External Table **OR** a Native Table - use whichever method you prefer to accomplish that (Workflow Orchestration with [pandas-gbq](https://cloud.google.com/bigquery/docs/samples/bigquery-pandas-gbq-to-gbq-simple), [dlt for gcs](https://dlthub.com/docs/dlt-ecosystem/destinations/filesystem), [dlt for BigQuery](https://dlthub.com/docs/dlt-ecosystem/destinations/bigquery), [gsutil](https://cloud.google.com/storage/docs/gsutil), etc)\n2. You should have exactly `7,778,101` records in your Green Taxi table\n3. You should have exactly `109,047,518` records in your Yellow Taxi table\n4. You should have exactly `43,244,696` records in your FHV table\n5. Build the staging models for green/yellow as shown in [here](../../../04-analytics-engineering/taxi_rides_ny/models/staging/)\n6. Build the dimension/fact for taxi_trips joining with `dim_zones`  as shown in [here](../../../04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql)\n\n**Note**: If you don't have access to GCP, you can spin up a local Postgres instance and ingest the datasets above\n\n\n### Question 1: Understanding dbt model resolution\n\nProvided you've got the following sources.yaml\n```yaml\nversion: 2\n\nsources:\n  - name: raw_nyc_tripdata\n    database: \"{{ env_var('DBT_BIGQUERY_PROJECT', 'dtc_zoomcamp_2025') }}\"\n    schema:   \"{{ env_var('DBT_BIGQUERY_SOURCE_DATASET', 'raw_nyc_tripdata') }}\"\n    tables:\n      - name: ext_green_taxi\n      - name: ext_yellow_taxi\n```\n\nwith the following env variables setup where `dbt` runs:\n```shell\nexport DBT_BIGQUERY_PROJECT=myproject\nexport DBT_BIGQUERY_DATASET=my_nyc_tripdata\n```\n\nWhat does this .sql model compile to?\n```sql\nselect * \nfrom {{ source('raw_nyc_tripdata', 'ext_green_taxi' ) }}\n```\n\n- `select * from dtc_zoomcamp_2025.raw_nyc_tripdata.ext_green_taxi`\n- `select * from dtc_zoomcamp_2025.my_nyc_tripdata.ext_green_taxi`\n- `select * from myproject.raw_nyc_tripdata.ext_green_taxi`\n- `select * from myproject.my_nyc_tripdata.ext_green_taxi`\n- `select * from dtc_zoomcamp_2025.raw_nyc_tripdata.green_taxi`\n\n\n### Question 2: dbt Variables & Dynamic Models\n\nSay you have to modify the following dbt_model (`fct_recent_taxi_trips.sql`) to enable Analytics Engineers to dynamically control the date range. \n\n- In development, you want to process only **the last 7 days of trips**\n- In production, you need to process **the last 30 days** for analytics\n\n```sql\nselect *\nfrom {{ ref('fact_taxi_trips') }}\nwhere pickup_datetime >= CURRENT_DATE - INTERVAL '30' DAY\n```\n\nWhat would you change to accomplish that in a such way that command line arguments takes precedence over ENV_VARs, which takes precedence over DEFAULT value?\n\n- Add `ORDER BY pickup_datetime DESC` and `LIMIT {{ var(\"days_back\", 30) }}`\n- Update the WHERE clause to `pickup_datetime >= CURRENT_DATE - INTERVAL '{{ var(\"days_back\", 30) }}' DAY`\n- Update the WHERE clause to `pickup_datetime >= CURRENT_DATE - INTERVAL '{{ env_var(\"DAYS_BACK\", \"30\") }}' DAY`\n- Update the WHERE clause to `pickup_datetime >= CURRENT_DATE - INTERVAL '{{ var(\"days_back\", env_var(\"DAYS_BACK\", \"30\")) }}' DAY`\n- Update the WHERE clause to `pickup_datetime >= CURRENT_DATE - INTERVAL '{{ env_var(\"DAYS_BACK\", var(\"days_back\", \"30\")) }}' DAY`\n\n\n### Question 3: dbt Data Lineage and Execution\n\nConsidering the data lineage below **and** that taxi_zone_lookup is the **only** materialization build (from a .csv seed file):\n\n![image](./homework_q2.png)\n\nSelect the option that does **NOT** apply for materializing `fct_taxi_monthly_zone_revenue`:\n\n- `dbt run`\n- `dbt run --select +models/core/dim_taxi_trips.sql+ --target prod`\n- `dbt run --select +models/core/fct_taxi_monthly_zone_revenue.sql`\n- `dbt run --select +models/core/`\n- `dbt run --select models/staging/+`\n\n\n### Question 4: dbt Macros and Jinja\n\nConsider you're dealing with sensitive data (e.g.: [PII](https://en.wikipedia.org/wiki/Personal_data)), that is **only available to your team and very selected few individuals**, in the `raw layer` of your DWH (e.g: a specific BigQuery dataset or PostgreSQL schema), \n\n - Among other things, you decide to obfuscate/masquerade that data through your staging models, and make it available in a different schema (a `staging layer`) for other Data/Analytics Engineers to explore\n\n- And **optionally**, yet  another layer (`service layer`), where you'll build your dimension (`dim_`) and fact (`fct_`) tables (assuming the [Star Schema dimensional modeling](https://www.databricks.com/glossary/star-schema)) for Dashboarding and for Tech Product Owners/Managers\n\nYou decide to make a macro to wrap a logic around it:\n\n```sql\n{% macro resolve_schema_for(model_type) -%}\n\n    {%- set target_env_var = 'DBT_BIGQUERY_TARGET_DATASET'  -%}\n    {%- set stging_env_var = 'DBT_BIGQUERY_STAGING_DATASET' -%}\n\n    {%- if model_type == 'core' -%} {{- env_var(target_env_var) -}}\n    {%- else -%}                    {{- env_var(stging_env_var, env_var(target_env_var)) -}}\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\nAnd use on your staging, dim_ and fact_ models as:\n```sql\n{{ config(\n    schema=resolve_schema_for('core'), \n) }}\n```\n\nThat all being said, regarding macro above, **select all statements that are true to the models using it**:\n- Setting a value for  `DBT_BIGQUERY_TARGET_DATASET` env var is mandatory, or it'll fail to compile\n- Setting a value for `DBT_BIGQUERY_STAGING_DATASET` env var is mandatory, or it'll fail to compile\n- When using `core`, it materializes in the dataset defined in `DBT_BIGQUERY_TARGET_DATASET`\n- When using `stg`, it materializes in the dataset defined in `DBT_BIGQUERY_STAGING_DATASET`, or defaults to `DBT_BIGQUERY_TARGET_DATASET`\n- When using `staging`, it materializes in the dataset defined in `DBT_BIGQUERY_STAGING_DATASET`, or defaults to `DBT_BIGQUERY_TARGET_DATASET`\n\n\n## Serious SQL\n\nAlright, in module 1, you had a SQL refresher, so now let's build on top of that with some serious SQL.\n\nThese are not meant to be easy - but they'll boost your SQL and Analytics skills to the next level.  \nSo, without any further do, let's get started...\n\nYou might want to add some new dimensions `year` (e.g.: 2019, 2020), `quarter` (1, 2, 3, 4), `year_quarter` (e.g.: `2019/Q1`, `2019-Q2`), and `month` (e.g.: 1, 2, ..., 12), **extracted from pickup_datetime**, to your `fct_taxi_trips` OR `dim_taxi_trips.sql` models to facilitate filtering your queries\n\n\n### Question 5: Taxi Quarterly Revenue Growth\n\n1. Create a new model `fct_taxi_trips_quarterly_revenue.sql`\n2. Compute the Quarterly Revenues for each year for based on `total_amount`\n3. Compute the Quarterly YoY (Year-over-Year) revenue growth \n  * e.g.: In 2020/Q1, Green Taxi had -12.34% revenue growth compared to 2019/Q1\n  * e.g.: In 2020/Q4, Yellow Taxi had +34.56% revenue growth compared to 2019/Q4\n\n***Important Note: The Year-over-Year (YoY) growth percentages provided in the examples are purely illustrative. You will not be able to reproduce these exact values using the datasets provided for this homework.***\n\nConsidering the YoY Growth in 2020, which were the yearly quarters with the best (or less worse) and worst results for green, and yellow\n\n- green: {best: 2020/Q2, worst: 2020/Q1}, yellow: {best: 2020/Q2, worst: 2020/Q1}\n- green: {best: 2020/Q2, worst: 2020/Q1}, yellow: {best: 2020/Q3, worst: 2020/Q4}\n- green: {best: 2020/Q1, worst: 2020/Q2}, yellow: {best: 2020/Q2, worst: 2020/Q1}\n- green: {best: 2020/Q1, worst: 2020/Q2}, yellow: {best: 2020/Q1, worst: 2020/Q2}\n- green: {best: 2020/Q1, worst: 2020/Q2}, yellow: {best: 2020/Q3, worst: 2020/Q4}\n\n\n### Question 6: P97/P95/P90 Taxi Monthly Fare\n\n1. Create a new model `fct_taxi_trips_monthly_fare_p95.sql`\n2. Filter out invalid entries (`fare_amount > 0`, `trip_distance > 0`, and `payment_type_description in ('Cash', 'Credit card')`)\n3. Compute the **continous percentile** of `fare_amount` partitioning by service_type, year and and month\n\nNow, what are the values of `p97`, `p95`, `p90` for Green Taxi and Yellow Taxi, in April 2020?\n\n- green: {p97: 55.0, p95: 45.0, p90: 26.5}, yellow: {p97: 52.0, p95: 37.0, p90: 25.5}\n- green: {p97: 55.0, p95: 45.0, p90: 26.5}, yellow: {p97: 31.5, p95: 25.5, p90: 19.0}\n- green: {p97: 40.0, p95: 33.0, p90: 24.5}, yellow: {p97: 52.0, p95: 37.0, p90: 25.5}\n- green: {p97: 40.0, p95: 33.0, p90: 24.5}, yellow: {p97: 31.5, p95: 25.5, p90: 19.0}\n- green: {p97: 55.0, p95: 45.0, p90: 26.5}, yellow: {p97: 52.0, p95: 25.5, p90: 19.0}\n\n\n### Question 7: Top #Nth longest P90 travel time Location for FHV\n\nPrerequisites:\n* Create a staging model for FHV Data (2019), and **DO NOT** add a deduplication step, just filter out the entries where `where dispatching_base_num is not null`\n* Create a core model for FHV Data (`dim_fhv_trips.sql`) joining with `dim_zones`. Similar to what has been done [here](../../../04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql)\n* Add some new dimensions `year` (e.g.: 2019) and `month` (e.g.: 1, 2, ..., 12), based on `pickup_datetime`, to the core model to facilitate filtering for your queries\n\nNow...\n1. Create a new model `fct_fhv_monthly_zone_traveltime_p90.sql`\n2. For each record in `dim_fhv_trips.sql`, compute the [timestamp_diff](https://cloud.google.com/bigquery/docs/reference/standard-sql/timestamp_functions#timestamp_diff) in seconds between dropoff_datetime and pickup_datetime - we'll call it `trip_duration` for this exercise\n3. Compute the **continous** `p90` of `trip_duration` partitioning by year, month, pickup_location_id, and dropoff_location_id\n\nFor the Trips that **respectively** started from `Newark Airport`, `SoHo`, and `Yorkville East`, in November 2019, what are **dropoff_zones** with the 2nd longest p90 trip_duration ?\n\n- LaGuardia Airport, Chinatown, Garment District\n- LaGuardia Airport, Park Slope, Clinton East\n- LaGuardia Airport, Saint Albans, Howard Beach\n- LaGuardia Airport, Rosedale, Bath Beach\n- LaGuardia Airport, Yorkville East, Greenpoint\n\n\n## Submitting the solutions\n\n* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2025/homework/hw4\n\n\n## Solution \n\n* To be published after deadline",
    "filename": "cohorts/2025/04-analytics-engineering/homework.md"
  },
  {
    "content": "# Module 5 Homework\n\nIn this homework we'll put what we learned about Spark in practice.\n\nFor this homework we will be using the Yellow 2024-10 data from the official website: \n\n```bash\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n```\n\n\n## Question 1: Install Spark and PySpark\n\n- Install Spark\n- Run PySpark\n- Create a local spark session\n- Execute spark.version.\n\nWhat's the output?\n\n> [!NOTE]\n> To install PySpark follow this [guide](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/pyspark.md)\n\n\n## Question 2: Yellow October 2024\n\nRead the October 2024 Yellow into a Spark Dataframe.\n\nRepartition the Dataframe to 4 partitions and save it to parquet.\n\nWhat is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n\n- 6MB\n- 25MB\n- 75MB\n- 100MB\n\n\n## Question 3: Count records \n\nHow many taxi trips were there on the 15th of October?\n\nConsider only trips that started on the 15th of October.\n\n- 85,567\n- 105,567\n- 125,567\n- 145,567\n\n\n## Question 4: Longest trip\n\nWhat is the length of the longest trip in the dataset in hours?\n\n- 122\n- 142\n- 162\n- 182\n\n\n## Question 5: User Interface\n\nSpark\u2019s User Interface which shows the application's dashboard runs on which local port?\n\n- 80\n- 443\n- 4040\n- 8080\n\n\n\n## Question 6: Least frequent pickup location zone\n\nLoad the zone lookup data into a temp view in Spark:\n\n```bash\nwget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n```\n\nUsing the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?\n\n- Governor's Island/Ellis Island/Liberty Island\n- Arden Heights\n- Rikers Island\n- Jamaica Bay\n\n\n## Submitting the solutions\n\n- Form for submitting: https://courses.datatalks.club/de-zoomcamp-2025/homework/hw5\n- Deadline: See the website",
    "filename": "cohorts/2025/05-batch/homework.md"
  },
  {
    "code": false,
    "content": "# Homework Documentation on PySpark and Data Manipulation\n\nThis document outlines the process of analyzing the yellow taxi trip data using PySpark. The workflow involves reading data, data transformations, creating temporary views, executing SQL queries, and integrating with Pandas for further analysis.\n\n## Setting Up the Spark Environment\n\nFirst, we import the necessary libraries to work with PySpark, specifically the `SparkSession` for initializing the Spark infrastructure, along with types and functions modules for data operations.\n\n```python\nimport pyspark\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import types\n\nfrom pyspark.sql import functions as F\n```\n\n### Initializing Spark Session\n\nUsing the `SparkSession` builder, we initiate a Spark application. This application runs locally and is named 'homework'. After starting the session, we can check the Spark version to ensure that it is set up correctly.\n\n```python\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('homework') \\\n    .getOrCreate()\n\nspark.version\n```\n\n## Reading the Data\n\nNext, we read the yellow taxi trip data stored in Parquet format. The Parquet format is optimal for big data processing because of its efficient data storage and retrieval capabilities.\n\n```python\ndf = spark.read.parquet('yellow_tripdata_2024-10.parquet')\n```\n\n### Investigating the Data Schema\n\nTo understand the structure of our DataFrame, we examine its schema. This provides insight into the data types and column names, which is crucial for further analysis and transformations.\n\n```python\ndf.schema\n```\n\n## Exploring the Data\n\nWe take a glimpse at the first five rows of the DataFrame to understand its contents better. This step allows us to verify that the data is loaded as expected.\n\n```python\ndf.take(5)\n```\n\n### Data Repartitioning\n\nTo optimize the processing, we repartition the DataFrame into 4 partitions. This helps in balancing the data across the available resources and improves performance during computations. After repartitioning, we save the DataFrame back to disk in Parquet format.\n\n```python\ndf = df.repartition(4)\ndf.write.parquet('data-out')\n```\n\n## Creating Temporary Views\n\nWe create temporary views of both our main dataset and taxi zones dataset. Temporary views allow us to execute SQL queries directly within Spark, making data manipulation more flexible.\n\n```python\ndf.createOrReplaceTempView('hw5')\n```\n\n```python\n!ls -lh data-out\n```\n\n## Filtering and Counting Records\n\nWe filter the DataFrame for trips that occurred on a specific date, '2024-10-15', and count the number of records. This operation demonstrates how to use date functions in PySpark.\n\n```python\ndf.withColumn('pickup_date', F.to_date(df.tpep_pickup_datetime)) \\\n    .filter(\"pickup_date = '2024-10-15'\") \\\n    .count()\n```\n\n### SQL Query for Date Filtering\n\nWe perform the same filtering using SQL syntax. This SQL query counts all trips where the pickup date matches '2024-10-15', showcasing the capability of executing SQL commands on Spark DataFrames.\n\n```python\nspark.sql(\"\"\" \nSELECT count(*)\nFROM hw5\nWHERE cast(tpep_pickup_datetime as date) = '2024-10-15'\nand tpep_pickup_datetime is not null\n\"\"\").show()\n```\n\n## Calculating Trip Duration\n\nIn this section, we calculate the duration of taxi trips by subtracting the pickup time from the dropoff time. The results are ordered by duration, allowing us to identify the longest trips.\n\n```python\ndf.withColumn('duration', (df.tpep_dropoff_datetime.cast('long') - df.tpep_pickup_datetime.cast('long'))/60/60) \\\n    .orderBy('duration', ascending=False) \\\n    .limit(5) \\\n    .select(\"duration\") \\\n    .show()\n```\n\n### Defining Taxi Zone Schema\n\nBefore we load the taxi zone data, we define its schema to ensure the data types are correctly interpreted when reading from the CSV file. This structured approach helps avoid type-related errors during analysis.\n\n```python\ntaxi_zone_schema = types.StructType([\n    types.StructField('LocationID', types.IntegerType(), True),\n    types.StructField('Borough', types.StringType(), True),\n    types.StructField('Zone', types.StringType(), True),\n    types.StructField('service_zone', types.StringType(), True)\n])\n```\n\n## Reading Taxi Zone Data\n\nHere, we read the taxi zone lookup data from a CSV file with specified schema configurations. This allows us to integrate location data with our main analysis.\n\n```python\ndf_zones = spark.read \\\n    .option(\"header\", \"true\") \\\n    .schema(taxi_zone_schema) \\\n    .csv('taxi_zone_lookup.csv')\n```\n\n### Verifying Taxi Zone Data\n\nTo ensure the data has been correctly loaded, we display its contents. This is an important step in validating our data preparation process.\n\n```python\ndf_zones.show()\n```\n\n## Joining DataFrames and SQL Analysis\n\nWe create temporary views for both DataFrames, allowing us to execute SQL queries that perform joins. In this instance, we join the yellow taxi data with zone data to count trips per zone, providing insights into taxi usage in different areas.\n\n```python\ndf_zones.createOrReplaceTempView(\"zones\")\ndf.createOrReplaceTempView(\"yellow\")\n```\n\n```python\nspark.sql(\"\"\"\nSELECT\n    zones.Zone,\n    COUNT(1) as cnt\nFROM\n    yellow y LEFT JOIN zones ON \n        y.PULocationID = zones.LocationID\nGROUP BY 1\nORDER BY 2 ASC;\n\"\"\").take(5)\n```\n\n## Using Pandas for Additional Analysis\n\nLastly, we perform similar analysis using Pandas for additional validation and manipulation. We read the same Parquet file using Pandas and count the occurrences of trips on a specific date.\n\n```python\nimport pandas as pd\ndf = pd.read_parquet('yellow_tripdata_2024-10.parquet')\ndf['dt'] = df['tpep_pickup_datetime'].dt.date\n(df['dt']==pd.to_datetime('2024-10-15').date()).sum()\n```\n\nThis step demonstrates the cross-compatibility of PySpark and Pandas, enabling users to utilize the strengths of both libraries in data analysis tasks.",
    "filename": "cohorts/2025/05-batch/homework/solution.ipynb"
  },
  {
    "content": "# Homework\n\nIn this homework, we're going to learn about streaming with PyFlink.\n\nInstead of Kafka, we will use Red Panda, which is a drop-in\nreplacement for Kafka. It implements the same interface, \nso we can use the Kafka library for Python for communicating\nwith it, as well as use the Kafka connector in PyFlink.\n\nFor this homework we will be using the Taxi data:\n- Green 2019-10 data from [here](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz)\n\n\n## Setup\n\nWe need:\n\n- Red Panda\n- Flink Job Manager\n- Flink Task Manager\n- Postgres\n\nIt's the same setup as in the [pyflink module](../../../06-streaming/pyflink/), so go there and start docker-compose:\n\n```bash\ncd ../../../06-streaming/pyflink/\ndocker-compose up\n```\n\n(Add `-d` if you want to run in detached mode)\n\nVisit http://localhost:8081 to see the Flink Job Manager\n\nConnect to Postgres with pgcli, pg-admin, [DBeaver](https://dbeaver.io/) or any other tool.\n\nThe connection credentials are:\n\n- Username `postgres`\n- Password `postgres`\n- Database `postgres`\n- Host `localhost`\n- Port `5432`\n\nWith pgcli, you'll need to run this to connect:\n\n```bash\npgcli -h localhost -p 5432 -u postgres -d postgres\n```\n\nRun these query to create the Postgres landing zone for the first events and windows:\n\n```sql \nCREATE TABLE processed_events (\n    test_data INTEGER,\n    event_timestamp TIMESTAMP\n);\n\nCREATE TABLE processed_events_aggregated (\n    event_hour TIMESTAMP,\n    test_data INTEGER,\n    num_hits INTEGER \n);\n```\n\n## Question 1: Redpanda version\n\nNow let's find out the version of redpandas. \n\nFor that, check the output of the command `rpk help` _inside the container_. The name of the container is `redpanda-1`.\n\nFind out what you need to execute based on the `help` output.\n\nWhat's the version, based on the output of the command you executed? (copy the entire version)\n\n\n## Question 2. Creating a topic\n\nBefore we can send data to the redpanda server, we\nneed to create a topic. We do it also with the `rpk`\ncommand we used previously for figuring out the version of \nredpandas.\n\nRead the output of `help` and based on it, create a topic with name `green-trips` \n\nWhat's the output of the command for creating a topic? Include the entire output in your answer.\n\n\n## Question 3. Connecting to the Kafka server\n\nWe need to make sure we can connect to the server, so\nlater we can send some data to its topics\n\nFirst, let's install the kafka connector (up to you if you\nwant to have a separate virtual environment for that)\n\n```bash\npip install kafka-python\n```\n\nYou can start a jupyter notebook in your solution folder or\ncreate a script\n\nLet's try to connect to our server:\n\n```python\nimport json\n\nfrom kafka import KafkaProducer\n\ndef json_serializer(data):\n    return json.dumps(data).encode('utf-8')\n\nserver = 'localhost:9092'\n\nproducer = KafkaProducer(\n    bootstrap_servers=[server],\n    value_serializer=json_serializer\n)\n\nproducer.bootstrap_connected()\n```\n\nProvided that you can connect to the server, what's the output\nof the last command?\n\n## Question 4: Sending the Trip Data\n\nNow we need to send the data to the `green-trips` topic\n\nRead the data, and keep only these columns:\n\n* `'lpep_pickup_datetime',`\n* `'lpep_dropoff_datetime',`\n* `'PULocationID',`\n* `'DOLocationID',`\n* `'passenger_count',`\n* `'trip_distance',`\n* `'tip_amount'`\n\nNow send all the data using this code:\n\n```python\nproducer.send(topic_name, value=message)\n```\n\nFor each row (`message`) in the dataset. In this case, `message`\nis a dictionary.\n\nAfter sending all the messages, flush the data:\n\n```python\nproducer.flush()\n```\n\nUse `from time import time` to see the total time \n\n```python\nfrom time import time\n\nt0 = time()\n\n# ... your code\n\nt1 = time()\ntook = t1 - t0\n```\n\nHow much time did it take to send the entire dataset and flush? \n\n\n## Question 5: Build a Sessionization Window (2 points)\n\nNow we have the data in the Kafka stream. It's time to process it.\n\n* Copy `aggregation_job.py` and rename it to `session_job.py`\n* Have it read from `green-trips` fixing the schema\n* Use a [session window](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/) with a gap of 5 minutes\n* Use `lpep_dropoff_datetime` time as your watermark with a 5 second tolerance\n* Which pickup and drop off locations have the longest unbroken streak of taxi trips?\n\n\n## Submitting the solutions\n\n- Form for submitting: https://courses.datatalks.club/de-zoomcamp-2025/homework/hw6\n- Deadline: See the website",
    "filename": "cohorts/2025/06-streaming/homework.md"
  },
  {
    "code": false,
    "content": "# Kafka Producer for NYC Taxi Data\n\nThis documentation outlines the process of sending NYC taxi trip data to a Kafka topic using a Kafka producer. The steps include data acquisition, preprocessing, and publishing the data to Kafka.\n\n## Setting Up the Kafka Producer\n\nTo start, we need to import the necessary libraries and set up our Kafka producer. The `json` library is used for converting data into a JSON format, and `kafka` provides functionalities for Kafka producers.\n\n```python\nimport json\nfrom kafka import KafkaProducer\n\ndef json_serializer(data):\n    return json.dumps(data).encode('utf-8')\n\nserver = 'localhost:9092'\n\nproducer = KafkaProducer(\n    bootstrap_servers=[server],\n    value_serializer=json_serializer\n)\n\nproducer.bootstrap_connected()\n```\n\nIn this code block:\n- We define a `json_serializer` function that converts Python dictionaries into JSON formatted strings.\n- The server is specified as `localhost:9092`.\n- A Kafka producer is created with the server configuration and a JSON serializer to format the data for Kafka.\n\n## Downloading the NYC Taxi Data\n\nNext, we download the NYC taxi data from a GitHub repository. This dataset contains information about taxi trips in October 2019.\n\n```python\n!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz\n```\n\nThis command retrieves the compressed CSV file containing the taxi trip data. It\u2019s assumed that you are running in an environment where shell commands can be executed.\n\n## Importing and Loading the Data\n\nAfter downloading the dataset, we need to import the `pandas` library for data manipulation and then load the CSV file into a DataFrame.\n\n```python\nimport pandas as pd\n\ndf = pd.read_csv('green_tripdata_2019-10.csv.gz')\ndf.head()\n```\n\nHere:\n- `pd.read_csv` reads the compressed CSV file and loads it into a DataFrame (`df`).\n- The `head()` function displays the first few rows of the DataFrame, providing a quick overview of the data structure.\n\n## Selecting Relevant Columns\n\nTo focus on specific aspects of the dataset, we will filter the DataFrame to include only the columns that are relevant for our analysis.\n\n```python\ncolumns = [\n    'lpep_pickup_datetime',\n    'lpep_dropoff_datetime',\n    'PULocationID',\n    'DOLocationID',\n    'passenger_count',\n    'trip_distance',\n    'tip_amount'\n]\n\ndf = df[columns]\n```\n\nIn this block:\n- We define a list of the desired column names.\n- The DataFrame `df` is updated to include only these selected columns, refining the dataset for further processing.\n\n## Preparing Messages for Kafka\n\nBefore sending the data to Kafka, we need to convert the DataFrame into a format suitable for sending. This is done by transforming it into a list of dictionaries.\n\n```python\nfrom time import time\nfrom tqdm.auto import tqdm\n\nmessages = df.to_dict(orient='records')\nlen(messages)\n```\n\nThis section accomplishes the following:\n- We import `time` for potential timing functionalities and `tqdm` for progress visualization.\n- The DataFrame is converted to a list of dictionaries, where each dictionary represents a trip record.\n- The `len` function checks the number of messages to be sent, ensuring we know how many records are ready for processing.\n\n## Sending Messages to Kafka\n\nFinally, we send each message to the designated Kafka topic. The use of `tqdm` provides a visual progress bar indicating how many messages have been processed.\n\n```python\ntopic_name = 'green-trips'\n\nfor message in tqdm(messages):\n    producer.send(topic_name, value=message)\n\nproducer.flush()\n```\n\nIn this last block:\n- We set the topic name as 'green-trips'.\n- A loop iterates through the message list, sending each message to Kafka.\n- `producer.flush()` is called to ensure that all buffered records are transmitted before ending the session.\n\nThis documentation should guide you through using a Kafka producer to send NYC taxi trip data effectively. Each code block corresponds to essential steps required to set up the environment, process the data, and publish messages to a Kafka topic.",
    "filename": "cohorts/2025/06-streaming/homework/homework.ipynb"
  },
  {
    "content": "## Data Engineering Zoomcamp 2025 Cohort\n\n* [Pre-launch Q&A stream](https://www.youtube.com/watch?v=DPnAOu2csYA)\n* [Launch stream with course overview](https://www.youtube.com/watch?v=X8cEEwi8DTM)\n* [Course Google calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ)\n* [FAQ](https://datatalks.club/faq/data-engineering-zoomcamp.html)\n* [Course Playlist](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n* [Cohort-specific playlist: only 2025 Live videos](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJZdpLpRHp7dg6EOx828q6y)\n\n\n[**Module 1: Introduction & Prerequisites**](01-docker-terraform/)\n\n* [Homework](01-docker-terraform/homework.md)\n\n\n[**Module 2: Workflow Orchestration**](02-workflow-orchestration)\n\n* [Homework](02-workflow-orchestration/homework.md)\n* Office hours\n\n[**Workshop 1: Data Ingestion**](workshops/dlt/README.md)\n\n* Workshop with dlt\n* [Homework](workshops/dlt/README.md)\n\n\n[**Module 3: Data Warehouse**](03-data-warehouse)\n\n* [Homework](03-data-warehouse/homework.md)\n\n\n[**Module 4: Analytics Engineering**](04-analytics-engineering/)\n\n* [Homework](04-analytics-engineering/homework.md)\n\n\n[**Module 5: Batch processing**](05-batch/)\n\n* [Homework](05-batch/homework.md)\n\n\n[**Module 6: Stream Processing**](06-streaming)\n\n* [Homework](06-streaming/homework.md)\n\n\n[**Project**](project.md)\n\nMore information [here](project.md)",
    "filename": "cohorts/2025/README.md"
  },
  {
    "content": "## Course Project\n\nThe goal of this project is to apply everything we learned\nin this course and build an end-to-end data pipeline.\n\nYou will have two attempts to submit your project. If you don't have \ntime to submit your project by the end of attempt #1 (you started the \ncourse late, you have vacation plans, life/work got in the way, etc.)\nor you fail your first attempt, \nthen you will have a second chance to submit your project as attempt\n#2. \n\nThere are only two attempts.\n\nRemember that to pass the project, you must evaluate 3 peers. If you don't do that,\nyour project can't be considered complete.\n\nTo find the projects assigned to you, use the peer review assignments link \nand find your hash in the first column. You will see three rows: you need to evaluate \neach of these projects. For each project, you need to submit the form once,\nso in total, you will make three submissions. \n\n\n### Submitting\n\n#### Project Attempt #1\n\n* Project: https://courses.datatalks.club/de-zoomcamp-2025/project/project1\n* Review: https://courses.datatalks.club/de-zoomcamp-2025/project/project1/eval\n\n#### Project Attempt #2\n\n* Project: https://courses.datatalks.club/de-zoomcamp-2025/project/project2\n* Review: https://courses.datatalks.club/de-zoomcamp-2025/project/project2/eval\n\n> **Important**: update your \"Certificate name\" here: https://courses.datatalks.club/de-zoomcamp-2025/enrollment -\nthis is what we will use when generating certificates for you.\n\n### Evaluation criteria\n\nSee [here](../../projects/README.md)",
    "filename": "cohorts/2025/project.md"
  },
  {
    "content": "# Data ingestion with dlt\n\nHomework: [dlt_homework.md](dlt_homework.md)\n\n\ud83c\udfa5 **Watch the workshop video**\n\n[![Watch the workshop video](https://markdown-videos-api.jorgenkh.no/youtube/pgJWP_xqO1g)](https://www.youtube.com/watch?v=pgJWP_xqO1g \"Watch the workshop video\")\n\nWelcome to this hands-on workshop, where you'll learn to build efficient and scalable data ingestion pipelines.\n\n### **What will you learn in this workshop?**  \n\nIn this workshop, you\u2019ll learn the core skills required to build and manage data pipelines:  \n- **How to build robust, scalable, and self-maintaining pipelines**.  \n- **Best practices**, like built-in data governance, for ensuring clean and reliable data flows.  \n- **Incremental loading techniques** to refresh data quickly and cost-effectively.  \n- **How to build a Data Lake** with dlt.\n\nBy the end of this workshop, you'll be able to build data pipelines like a senior data engineer \u2014 quickly, concisely, and with best practices baked in.\n\n\n--- \n\n## \ud83d\udcc2 Navigation & Resources\n\n- Workshop:\n  - [Workshop content](data_ingestion_workshop.md).\n  - [Workshop Colab Notebook](https://colab.research.google.com/drive/1FiAHNFenM8RyptyTPtDTfqPCi5W6KX_V?usp=sharing).\n- Homework:\n  - [Homework Markdown](dlt_homework.md).\n  - [Homework Colab Notebook](https://colab.research.google.com/drive/1plqdl33K_HkVx0E0nGJrrkEUssStQsW7).\n- \ud83c\udf10 [Official dlt Documentation](https://dlthub.com/docs/intro).\n- \ud83d\udcac Join our [Slack Community](https://dlthub.com/community).\n\n---\n\n## \ud83d\udcd6 Course overview\nThis workshop is structured into three key parts:\n\n1\ufe0f\u20e3 **[Extracting Data](data_ingestion_workshop.md#extracting-data)** \u2013 Learn scalable data extraction techniques.  \n2\ufe0f\u20e3 **[Normalizing Data](data_ingestion_workshop.md#normalizing-data)** \u2013 Clean and structure data before loading.  \n3\ufe0f\u20e3 **[Loading & Incremental Updates](data_ingestion_workshop.md#loading-data)** \u2013 Efficiently load and update data.  \n\n\ud83d\udccc **Find the full course file here**: [Course File](data_ingestion_workshop.md)  \n\n---\n\n## \ud83d\udc69\u200d\ud83c\udfeb Teacher\n\nWelcome to the DataTalks.Club Data Engineering Zoomcamp the data ingestion workshop!\n\nI'm Violetta Mishechkina, Solutions Engineer at dltHub. \ud83d\udc4b\n- I\u2019ve been working in the data field since 2018, with a background in machine learning.\n- I started as a Data Scientist, training ML models and neural networks.\n- Over time, I realized that in production, hitting the highest RMSE isn\u2019t as important as model size, infrastructure, and data quality - so I transitioned into MLOps.\n- A year ago, I joined dltHub\u2019s Customer Success team and discovered dlt, a Python library that automates 90% of tedious data engineering tasks.\n- Now, I work closely with customers and partners to help them integrate and optimize dlt in production.\n- I also collaborate with our development team as the voice of the customer, ensuring our product meets real-world data engineering needs.\n- My experience across ML, MLOps, and data engineering gives me a practical, hands-on perspective on solving data challenges.\n\n---\n\n## Homework\n\n- [Homework Markdown](dlt_homework.md).\n- [Homework Colab Notebook](https://colab.research.google.com/drive/1plqdl33K_HkVx0E0nGJrrkEUssStQsW7).\n\n--- \n## Next steps\n\nAs you are learning the various concepts of data engineering, \nconsider creating a portfolio project that will further your own knowledge.\n\nBy demonstrating the ability to deliver end to end, you will have an easier time finding your first role. \nThis will help regardless of whether your hiring manager reviews your project, largely because you will have a better \nunderstanding and will be able to talk the talk.\n\nHere are some example projects that others did with dlt:\n- Serverless dlt-dbt on cloud functions: [Article](https://docs.getdbt.com/blog/serverless-dlt-dbt-stack)\n- Bird finder: [Part 1](https://publish.obsidian.md/lough-on-data/blogs/bird-finder-via-dlt-i), [Part 2](https://publish.obsidian.md/lough-on-data/blogs/bird-finder-via-dlt-ii)\n- Event ingestion on GCP: [Article and repo](https://dlthub.com/docs/blog/streaming-pubsub-json-gcp)\n- Event ingestion on AWS: [Article and repo](https://dlthub.com/docs/blog/dlt-aws-taktile-blog)\n- Or see one of the many demos created by our working students: [Hacker news](https://dlthub.com/docs/blog/hacker-news-gpt-4-dashboard-demo), \n[GA4 events](https://dlthub.com/docs/blog/ga4-internal-dashboard-demo), \n[an E-Commerce](https://dlthub.com/docs/blog/postgresql-bigquery-metabase-demo), \n[Google Sheets](https://dlthub.com/docs/blog/google-sheets-to-data-warehouse-pipeline), \n[Motherduck](https://dlthub.com/docs/blog/dlt-motherduck-demo), \n[MongoDB + Holistics](https://dlthub.com/docs/blog/MongoDB-dlt-Holistics), \n[Deepnote](https://dlthub.com/docs/blog/deepnote-women-wellness-violence-tends), \n[Prefect](https://dlthub.com/docs/blog/dlt-prefect),\n[PowerBI vs GoodData vs Metabase](https://dlthub.com/docs/blog/semantic-modeling-tools-comparison),\n[Dagster](https://dlthub.com/docs/blog/dlt-dagster),\n[Ingesting events via gcp webhooks](https://dlthub.com/docs/blog/dlt-webhooks-on-cloud-functions-for-event-capture),\n[SAP to snowflake replication](https://dlthub.com/docs/blog/sap-hana-to-snowflake-demo-blog),\n[Read emails and send sumamry to slack with AI and Kestra](https://dlthub.com/docs/blog/dlt-kestra-demo-blog),\n[Mode +dlt capabilities](https://dlthub.com/docs/blog/dlt-mode-blog),\n[dbt on cloud functions](https://dlthub.com/docs/blog/dlt-dbt-runner-on-cloud-functions)\n- If you want to use dlt in your project, [check this list of public APIs](https://dlthub.com/docs/blog/practice-api-sources)\n\n\nIf you create a personal project, consider submitting it to our blog - we will be happy to showcase it. Just drop us a line in the dlt Slack.\n\n\n\n## **\ud83d\udc9b If you enjoy dlt, support us!**  \n\n* \u2b50 **Give us a [GitHub Star](https://github.com/dlt-hub/dlt)!**  \n* \ud83d\udcac **Join our [Slack Community](https://dlthub.com/community)!**  \n* \ud83d\ude80 **Let\u2019s build great data pipelines together!**  \n\n---\n\n# Community notes\n\nDid you take notes? You can share them by creating a PR to this file!\n\n* [Ingest Data to GCS by dlt from peatwan](https://github.com/peatwan/de-zoomcamp/tree/main/workshop/dlt/homework/load_to_gcs)\n* Add your notes above this line",
    "filename": "cohorts/2025/workshops/dlt/README.md"
  },
  {
    "content": "# Data ingestion with dlt\n\n* Sign up: https://lu.ma/quyfn4q8 (optional) \n* Homework: [dlt_homework.md](dlt_homework.md)\n\n## **What is data ingestion?**  \nData ingestion is the process of **extracting** data from a source, transporting it to a suitable environment, and preparing it for use. This often includes **normalizing**, **cleaning**, and **adding metadata**.\n\n---\n\n### **\u201cA wild dataset Magically appears!\u201d**  \n\nIn many data science teams, data seems to appear out of nowhere \u2014 because an engineer loads it.  \n\nFor example, the well-known **NYC Taxi dataset** looks well-structured and ready to use, making it easy to query and analyze. However, not all datasets arrive in such a clean format.\n\n- **Well-structured data** (with an explicit schema) can be used immediately.  \n  - Examples: Parquet, Avro, or database tables where data types and structures are predefined.  \n- **Unstructured or weakly typed data** (without a defined schema) often needs cleaning and formatting first.  \n  - Examples: CSV, JSON, where fields might be inconsistent, nested or missing key details.  \n\n\ud83d\udca1 **What is a schema?**  \nA schema defines the expected format and structure of data, including field names, data types, and relationships.  \n\n---\n\n### **Be the Magician! \ud83d\ude0e**  \n\nSince you're here to learn data engineering, **you** will be the one making datasets magically appear!  \n\nTo build effective pipelines, you need to master:  \n\n\u2705 **Extracting** data from various sources (APIs, databases, files).  \n\u2705 **Normalization** data by transforming, cleaning, and defining schemas.  \n\u2705 **Loading** data where it can be used (data warehouse, lake, or database).\n\n---\n\n### **Why are data pipelines so amazing?**  \n\nData pipelines are the backbone of modern data-driven organizations, transforming raw, scattered data into actionable insights. \nThey ensure data flows seamlessly from its source to its final destination, where it can drive decision-making, analytics, and innovation. \nBut pipelines don\u2019t just move data \u2014 they enable an entire ecosystem of functionality that makes them indispensable.  \n\n![pipes](img/pipes.jpg)\n\n### **What makes data pipelines so essential?**  \n\n1. **Collect**:  \n   Data pipelines gather information from a variety of sources, such as databases, data streams, and applications. This ensures no data is overlooked.  \n   - Example: Retrieving sales data from an online store or capturing user activity logs from an app.  \n\n2. **Ingest**:  \n   The collected data flows into an event queue, where it\u2019s organized and prepared for the next steps.  \n   - **Structured data** (like Parquet files or database tables) can be processed immediately.  \n   - **Unstructured data** (like CSV or JSON files) often needs cleaning and normalization.  \n   - Example: Cleaning a JSON response by standardizing its fields or formatting dates in a CSV file.  \n\n3. **Store**:  \n   Pipelines send the processed data to **data lakes**, **data warehouses**, or **data lakehouses** for efficient storage and easy access.  \n   - Example: Storing marketing campaign data in a data warehouse to analyze its performance.  \n\n4. **Compute**:  \n   Data is processed either in **batches** (large chunks) or as **streams** (real-time updates) to make it ready for analysis.  \n   - Example: Calculating monthly revenue or processing live stock market data.  \n\n5. **Consume**:  \n   Finally, the prepared data is delivered to users in forms they can act on:  \n   - **Dashboards** for executives and analysts.  \n   - **Self-service analytics tools** for teams exploring trends.  \n   - **Machine learning models** for predictions and automation.  \n\n---\n\n### **Why are data engineers so important in this process?**  \n\nData engineers are the architects behind these pipelines. They don\u2019t just build pipelines\u2014they make sure they\u2019re reliable, efficient, and scalable. Beyond pipeline development, data engineers:  \n- **Optimize data storage** to keep costs low and performance high.  \n- **Ensure data quality and integrity**, addressing duplicates, inconsistencies, and missing values.  \n- **Implement governance** for secure, compliant, and well-managed data.  \n- **Adapt data architectures** to meet the changing needs of the organization.  \n\nUltimately, their role is to strategically manage the entire **data lifecycle**, from collection to consumption.\n\n---\n\n### **What will you learn in this workshop?**  \n\nIn this workshop, you\u2019ll learn the core skills required to build and manage data pipelines:  \n- **How to build robust, scalable, and self-maintaining pipelines**.  \n- **Best practices**, like built-in data governance, for ensuring clean and reliable data flows.  \n- **Incremental loading techniques** to refresh data quickly and cost-effectively.  \n- **How to build a Data Lake** with dlt.\n\nBy the end, you\u2019ll not only understand why data pipelines are amazing, but you\u2019ll also know how to create them with best practices to power your organization\u2019s data-driven success.\ud83d\ude80\n\n---\n## **Extracting data**\n\nMost of the data you\u2019ll work with is stored behind an **API**, which is like a doorway to the data. Here are the most common types:  \n\n- **RESTful APIs**: Provide records of data from business applications.  \n  - Example: Getting a list of customers from a CRM system.  \n- **File-based APIs**: Return secure file paths to bulk data like JSON or Parquet files stored in buckets.  \n  - Example: Downloading monthly sales reports.  \n- **Database APIs**: Connect to databases like MongoDB or SQL, often returning data as JSON, the most common interchange format.  \n\nAs an engineer, you will need to build pipelines that \u201cjust work\u201d.\n\nSo here\u2019s what you need to consider on extraction, to prevent the pipelines from breaking, and to keep them running smoothly:  \n\n1. **Hardware limits**: Be mindful of memory (RAM) and storage (disk space). Overloading these can crash your system.  \n2. **Network reliability**: Networks can fail! Always account for retries to make your pipelines more robust.  \n   - Tip: Use libraries like `dlt` that have built-in retry mechanisms.  \n3. **API rate limits**: APIs often restrict the number of requests you can make in a given time.  \n   - Tip: Check the API documentation to understand its limits (e.g., [Zendesk](https://developer.zendesk.com/api-reference/introduction/rate-limits/), [Shopify](https://shopify.dev/docs/api/usage/rate-limits)).  \n\nThere are even more challenges to consider when working with APIs \u2014 such as **pagination and authentication**. Let\u2019s explore how to handle these effectively when working with **REST APIs**.\n\n### **Working with REST APIs**\n\nREST APIs (Representational State Transfer APIs) are one of the most common ways to extract data. They allow you to retrieve structured data using simple HTTP requests. However, working with APIs comes with its own challenges.\n\n#### **Common Challenges**\n\n![rest_api](img/Rest_API.png)\n\n#### **1. Rate limits**  \nMany APIs **limit the number of requests** you can make within a certain time frame to prevent overloading their servers. If you exceed this limit, the API may **reject your requests** temporarily or even block you for a period.  \n\nTo avoid hitting these limits, we can:  \n- **Monitor API rate limits** \u2013 Some APIs provide headers that tell you how many requests you have left.  \n- **Pause requests when needed** \u2013 If we're close to the limit, we wait before making more requests.  \n- **Implement automatic retries** \u2013 If a request fails due to rate limiting, we can wait and retry after some time.  \n\n\ud83d\udca1Some APIs provide a **retry-after** header, which tells you how long to wait before making another request. Always check the API documentation for best practices!\n\n---\n\n#### **2. Authentication**  \nMany APIs require an **API key or token** to access data securely. Without authentication, requests may be limited or denied.  \n\n\ud83d\udd10 **Types of Authentication in APIs:**  \n- **API Keys** \u2013 A simple token included in the request header or URL.  \n- **OAuth Tokens** \u2013 A more secure authentication method requiring user authorization.  \n- **Basic Authentication** \u2013 Using a username and password (less common today).  \n\n\ud83d\udca1 Never share your API token publicly! Store it in environment variables or use a secure secrets manager.\n\n----\n#### **3. Pagination**\n\nMany APIs return data in **chunks (or pages)** rather than sending everything at once. This prevents **overloading the server** and improves performance, especially for large datasets. To retrieve **all the data**, we need to make multiple requests and keep track of pages until we reach the last one.\n\n\ud83d\udccc Example:\n\n>In this example, we\u2019ll request data from an API that serves the **NYC taxi dataset**.\n\nFor these purposes we created an API that can serve the data you are already familiar with. The API returns **1,000 records per page**, and we must request multiple pages to retrieve the full dataset.\n\n```py\nimport requests\n\nBASE_API_URL = \"https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api\"\n\npage_number = 1\nwhile True:\n    params = {'page': page_number}\n    response = requests.get(BASE_API_URL, params=params)\n    page_data = response.json()\n\n    if not page_data:\n        break\n\n    print(page_data)\n    page_number += 1\n\n    # limit the number of pages for testing\n    if page_number > 2:\n      break\n```\nWhat happens here:\n- Starts at page 1 and makes a GET request to the API.\n- Retrieves JSON data and checks if the page contains records.\n- If data exists, prints it and moves to the next page.\n- If the page is empty, stops requesting more data.\n\n\ud83d\udca1 Different APIs handle pagination differently (some use offsets, cursors, or tokens instead of page numbers). Always check the API documentation for the correct method!\n\n---\n\n#### **4. Avoiding memory issues during extraction**  \n\nTo prevent your pipeline from crashing, you need to control memory usage.  \n\n#### **Challenges with memory**  \n- Many pipelines run on systems with limited memory, like serverless functions or shared clusters.  \n- If you try to load all the data into memory at once, it can crash the entire system.  \n- Even disk space can become an issue if you\u2019re storing large amounts of data.  \n\n\n#### **The solution: streaming data**  \n\n**Streaming** means processing data in small chunks or events, rather than loading everything at once. This keeps memory usage low and ensures your pipeline remains efficient.\n\nAs a data engineer, you\u2019ll use streaming to transfer data between buffers, such as:  \n- from APIs to local files;  \n- from Webhooks to event queues;  \n- from Event queues (like Kafka) to storage buckets.\n\n---\n\n### **Example of extracting data: Grabbing data from an API**\n\nIn this example, we\u2019ll request data from an API that serves the **NYC taxi dataset**. For these purposes we created an API that can serve the data you are already familiar with.\n\n#### **API documentation**:  \n- **Data**: Comes in pages of 1,000 records.  \n- **Pagination**: When there\u2019s no more data, the API returns an empty page.  \n- **Details**:  \n  - **Method**: GET  \n  - **URL**: `https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api`  \n  - **Parameters**:  \n    - `page`: Integer (page number), defaults to 1.  \n\nHere\u2019s how we design our requester:  \n1. **Request page by page** until we hit an empty page. Since we don\u2019t know how much data is behind the API, we must assume it could be as little as 1,000 records or as much as 10GB.\n2. **Use a generator** to handle this efficiently and avoid loading all data into memory.  \n\n\n```py\nimport requests\n\nBASE_API_URL = \"https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api\"\n\ndef paginated_getter():\n    page_number = 1\n    while True:\n        params = {'page': page_number}\n        response = requests.get(BASE_API_URL, params=params)\n        response.raise_for_status()\n        page_json = response.json()\n        print(f'Got page {page_number} with {len(page_json)} records')\n\n        if page_json:\n            yield page_json\n            page_number += 1\n        else:\n            break\n\n\nfor page_data in paginated_getter():\n    print(page_data)\n```\n\nIn this approach to grabbing data from APIs, there are both pros and cons:  \n\n\u2705 Pros: **Easy memory management** since the API returns data in small pages or events.  \n\u274c Cons: **Low throughput** because data transfer is limited by API constraints (rate limits, response time).\n\n\nTo simplify data extraction, use specialized tools that follow best practices like streaming \u2014 for example, [dlt (data load tool)](https://dlthub.com). It efficiently processes data while **keeping memory usage low** and **leveraging parallelism** for better performance.\n\n### **Extracting data with dlt**\n\nExtracting data from APIs manually requires handling\n- **pagination**,\n- **rate limits**,\n- **authentication**,\n- **errors**.\n\nInstead of writing custom scripts, **[dlt](https://dlthub.com/)** simplifies the process with a built-in **[REST API Client](https://dlthub.com/docs/general-usage/http/rest-client)**, making extraction **efficient, scalable, and reliable**.  \n\n---\n\n### **Why use dlt for extraction?**  \n\n\u2705 **Built-in REST API support** \u2013 Extract data from APIs with minimal code.  \n\u2705 **Automatic pagination handling** \u2013 No need to loop through pages manually.  \n\u2705 **Manages Rate Limits & Retries** \u2013 Prevents exceeding API limits and handles failures.  \n\u2705 **Streaming support** \u2013 Extracts and processes data without loading everything into memory.  \n\u2705 **Seamless integration** \u2013 Works with **normalization and loading** in a single pipeline.  \n\n![dlt](img/dlt.png)\n\n### **Install dlt**\n\n[Install](https://dlthub.com/docs/reference/installation) dlt with DuckDB as destination:\n\n```shell\npip install dlt[duckdb]\n```\n\n### **Example of extracting data with dlt**  \n\nInstead of manually writing pagination logic, let\u2019s use **dlt\u2019s [`RESTClient` helper](https://dlthub.com/docs/general-usage/http/rest-client)** to extract NYC taxi ride data:  \n```py\nimport dlt\nfrom dlt.sources.helpers.rest_client import RESTClient\nfrom dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n\n\ndef paginated_getter():\n    client = RESTClient(\n        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n        # Define pagination strategy - page-based pagination\n        paginator=PageNumberPaginator(   # <--- Pages are numbered (1, 2, 3, ...)\n            base_page=1,   # <--- Start from page 1\n            total_path=None    # <--- No total count of pages provided by API, pagination should stop when a page contains no result items\n        )\n    )\n\n    for page in client.paginate(\"data_engineering_zoomcamp_api\"):    # <--- API endpoint for retrieving taxi ride data\n        yield page   # remember about memory management and yield data\n\nfor page_data in paginated_getter():\n    print(page_data)\n```\n\n**How dlt simplifies API extraction:**  \n\n\ud83d\udd39 **No manual pagination** \u2013 dlt **automatically** fetches **all pages** of data.  \n\ud83d\udd39 **Low memory usage** \u2013 Streams data **chunk by chunk**, avoiding RAM overflows.  \n\ud83d\udd39 **Handles rate limits & retries** \u2013 Ensures requests are sent efficiently **without failures**.  \n\ud83d\udd39 **Flexible destination support** \u2013 Load extracted data into **databases, warehouses, or data lakes**.\n\n---\n\nWell, you\u2019ve successfully **extracted** the data \u2014 great! \ud83c\udf89 But raw data isn\u2019t always ready to use. Now, you need to **process**, **clean**, and **structure** it before it can be loaded into a data lake or data warehouse.\n\n\n## **Normalizing data**\n\nYou often hear that data professionals spend most of their time **\u201ccleaning\u201d data** \u2014 but what does that actually mean?  \n\nData cleaning typically involves two key steps:  \n\n1. **Normalizing data** \u2013 Structuring and standardizing data **without changing its meaning**.  \n2. **Filtering data for a specific use case** \u2013 Selecting or modifying data **in a way that changes its meaning** to fit the analysis.\n\n### **Data cleaning: more than just fixing errors**  \n\nA big part of **data cleaning** is actually **metadata work** \u2014 ensuring data is structured and standardized so it can be used effectively.  \n\n#### **Metadata tasks in data cleaning:**  \n\n\u2705 **Add types** \u2013 Convert strings to numbers, timestamps, etc.  \n\u2705 **Rename columns** \u2013 Ensure names follow a standard format (e.g., no special characters).  \n\u2705 **Flatten nested dictionaries** \u2013 Bring values from nested dictionaries into the top-level row.  \n\u2705 **Unnest lists/arrays** \u2013 Convert lists into **child tables** since they can\u2019t be stored directly in a flat format.  \n\n\ud83d\udc49 **We\u2019ll look at a practical example next, as these concepts are easier to understand with real data.**\n\n---\n\n### **Why prepare data? Why not use JSON directly?**  \n\nWhile JSON is a great format for **data transfer**, it\u2019s not ideal for analysis. Here\u2019s why:  \n\n\u274c **No enforced schema** \u2013 We don\u2019t always know what fields exist in a JSON document.  \n\u274c **Inconsistent data types** \u2013 A field like `age` might appear as `25`, `\"twenty five\"`, or `25.00`, which can break downstream applications.  \n\u274c **Hard to process** \u2013 If we need to group data by day, we must manually convert date strings to timestamps.  \n\u274c **Memory-heavy** \u2013 JSON requires reading the entire file into memory, unlike databases or columnar formats that allow scanning just the necessary fields.  \n\u274c **Slow for aggregation and search** \u2013 JSON is not optimized for quick lookups or aggregations like columnar formats (e.g., Parquet).  \n\n\nJSON is great for **data exchange** but **not for direct analytical use**. To make data useful, we need to **normalize it** \u2014 flattening, typing, and structuring it for efficiency.\n\n---\n\n### **Normalization example**  \n\nTo understand what we\u2019re working with, let\u2019s look at a sample record from our API:\n\n```py\nitem = page_data[0]\nitem\n```\nOutput:\n```json\n{'End_Lat': 40.742963,\n 'End_Lon': -73.980072,\n 'Fare_Amt': 45.0,\n 'Passenger_Count': 1,\n 'Payment_Type': 'Credit',\n 'Rate_Code': None,\n 'Start_Lat': 40.641525,\n 'Start_Lon': -73.787442,\n 'Tip_Amt': 9.0,\n 'Tolls_Amt': 4.15,\n 'Total_Amt': 58.15,\n 'Trip_Distance': 17.52,\n 'Trip_Dropoff_DateTime': '2009-06-14 23:48:00',\n 'Trip_Pickup_DateTime': '2009-06-14 23:23:00',\n 'mta_tax': None,\n 'store_and_forward': None,\n 'surcharge': 0.0,\n 'vendor_name': 'VTS'}\n```\n\nThe data we retrieved from the API has **already been processed and unnested**, meaning that any **nested structures** (like dictionaries and lists) have been flattened, making it easier to store and query in a database or a dataframe. However, let\u2019s imagine we originally received the **raw data** in a more complex format.\n\n---\n\n### **How was this data processed?**  \n\nBefore reaching this format, the raw data likely contained **nested structures** that had to be **flattened and transformed**.  \n\n1\ufe0f\u20e3 **Flattened nested coordinates:**  \n   - Originally, the latitude and longitude values might have been nested like this:  \n     ```json\n     \"coordinates\": {\n         \"start\": {\"lat\": 40.641525, \"lon\": -73.787442},\n         \"end\": {\"lat\": 40.742963, \"lon\": -73.980072}\n     }\n     ```\n   - These were **flattened** into `Start_Lat`, `Start_Lon`, `End_Lat`, and `End_Lon`.  \n\n2\ufe0f\u20e3 **Converted timestamps:**  \n   - Originally, timestamps might have been stored as Unix timestamps or separate date/time fields:  \n     ```json\n     \"Trip_Pickup\": {\"date\": \"2009-06-14\", \"time\": \"23:23:00\"}\n     ```\n   - Now, they are **formatted as ISO datetime strings**:  \n     ```json\n     \"Trip_Pickup_DateTime\": \"2009-06-14 23:23:00\"\n     ```\n\n3\ufe0f\u20e3 **Unnested passenger & payment information:**  \n   - The original structure might have included a nested list for passengers:  \n     ```json\n     \"passengers\": [\n         {\"name\": \"John\", \"rating\": 4.9},\n         {\"name\": \"Jack\", \"rating\": 3.9}\n     ]\n     ```\n   - Since lists **cannot be stored directly in a database table**, they were likely **moved to a separate table**.\n\n\ud83d\udca1 **However, real-world data is rarely this clean!** We often receive raw, nested, and inconsistent data. This is why the **normalization process** is so important\u2014it **prepares** the data for efficient storage and analysis.  \n**[dlt (data load tool)](https://dlthub.com/docs/intro)** simplifies the **normalization process**, automatically transforming raw data into a **structured, clean format** that is ready for storage and analysis.\n\n---\n\n### **Normalizing data with dlt**  \n\n**Why use dlt for normalization?**  \n\n\u2705 **Automatically detects schema** \u2013 No need to define column types manually.  \n\u2705 **Flattens nested JSON** \u2013 Converts complex structures into table-ready formats.  \n\u2705 **Handles data type conversion** \u2013 Converts dates, numbers, and booleans correctly.  \n\u2705 **Splits lists into child tables** \u2013 Ensures relational integrity for better analysis.  \n\u2705 **Schema evolution support** \u2013 Adapts to changes in data structure over time.  \n\n---\n\n### **Example**  \n\nLet's assume we extracted the following raw NYC taxi ride data, which contains **nested dictionaries** and **lists**:\n\n```py\ndata = [\n    {\n        \"vendor_name\": \"VTS\",\n        \"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n        \"time\": {\n            \"pickup\": \"2009-06-14 23:23:00\",\n            \"dropoff\": \"2009-06-14 23:48:00\"\n        },\n        \"coordinates\": {\n            \"start\": {\"lon\": -73.787442, \"lat\": 40.641525},\n            \"end\": {\"lon\": -73.980072, \"lat\": 40.742963}\n        },\n        \"passengers\": [\n            {\"name\": \"John\", \"rating\": 4.9},\n            {\"name\": \"Jack\", \"rating\": 3.9}\n        ]\n    }\n]\n```\n\n### **How dlt normalizes this data automatically**  \n\nInstead of manually flattening fields and extracting nested lists, we can **load it directly into dlt**:\n\n```py\nimport dlt\n\n# Define a dlt pipeline with automatic normalization\npipeline = dlt.pipeline(\n    pipeline_name=\"ny_taxi_data\",\n    destination=\"duckdb\",\n    dataset_name=\"taxi_rides\",\n)\n\n# Run the pipeline with raw nested data\ninfo = pipeline.run(data, table_name=\"rides\", write_disposition=\"replace\")\n\n# Print the load summary\nprint(info)\n\nprint(pipeline.last_trace)\n```\n\n---\n\n### **What happens behind the scenes?**  \n\nAfter running this pipeline, dlt automatically **transforms the data** into the following **normalized structure**:  \n\n**Main table: `rides`**  \n\n```py\npipeline.dataset(dataset_type=\"default\").rides.df()\n```\n\n| vendor_name | record_hash                         | time__pickup              | time__dropoff             | coordinates__start__lon | coordinates__start__lat | coordinates__end__lon | coordinates__end__lat | _dlt_load_id      | _dlt_id        |\n|-------------|------------------------------------|---------------------------|---------------------------|-------------------------|-------------------------|-----------------------|-----------------------|-------------------|---------------|\n| VTS         | b00361a396177a9cb410ff61f20015ad  | 2009-06-14 23:23:00+00:00 | 2009-06-14 23:48:00+00:00 | -73.787442              | 40.641525               | -73.980072            | 40.742963            | 1738604244.2625916 | k+bnoLuti245ag |\n  \n\nThis table **displays structured taxi ride data**, including **vendor details, timestamps, coordinates, and dlt metadata**. \n\n**Child Table: `rides_passengers`** \n\n```py\npipeline.dataset(dataset_type=\"default\").rides__passengers.df()\n```\n\n| name  | rating | _dlt_parent_id    | _dlt_list_idx | _dlt_id        |\n|-------|--------|------------------|--------------|---------------|\n| John  | 4.9    | k+bnoLuti245ag    | 0            | 8ppDh+8gQ7SSHg |\n| Jack  | 3.9    | k+bnoLuti245ag    | 1            | oQnWuvkgHhxlaA |\n\n\n\u2705 **Nested structures were flattened** into separate columns.  \n\u2705 **Lists were extracted into child tables**, preserving relationships.  \n\u2705 **Timestamps were converted to the correct format.**  \n\n---\n\n### **Why dlt makes normalization easy**  \n\n\ud83d\udd39  **No manual transformations needed** \u2013 Just load the raw data, and dlt does the rest!  \n\ud83d\udd39 **Database-ready format** \u2013 Ensures clean, structured tables for easy querying.  \n\ud83d\udd39 **Handles schema evolution** \u2013 Adapts to new fields automatically.  \n\ud83d\udd39 **Scales effortlessly** \u2013 Works for small datasets and enterprise-scale pipelines.  \n\n\ud83d\udca1 With dlt, normalization happens automatically, so you can focus on insights instead of data wrangling.\n\n---\n\n## **Loading data**\n\nNow that we\u2019ve covered **extracting** and **normalizing** data, the final step is **loading** the data **into a destination**. This is where the processed data is stored, making it ready for querying, analysis, or further transformations.\n\n\n### **How data loading happens without dlt**  \n\nBefore dlt, data engineers had to manually handle **schema validation, batch processing, error handling, and retries** for every destination. This process becomes especially complex when loading data into **data warehouses and data lakes**, where performance optimization, partitioning, and incremental updates are critical.\n\n### **Example: Loading data into database without dlt**  \nA basic pipeline requires:  \n1. Setting up a database connection.  \n2. Creating tables and defining schemas.  \n3. Handling schema changes manually.  \n4. Writing queries to insert/update data.\n\n```py\nimport duckdb\n\n# 1. Create a connection to an in-memory DuckDB database\nconn = duckdb.connect(\"ny_taxi_manual.db\")\n\n# 2. Create the rides Table\n# Since our dataset has nested structures, we must manually flatten it before inserting data.\nconn.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS rides (\n    record_hash TEXT PRIMARY KEY,\n    vendor_name TEXT,\n    pickup_time TIMESTAMP,\n    dropoff_time TIMESTAMP,\n    start_lon DOUBLE,\n    start_lat DOUBLE,\n    end_lon DOUBLE,\n    end_lat DOUBLE\n);\n\"\"\")\n\n# 3. Insert Data Manually\n# Since JSON data has nested fields, we need to extract and transform them before inserting them into DuckDB.\ndata = [\n    {\n        \"vendor_name\": \"VTS\",\n        \"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n        \"time\": {\n            \"pickup\": \"2009-06-14 23:23:00\",\n            \"dropoff\": \"2009-06-14 23:48:00\"\n        },\n        \"coordinates\": {\n            \"start\": {\"lon\": -73.787442, \"lat\": 40.641525},\n            \"end\": {\"lon\": -73.980072, \"lat\": 40.742963}\n        }\n    }\n]\n\n# Prepare data for insertion\nflattened_data = [\n    (\n        ride[\"record_hash\"],\n        ride[\"vendor_name\"],\n        ride[\"time\"][\"pickup\"],\n        ride[\"time\"][\"dropoff\"],\n        ride[\"coordinates\"][\"start\"][\"lon\"],\n        ride[\"coordinates\"][\"start\"][\"lat\"],\n        ride[\"coordinates\"][\"end\"][\"lon\"],\n        ride[\"coordinates\"][\"end\"][\"lat\"]\n    )\n    for ride in data\n]\n\n# Insert into DuckDB\nconn.executemany(\"\"\"\nINSERT INTO rides (record_hash, vendor_name, pickup_time, dropoff_time, start_lon, start_lat, end_lon, end_lat)\nVALUES (?, ?, ?, ?, ?, ?, ?, ?)\n\"\"\", flattened_data)\n\nprint(\"Data successfully loaded into DuckDB!\")\n\n\n# 4. Query Data in DuckDB\n# Now that the data is loaded, we can query it using DuckDB\u2019s SQL engine.\ndf = conn.execute(\"SELECT * FROM rides\").df()\n\nconn.close()\n```\n\nProblems without dlt:\n\n\u274c **Schema management is manual** \u2013 If the schema changes, you need to update table structures manually.  \n\u274c **No automatic retries** \u2013 If the network fails, data may be lost.  \n\u274c **No incremental loading** \u2013 Every run reloads everything, making it slow and expensive.  \n\u274c **More code to maintain** \u2013 A simple pipeline quickly becomes complex.\n\n---\n\n### **How dlt handles the load step automatically**  \n\nWith dlt, loading data **requires just a few lines of code** \u2014 schema inference, error handling, and incremental updates are all handled automatically!\n\n### **Why use dlt for loading?**  \n\n\u2705 **Supports multiple destinations** \u2013 Load data into **BigQuery, Redshift, Snowflake, Postgres, DuckDB, Parquet (S3, GCS)** and more.  \n\u2705 **Optimized for performance** \u2013 Uses **batch loading, parallelism, and streaming** for fast and scalable data transfer.  \n\u2705 **Schema-aware** \u2013 Ensures that **column names, data types, and structures match** the destination\u2019s requirements.  \n\u2705 **Incremental loading** \u2013 Avoids unnecessary reloading by **only inserting new or updated records**.  \n\u2705 **Resilience & retries** \u2013 Automatically handles failures, ensuring data is loaded **without missing records**.\n\n![dlt](img/dlt.png)\n\n### **Example: Loading data into database with dlt**\n\n\n\nTo use all the power of dlt is better to wrap our API Client in the `@dlt.resource` decorator which denotes a logical grouping of data within a data source, typically holding data of similar structure and origin:\n\n```py\nimport dlt\nfrom dlt.sources.helpers.rest_client import RESTClient\nfrom dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n\n\n# Define the API resource for NYC taxi data\n@dlt.resource(name=\"rides\")   # <--- The name of the resource (will be used as the table name)\ndef ny_taxi():\n    client = RESTClient(\n        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n        paginator=PageNumberPaginator(\n            base_page=1,\n            total_path=None\n        )\n    )\n\n    for page in client.paginate(\"data_engineering_zoomcamp_api\"):    # <--- API endpoint for retrieving taxi ride data\n        yield page   # <--- yield data to manage memory\n\n\n# define new dlt pipeline\npipeline = dlt.pipeline(destination=\"duckdb\")\n\n# run the pipeline with the new resource\nload_info = pipeline.run(ny_taxi, write_disposition=\"replace\")\nprint(load_info)\n\n# explore loaded data\npipeline.dataset(dataset_type=\"default\").rides.df()\n```\n\n**Done!** The data is now stored in **DuckDB**, with schema managed automatically!\n\n---\n### **Incremental Loading**  \n\nIncremental loading allows us to update datasets by **loading only new or changed data**, instead of replacing the entire dataset. This makes pipelines **faster and more cost-effective** by reducing redundant data processing.  \n\n\n### **How does incremental loading work?**  \n\nIncremental loading works alongside two key concepts:  \n\n- **Incremental extraction** \u2013 Only extracts the new or modified data rather than retrieving everything again.  \n- **State tracking** \u2013 Keeps track of what has already been loaded, ensuring that only new data is processed.  \n\nIn dlt, **state** is stored in a **separate table** at the destination, allowing pipelines to track what has been processed.\n\n\ud83d\udd39 **Want to learn more?** You can read about incremental extraction and state management in the [dlt documentation](https://dlthub.com/docs).  \n\n---\n\n### **Incremental loading methods in dlt**  \n\ndlt provides two ways to load data incrementally:  \n\n#### **1. Append (adding new records)**  \n\n- Best for **immutable or stateless data**, such as taxi ride records.  \n- Each run **adds new records** without modifying previous data.  \n- Can also be used to create a **history of changes** (slowly changing dimensions).  \n\n**Example:**  \n- If taxi ride data is loaded daily, only **new rides** are added, rather than reloading the full history.  \n- If tracking changes in a list of vehicles, **each version** is stored as a new row for auditing.  \n\n---\n\n#### **2. Merge (updating existing records)**  \n\n- Best for **updating existing records** (stateful data).  \n- Replaces old records with updated ones based on a **unique key**.  \n- Useful for tracking **status changes**, such as payment updates.  \n\n**Example:**  \n- A taxi ride's **payment status** could change from `\"booked\"` to `\"cancelled\"`, requiring an update.  \n- A **customer profile** might be updated with a new email or phone number.  \n\n---\n\n### **Choosing between Append and Merge**  \n\n| **Scenario**                      | **Use Append** | **Use Merge** |\n|-----------------------------------|--------------|--------------|\n| Immutable records (e.g., ride history) | \u2705 Yes         | \u274c No        |\n| Tracking historical changes (slowly changing dimensions) | \u2705 Yes         | \u274c No        |\n| Updating existing records (e.g., payment status) | \u274c No         | \u2705 Yes        |\n| Keeping full change history       | \u2705 Yes         | \u274c No        |\n\n\n### **Example: Incremental loading with dlt**\n\n**The goal**: download only trips made after June 15, 2009, skipping the old ones.\n\nUsing `dlt`, we set up an [incremental filter](https://dlthub.com/docs/general-usage/incremental-loading%23incremental-loading-with-a-cursor-field) to only fetch trips made after a certain date:\n\n```python\ncursor_date = dlt.sources.incremental(\"Trip_Dropoff_DateTime\", initial_value=\"2009-06-15\")\n```\n\nThis tells `dlt`:\n- **Start date**: June 15, 2009 (`initial_value`).\n- **Field to track**: `Trip_Dropoff_DateTime` (our timestamp).\n\nAs you run the pipeline repeatedly, `dlt` will keep track of the latest `Trip_Dropoff_DateTime` value processed. It will skip records older than this date in future runs.\n\nLet's make the data resource incremental using `dlt.sources.incremental`:\n\n```py\nimport dlt\nfrom dlt.sources.helpers.rest_client import RESTClient\nfrom dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n\n\n@dlt.resource(name=\"rides\", write_disposition=\"append\")\ndef ny_taxi(\n    cursor_date=dlt.sources.incremental(\n        \"Trip_Dropoff_DateTime\",   # <--- field to track, our timestamp\n        initial_value=\"2009-06-15\",   # <--- start date June 15, 2009\n        )\n    ):\n    client = RESTClient(\n        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n        paginator=PageNumberPaginator(\n            base_page=1,\n            total_path=None\n        )\n    )\n\n    for page in client.paginate(\"data_engineering_zoomcamp_api\"):\n        yield page\n```\n\nFinally, we run our pipeline and load the fresh taxi rides data:\n\n```py\n# define new dlt pipeline\npipeline = dlt.pipeline(pipeline_name=\"ny_taxi\", destination=\"duckdb\", dataset_name=\"ny_taxi_data\")\n\n# run the pipeline with the new resource\nload_info = pipeline.run(ny_taxi)\nprint(pipeline.last_trace)\n```\n\n\nOnly 5325 rows were flitered out and loaded into the `duckdb` destination. Let's take a look at the earliest date in the loaded data:\n\n```py\nwith pipeline.sql_client() as client:\n    res = client.execute_sql(\n            \"\"\"\n            SELECT\n            MIN(trip_dropoff_date_time)\n            FROM rides;\n            \"\"\"\n        )\n    print(res)\n```\n\nRun the same pipeline again.\n\n```py\n# define new dlt pipeline\npipeline = dlt.pipeline(pipeline_name=\"ny_taxi\", destination=\"duckdb\", dataset_name=\"ny_taxi_data\")\n\n\n# run the pipeline with the new resource\nload_info = pipeline.run(ny_taxi)\nprint(pipeline.last_trace)\n```\n\nThe pipeline will detect that there are **no new records** based on the `Trip_Dropoff_DateTime` field and the incremental cursor. As a result, **no new data will be loaded** into the destination:\n>0 load package(s) were loaded\n\n\n\ud83d\udca1 **With dlt, incremental loading is simple, scalable, and automatic!**\n\n---\n\n### **Example: Loading data into a Data Warehouse (BigQuery)**  \nFirst, install the dependencies, define the source, then change the destination name and run the pipeline.\n\n```shell\npip install dlt[bigquery]\n```\n\nLet's use our NY Taxi API and load data from the source into destination.\n\n```py\nimport dlt\nfrom dlt.sources.helpers.rest_client import RESTClient\nfrom dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n\n\n@dlt.resource(name=\"rides\", write_disposition=\"replace\")\ndef ny_taxi():\n    client = RESTClient(\n        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n        paginator=PageNumberPaginator(\n            base_page=1,\n            total_path=None\n        )\n    )\n\n    for page in client.paginate(\"data_engineering_zoomcamp_api\"):\n        yield page\n```\n\n\n**Choosing a destination**\n\nSwitching between  **data warehouses (BigQuery, Snowflake, Redshift)** or **data lakes (S3, Google Cloud Storage, Parquet files)**  in dlt is incredibly straightforward \u2014 simply modify the `destination` parameter in your pipeline configuration. \n\nFor example:\n\n```py\npipeline = dlt.pipeline(\n    pipeline_name='taxi_data',\n    destination='duckdb', # <--- to test pipeline locally\n    dataset_name='taxi_rides',\n)\n\npipeline = dlt.pipeline(\n    pipeline_name='taxi_data',\n    destination='bigquery', # <--- to run pipeline in production\n    dataset_name='taxi_rides',\n)\n```\n\nThis flexibility allows you to easily transition from local development to production-grade environments.\n\n> \ud83d\udca1 No need to rewrite your pipeline \u2014 dlt adapts automatically!\n\n**Set Credentials**  \n\nThe next logical step is to [set credentials](https://dlthub.com/docs/general-usage/credentials/) using **dlt's TOML providers** or **environment variables (ENVs)**.\n\n```py\nimport os\nfrom google.colab import userdata\n\nos.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = userdata.get('BIGQUERY_CREDENTIALS')\n```\n\nRun the pipeline:\n```py\npipeline = dlt.pipeline(\n    pipeline_name=\"taxi_data\",\n    destination=\"bigquery\",\n    dataset_name=\"taxi_rides\",\n    dev_mode=True,\n)\n\ninfo = pipeline.run(ny_taxi)\nprint(info)\n```\n\n\ud83d\udca1 **What\u2019s different?**  \n- **dlt automatically adapts the schema** to fit BigQuery.  \n- **Partitioning & clustering** can be applied for performance optimization.  \n- **Efficient batch loading** ensures scalability.\n\n---\n\n### **Example: Loading data into a Data Lake (Parquet on Local FS or S3)**  \n\n**Why use a Data Lake?**  \n- **Cost-effective storage** \u2013 Cheaper than traditional databases.   \n- **Optimized for big data processing** \u2013 Works seamlessly with Spark, Databricks, and Presto.  \n- **Easy scalability** \u2013 Store petabytes of data efficiently.  \n\n\nThe `filesystem` destination enables you to load data into **files stored locally** or in **cloud storage** solutions, making it an excellent choice for lightweight testing, prototyping, or file-based workflows.\n\nBelow is an **example** demonstrating how to use the `filesystem` destination to load data in **Parquet** format:\n\n* Step 1: Set up a local bucket or cloud directory for storing files\n\n```py\nimport os\n\nos.environ[\"BUCKET_URL\"] = \"/content\"\n```\n\n* Step 2: Define the data source (above)\n* Step 3: Run the pipeline\n\n```py\nimport dlt\n\n\npipeline = dlt.pipeline(\n    pipeline_name='fs_pipeline',\n    destination='filesystem', # <--- change destination to 'filesystem'\n    dataset_name='fs_data',\n)\n\nload_info = pipeline.run(ny_taxi, loader_file_format=\"parquet\") # <--- choose a file format: parquet, csv or jsonl\nprint(load_info)\n```\n\nLook at the files:\n\n```shell\n! ls fs_data/rides\n```\n\nLook at the loaded data:\n\n```py\n# explore loaded data\npipeline.dataset(dataset_type=\"default\").rides.df()\n```\n\n#### **Table formats: [Delta tables & Iceberg](https://dlthub.com/docs/dlt-ecosystem/destinations/delta-iceberg)**\n\ndlt supports writing **Delta** and **Iceberg** tables when using the `filesystem` destination.\n\n**How it works:**\n\ndlt uses the `deltalake` and `pyiceberg` libraries to write Delta and Iceberg tables, respectively. One or multiple Parquet files are prepared during the extract and normalize steps. In the load step, these Parquet files are exposed as an Arrow data structure and fed into `deltalake` or `pyiceberg`.\n\n```shell\n !pip install \"dlt[pyiceberg]\"\n```\n\n```py\npipeline = dlt.pipeline(\n    pipeline_name='fs_pipeline',\n    destination='filesystem', # <--- change destination to 'filesystem'\n    dataset_name='fs_iceberg_data',\n)\n\nload_info = pipeline.run(\n    ny_taxi,\n    loader_file_format=\"parquet\",\n    table_format=\"iceberg\",  # <--- choose a table format: delta or iceberg\n)\nprint(load_info)\n```\n\n\ud83d\udca1**Note:**\n\nOpen source version of dlt supports basic functionality for **iceberg**, but the dltHub team is currently working on an **extended** and **more powerful** integration with iceberg.\n\n[Join the waiting list to learn more about dlt+ and Iceberg.](https://info.dlthub.com/waiting-list)\n\n\n---\n\n## **What\u2019s Next?**  \n\n- **Try loading data into different [destinations](https://dlthub.com/docs/dlt-ecosystem/destinations/)** \u2013 Test Postgres, Snowflake, or Parquet.  \n- **Experiment with [incremental loading](https://dlthub.com/docs/general-usage/incremental-loading)** \u2013 Load only new records for better efficiency.  \n- **Explore dlt\u2019s [schema evolution](https://dlthub.com/docs/general-usage/schema-evolution)** \u2013 Automatically adjust to data structure changes.  \n- **Join our [Slack community](https://dlthub.com/community)** to share your progress!  \n\n\nWith **dlt\u2019s automated load step**, you get **effortless, scalable, and resilient data loading**\u2014so you can focus on insights instead of pipeline maintenance. \ud83d\ude80\n\n---\n\n### Extra homework \ud83d\udcbb\n* [Data ingestion with DLT to Bigquery from Sara Sabater](https://github.com/saraisab/Data_Engineer/blob/main/courses/DE_zoomcamp/Homework/DLT-Workshop/extra_homework/Data_ingestion_with_DLT_to_bigquery.ipynb).",
    "filename": "cohorts/2025/workshops/dlt/data_ingestion_workshop.md"
  },
  {
    "content": "Original file is located at\n    https://colab.research.google.com/drive/1plqdl33K_HkVx0E0nGJrrkEUssStQsW7\n\n# **Workshop \"Data Ingestion with dlt\": Homework**\n\n---\n\n## **Dataset & API**\n\nWe\u2019ll use **NYC Taxi data** via the same custom API from the workshop:\n\n\ud83d\udd39 **Base API URL:**  \n```\nhttps://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api\n```\n\ud83d\udd39 **Data format:** Paginated JSON (1,000 records per page).  \n\ud83d\udd39 **API Pagination:** Stop when an empty page is returned.\n\n## **Question 1: dlt Version**\n\n1. **Install dlt**:\n\n```\n!pip install dlt[duckdb]\n```\n\n> Or choose a different bracket\u2014`bigquery`, `redshift`, etc.\u2014if you prefer another primary destination. For this assignment, we\u2019ll still do a quick test with DuckDB.\n\n2. **Check** the version:\n\n```\n!dlt --version\n```\n\nor:\n\n```py\nimport dlt\nprint(\"dlt version:\", dlt.__version__)\n```\n\nProvide the **version** you see in the output.\n\n## **Question 2: Define & Run the Pipeline (NYC Taxi API)**\n\nUse dlt to extract all pages of data from the API.\n\nSteps:\n\n1\ufe0f\u20e3 Use the `@dlt.resource` decorator to define the API source.\n\n2\ufe0f\u20e3 Implement automatic pagination using dlt's built-in REST client.\n\n3\ufe0f\u20e3 Load the extracted data into DuckDB for querying.\n\n```py\nimport dlt\nfrom dlt.sources.helpers.rest_client import RESTClient\nfrom dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n\n\n# your code is here\n\n\npipeline = dlt.pipeline(\n    pipeline_name=\"ny_taxi_pipeline\",\n    destination=\"duckdb\",\n    dataset_name=\"ny_taxi_data\"\n)\n```\n\nLoad the data into DuckDB to test:\n```py\nload_info = pipeline.run(ny_taxi)\nprint(load_info)\n```\nStart a connection to your database using native `duckdb` connection and look what tables were generated:\"\"\"\n\n```py\nimport duckdb\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\n# A database '<pipeline_name>.duckdb' was created in working directory so just connect to it\n\n# Connect to the DuckDB database\nconn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n\n# Set search path to the dataset\nconn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n\n# Describe the dataset\nconn.sql(\"DESCRIBE\").df()\n\n```\n\nHow many tables were created?\n\n* 2\n* 4\n* 6\n* 8\n\n## **Question 3: Explore the loaded data**\n\nInspect the table `ride`:\n\n```py\ndf = pipeline.dataset(dataset_type=\"default\").rides.df()\ndf\n```\n\nWhat is the total number of records extracted?\n\n* 2500\n* 5000\n* 7500\n* 10000\n\n## **Question 4: Trip Duration Analysis**\n\nRun the SQL query below to:\n\n* Calculate the average trip duration in minutes.\n\n```py\nwith pipeline.sql_client() as client:\n    res = client.execute_sql(\n            \"\"\"\n            SELECT\n            AVG(date_diff('minute', trip_pickup_date_time, trip_dropoff_date_time))\n            FROM rides;\n            \"\"\"\n        )\n    # Prints column values of the first row\n    print(res)\n```\n\nWhat is the average trip duration?\n\n* 12.3049\n* 22.3049\n* 32.3049\n* 42.3049\n\n## **Submitting the solutions**\n\n* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2025/homework/workshop1\n\n## **Solution**\n\nWe will publish the solution here after deadline.",
    "filename": "cohorts/2025/workshops/dlt/dlt_homework.md"
  },
  {
    "code": false,
    "content": "# Data Loading Script Documentation\n\nThis script facilitates the downloading and processing of trip data in Parquet format from a cloud source and loads it into BigQuery. It supports two methods for loading data \u2013 either by first downloading the files to Google Cloud Storage (GCS) or by streaming data directly from the web.\n\n## Overview\n\n1. **Configuration Loading**: The script begins by loading a configuration file in the TOML format, containing GCP credentials.\n2. **Environment Setup**: It sets the necessary environment variables needed for authentication with Google Cloud.\n3. **URL Generation Function**: It defines a function to generate the URLs for the requested trip data based on user inputs regarding color, year, and month.\n4. **User Input Section**: The script collects user input to define the parameters for data retrieval.\n5. **Data Fetching Method Selection**: Users are prompted to select one of two methods for loading data into BigQuery.\n6. **Data Download and Loading**: Depending on the chosen method, the script downloads the data and uploads it either to GCS or directly streams it to BigQuery.\n7. **Pipeline Execution**: Finally, it sets up and runs a data pipeline using the `dlt` library to load the data into BigQuery.\n\n## Configuration Loading\n\nThe script imports necessary libraries including `json`, `os`, `toml`, `requests`, and GCP client libraries. It begins by loading credentials from a TOML file, expected at the path `./.dlt/secrets.toml`. The credentials include a project ID, private key, and client email. \n\n```python\nconfig = toml.load(\"./.dlt/secrets.toml\")\n```\n\nThis information is essential for authenticating interactions with Google Cloud services.\n\n## Environment Setup\n\nUsing the loaded credentials, it sets environment variables. These variables are important for securing sensitive information used for validations when making requests to Google Cloud services.\n\n```python\nos.environ[\"CREDENTIALS__PROJECT_ID\"] = config[\"credentials\"][\"project_id\"]\nos.environ[\"CREDENTIALS__PRIVATE_KEY\"] = config[\"credentials\"][\"private_key\"]\nos.environ[\"CREDENTIALS__CLIENT_EMAIL\"] = config[\"credentials\"][\"client_email\"]\n```\n\n## URL Generation Function\n\nThe function `generate_urls` constructs a list of URLs based on user-specified parameters: trip color, start and end years, and start and end months. The URLs point to various Parquet files hosted on a cloud front service.\n\n```python\ndef generate_urls(color, start_year, end_year, start_month, end_month):\n    ...\n    return urls\n```\n\nThis function is crucial as it provides the specific files that the script will later process.\n\n## User Input Section\n\nNext, the script prompts the user to input:\n- The trip color (green or yellow)\n- A start and end year.\n- A start and end month.\n\n```python\ncolor = input(\"Enter color (green, yellow): \").lower()  \nstart_year = int(input(\"Enter the start year (e.g., 2019): \"))\n...\n```\n\nThis interactive section allows users to tailor the data download based on their needs.\n\n## Data Fetching Method Selection\n\nAfter generating the URLs, the script asks the user to choose one of two data loading methods:\n1. Load data through GCS.\n2. Directly load data from the web.\n\n```python\ndlt_method = input(\"Choose loading method: 1 for GCS -> Bigquery, 2 for Direct Web -> Bigquery: \")\n```\n\nThis flexibility enables different workflows depending on the user's requirements or environment settings.\n\n## Data Download and Loading\n\n### Method 1: GCS Upload\n\nIf the first method is selected, the script initializes a GCS client and downloads Parquet files from the URLs. It uploads these files to a specified GCS bucket.\n\n```python\nif dlt_method == \"1\":\n    ...\n    gcs_blob.upload_from_string(response.content)\n```\n\nIt then defines a function `parquet_source()` for the data ingestion pipeline that uses the `dlt` library to read files stored in GCS and yield individual rows.\n\n### Method 2: Direct Streaming\n\nFor the second method, the function `paginated_getter()` is defined to stream data from the URLs directly into the pipeline:\n\n```python\nelif dlt_method == \"2\":\n    ...\n```\n\nThis approach avoids intermediate storage in GCS, which can be more efficient depending on the use case.\n\n## Pipeline Execution\n\nFinally, the script creates and runs a `dlt` pipeline named \u201ctest_taxi.\u201d User input for the dataset name is also collected:\n\n```python\npipeline = dlt.pipeline(\n    pipeline_name=\"test_taxi\",\n    dataset_name=input(\"Enter the dataset name: \"),\n    destination=\"bigquery\"\n)\n```\n\nThe pipeline executes using either the `parquet_source()` or `paginated_getter()` function based on the user's previous choice, and it returns information on the process completion.\n\n```python\ninfo = pipeline.run(parquet_source())  # or paginated_getter()\nprint(info)\n```\n\nThis comprehensive implementation connects data from external sources to BigQuery effectively while providing users flexibility in their data processing workflow.",
    "filename": "cohorts/2025/workshops/dynamic_load_dlt.py"
  },
  {
    "content": "[Medium article](https://medium.com/@NYCTLC/what-makes-a-city-street-smart-23496d92f60d)\n\n[Trip record user guide](https://www1.nyc.gov/assets/tlc/downloads/pdf/trip_record_user_guide.pdf)\n\nThe data set is divided into 4 parts:\n\n- Yellow cabs\n- Green cabs\n- For Hire Vehicles\n- High volume for hire vehicles\n\n\n\nBelow I am only concentrating on Yellow and green cabs\n\n### Yellow and green cabs\n\n,\n\n| Columns               | Definition | Example             |\n| --------------------- | ---------- | ------------------- |\n| VendorID              |            | 2                   |\n| lpep_pickup_datetime  |            | 2021-01-01 00:15:56 |\n| lpep_dropoff_datetime |            | 2021-01-01 00:19:52 |\n| store_and_fwd_flag    |            | N,                  |\n| RatecodeID            |            | 1                   |\n| PULocationID          |            | 43                  |\n| DOLocationID          |            | 151                 |\n| passenger_count       |            | 1                   |\n| trip_distance         |            | 1.01                |\n| fare_amount           |            | 5.5                 |\n| extra                 |            | 0.5                 |\n| mta_tax               |            | 0.5                 |\n| tip_amount            |            | 0                   |\n| tolls_amount          |            | 0                   |\n| ehail_fee             |            |                     |\n| improvement_surcharge |            | 0.3                 |\n| total_amount          |            | 6.8                 |\n| payment_type          |            | 2                   |\n| trip_type             |            | 1                   |\n| congestion_surcharge  |            | 0                   |\n\n\n\n### Taxi zone Lookup\n\n| Columns      | Definition | Example        |\n| ------------ | ---------- | -------------- |\n| LocationID   |            | 1              |\n| Borough      |            | EWR            |\n| Zone         |            | Newark Airport |\n| service_zone |            | EWR            |\n\n[Shapefile from S3](https://s3.amazonaws.com/nyctlc/misc/taxi_zones.zip)\n\n[Taxi zones](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc)",
    "filename": "dataset.md"
  },
  {
    "content": "# Learning in public\n\nMost people learn in private: they consume content but don't tell\nanyone about it. There's nothing wrong with it.\n\nBut we want to encourage you to document your progress and\nshare it publicly on social media.\n\nIt helps you get noticed and will lead to:\n\n* Expanding your network: meeting new people and making new friends\n* Being invited to meetups, conferences and podcasts\n* Landing a job or getting clients\n* Many other good things\n\nHere's a more comprehensive reading on why you want to do it: https://github.com/readme/guides/publishing-your-work\n\n\n## Learning in Public for Zoomcamps\n\nWhen you submit your homework or project, you can also submit\nlearning in public posts:\n\n<img src=\"https://github.com/DataTalksClub/mlops-zoomcamp/raw/main/images/learning-in-public-links.png\" />\n\nYou can watch this video to see how your learning in public posts may look like:\n\n<a href=\"https://www.loom.com/share/710e3297487b409d94df0e8da1c984ce\" target=\"_blank\">\n    <img src=\"https://github.com/DataTalksClub/mlops-zoomcamp/raw/main/images/learning-in-public.png\" height=\"240\" />\n</a>\n\n## Daily Documentation\n\n- **Post Daily Diaries**: Document what you learn each day, including the challenges faced and the methods used to overcome them.\n- **Create Quick Videos**: Make short videos showcasing your work and upload them to GitHub.\n\nSend a PR if you want to suggest improvements for this document",
    "filename": "learning-in-public.md"
  },
  {
    "content": "## Course Project\n\n[\ud83c\udfa5 Projects how-to (watch it!)](https://www.loom.com/share/8f99d25893de4fb8aaa95c0395c740b6)\n\n\n### Objective\n\nThe goal of this project is to apply everything we have learned\nin this course to build an end-to-end data pipeline.\n\n### Problem statement\n\nDevelop a dashboard with two tiles by:\n\n* Selecting a dataset of interest (see [Datasets](#datasets))\n* Creating a pipeline for processing this dataset and putting it to a datalake\n* Creating a pipeline for moving the data from the lake to a data warehouse\n* Transforming the data in the data warehouse: prepare it for the dashboard\n* Building a dashboard to visualize the data\n\n\n## Data Pipeline \n\nThe pipeline could be **stream** or **batch**: this is the first thing you'll need to decide \n\n* **Stream**: If you want to consume data in real-time and put them to data lake\n* **Batch**: If you want to run things periodically (e.g. hourly/daily)\n\n## Technologies \n\nYou don't have to limit yourself to technologies covered in the course. You can use alternatives as well:\n\n* **Cloud**: AWS, GCP, Azure, ...\n* **Infrastructure as code (IaC)**: Terraform, Pulumi, Cloud Formation, ...\n* **Workflow orchestration**: Airflow, Prefect, Luigi, ...\n* **Data Warehouse**: BigQuery, Snowflake, Redshift, ...\n* **Batch processing**: Spark, Flink, AWS Batch, ...\n* **Stream processing**: Kafka, Pulsar, Kinesis, ...\n\nIf you use a tool that wasn't covered in the course, be sure to explain what that tool does.\n\nIf you're not certain about some tools, ask in Slack.\n\n## Dashboard\n\nYou can use any of the tools shown in the course (Data Studio or Metabase) or any other BI tool of your choice to build a dashboard. If you do use another tool, please specify and make sure that the dashboard is somehow accessible to your peers. \n\nYour dashboard should contain at least two tiles, we suggest you include:\n\n- 1 graph that shows the distribution of some categorical data \n- 1 graph that shows the distribution of the data across a temporal line\n\nEnsure that your graph is easy to understand by adding references and titles.\n \nExample dashboard: ![image](https://user-images.githubusercontent.com/4315804/159771458-b924d0c1-91d5-4a8a-8c34-f36c25c31a3c.png)\n\n\n## Peer reviewing\n\n> [!IMPORTANT]  \n> To evaluate the projects, we'll use peer reviewing. This is a great opportunity for you to learn from each other.\n> * To get points for your project, you need to evaluate 3 projects of your peers\n> * You get 3 extra points for each evaluation\n\n## Evaluation Criteria\n\n* Problem description\n    * 0 points: Problem is not described\n    * 2 points: Problem is described but shortly or not clearly \n    * 4 points: Problem is well described and it's clear what the problem the project solves\n* Cloud\n    * 0 points: Cloud is not used, things run only locally\n    * 2 points: The project is developed in the cloud\n    * 4 points: The project is developed in the cloud and IaC tools are used\n* Data ingestion (choose either batch or stream)\n    * Batch / Workflow orchestration\n        * 0 points: No workflow orchestration\n        * 2 points: Partial workflow orchestration: some steps are orchestrated, some run manually\n        * 4 points: End-to-end pipeline: multiple steps in the DAG, uploading data to data lake\n    * Stream\n        * 0 points: No streaming system (like Kafka, Pulsar, etc)\n        * 2 points: A simple pipeline with one consumer and one producer\n        * 4 points: Using consumer/producers and streaming technologies (like Kafka streaming, Spark streaming, Flink, etc)\n* Data warehouse\n    * 0 points: No DWH is used\n    * 2 points: Tables are created in DWH, but not optimized\n    * 4 points: Tables are partitioned and clustered in a way that makes sense for the upstream queries (with explanation)\n* Transformations (dbt, spark, etc)\n    * 0 points: No tranformations\n    * 2 points: Simple SQL transformation (no dbt or similar tools)\n    * 4 points: Tranformations are defined with dbt, Spark or similar technologies\n* Dashboard\n    * 0 points: No dashboard\n    * 2 points: A dashboard with 1 tile\n    * 4 points: A dashboard with 2 tiles\n* Reproducibility\n    * 0 points: No instructions how to run the code at all\n    * 2 points: Some instructions are there, but they are not complete\n    * 4 points: Instructions are clear, it's easy to run the code, and the code works\n\n\n> [!NOTE]\n> It's highly recommended to create a new repository for your project (not inside an existing repo) with a meaningful title, such as\n> \"Quake Analytics Dashboard\" or \"Bike Data Insights\" and include as many details as possible in the README file. ChatGPT can assist you with this. Doing so will not only make it easier to showcase your project for potential job opportunities but also have it featured on the [Projects Gallery App](#projects-gallery).\n> If you leave the README file empty or with minimal details, there may be point deductions as per the [Evaluation Criteria](#evaluation-criteria).\n\n## Going the extra mile (Optional)\n\n> [!NOTE]\n> The following things are not covered in the course, are entirely optional and they will not be graded.\n\nHowever, implementing these could significantly enhance the quality of your project:\n\n* Add tests\n* Use make\n* Add CI/CD pipeline\n\nIf you intend to include this project in your portfolio, adding these additional features will definitely help you to stand out from others.\n\n## Cheating and plagiarism\n\nPlagiarism in any form is not allowed. Examples of plagiarism:\n\n* Taking somebody's else notebooks and projects (in full or partly) and using it for the capstone project\n* Re-using your own projects (in full or partly) from other courses and bootcamps\n* Re-using your midterm project from ML Zoomcamp in capstone\n* Re-using your ML Zoomcamp from previous iterations of the course\n\nViolating any of this will result in 0 points for this project.\n\n## Resources\n\n### Datasets\n\nRefer to the provided [datasets](datasets.md) for possible selection.\n\n### Helpful Links\n\n* [Unit Tests + CI for Airflow](https://www.astronomer.io/events/recaps/testing-airflow-to-bulletproof-your-code/)\n* [CI/CD for Airflow (with Gitlab & GCP state file)](https://engineering.ripple.com/building-ci-cd-with-airflow-gitlab-and-terraform-in-gcp)\n* [CI/CD for Airflow (with GitHub and S3 state file)](https://programmaticponderings.com/2021/12/14/devops-for-dataops-building-a-ci-cd-pipeline-for-apache-airflow-dags/)\n* [CD for Terraform](https://medium.com/towards-data-science/git-actions-terraform-for-data-engineers-scientists-gcp-aws-azure-448dc7c60fcc)\n* [Spark + Airflow](https://medium.com/doubtnut/github-actions-airflow-for-automating-your-spark-pipeline-c9dff32686b)\n\n\n### Projects Gallery\n\nExplore a collection of projects completed by members of our community. The projects cover a wide range of topics and utilize different tools and techniques. Feel free to delve into any project and see how others have tackled real-world problems with data, structured their code, and presented their findings. It's a great resource to learn and get ideas for your own projects.\n\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://datatalksclub-projects.streamlit.app/)\n\n### DE Zoomcamp 2023\n\n* [2023 Projects](../cohorts/2023/project.md)\n\n### DE Zoomcamp 2022\n\n* [2022 Projects](../cohorts/2022/project.md)",
    "filename": "projects/README.md"
  },
  {
    "content": "## Datasets\n\nHere are some datasets that you could use for the project:\n\n\n* [Kaggle](https://www.kaggle.com/datasets)\n* [AWS datasets](https://registry.opendata.aws/)\n* [UK government open data](https://data.gov.uk/)\n* [Github archive](https://www.gharchive.org)\n* [Awesome public datasets](https://github.com/awesomedata/awesome-public-datasets)\n* [Million songs dataset](http://millionsongdataset.com)\n* [Some random datasets](https://components.one/datasets/)\n* [COVID Datasets](https://www.reddit.com/r/datasets/comments/n3ph2d/coronavirus_datsets/)\n* [Datasets from Azure](https://docs.microsoft.com/en-us/azure/azure-sql/public-data-sets)\n* [Datasets from BigQuery](https://cloud.google.com/bigquery/public-data/)\n* [Dataset search engine from Google](https://datasetsearch.research.google.com/)\n* [Public datasets offered by different GCP services](https://cloud.google.com/solutions/datasets)\n* [European statistics datasets](https://ec.europa.eu/eurostat/data/database)\n* [Datasets for streaming](https://github.com/ColinEberhardt/awesome-public-streaming-datasets)\n* [Dataset for Santander bicycle rentals in London](https://cycling.data.tfl.gov.uk/)\n* [Common crawl data](https://commoncrawl.org/) (copy of the internet)\n* [NASA's EarthData](https://search.earthdata.nasa.gov/search) (May require introductory geospatial analysis)\n* Collection Of Data Repositories\n  * [part 1](https://www.kdnuggets.com/2022/04/complete-collection-data-repositories-part-1.html) (from agriculture and finance to government)\n  * [part 2](https://www.kdnuggets.com/2022/04/complete-collection-data-repositories-part-2.html) (from healthcare to transportation)\n* [Data For Good by Meta](https://dataforgood.facebook.com/dfg/tools)\n\nPRs with more datasets are welcome!\n\nIt's not mandatory that you use a dataset from this list. You can use any dataset you want.",
    "filename": "projects/datasets.md"
  }
]