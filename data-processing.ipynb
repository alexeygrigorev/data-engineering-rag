{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "deea5eb6-4af4-415b-8acd-9e02066523e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "extensions = {'md', 'mdx', 'py', 'sql', 'java', 'ipynb'}\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filepath = file_info.filename\n",
    "        filepath_lower = filepath.lower()\n",
    "\n",
    "        if filepath_lower.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        filename = filepath_lower.split('/')[-1]\n",
    "\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        ext = filename.split('.')[-1]\n",
    "\n",
    "        if ext not in extensions:\n",
    "            continue\n",
    "\n",
    "        filepath_edited = filepath.split('/', maxsplit=1)[1]\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filepath_edited\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef88a6ec-8511-4dd5-86ad-2ad9f967626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zoomcamp_data = read_repo_data('DataTalksClub', 'data-engineering-zoomcamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3031e517-b52b-4779-b463-16e2f6b04599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_zoomcamp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a77a382b-edc7-49c9-95a3-a40b05160159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-docker-terraform/1_terraform_gcp/1_terraform_overview.md\n",
      "01-docker-terraform/1_terraform_gcp/2_gcp_overview.md\n",
      "01-docker-terraform/1_terraform_gcp/README.md\n",
      "01-docker-terraform/1_terraform_gcp/terraform/README.md\n",
      "01-docker-terraform/1_terraform_gcp/windows.md\n",
      "01-docker-terraform/2_docker_sql/README.md\n",
      "01-docker-terraform/2_docker_sql/data-loading-parquet.ipynb\n",
      "01-docker-terraform/2_docker_sql/data-loading-parquet.py\n",
      "01-docker-terraform/2_docker_sql/ingest_data.py\n",
      "01-docker-terraform/2_docker_sql/pg-test-connection.ipynb\n",
      "01-docker-terraform/2_docker_sql/pipeline.py\n",
      "01-docker-terraform/2_docker_sql/upload-data.ipynb\n",
      "01-docker-terraform/README.md\n",
      "02-workflow-orchestration/README.md\n",
      "03-data-warehouse/README.md\n",
      "03-data-warehouse/big_query.sql\n",
      "03-data-warehouse/big_query_hw.sql\n",
      "03-data-warehouse/big_query_ml.sql\n",
      "03-data-warehouse/extract_model.md\n",
      "03-data-warehouse/extras/README.md\n",
      "03-data-warehouse/extras/web_to_gcs.py\n",
      "04-analytics-engineering/README.md\n",
      "04-analytics-engineering/SQL_refresher.md\n",
      "04-analytics-engineering/dbt_cloud_setup.md\n",
      "04-analytics-engineering/docker_setup/README.md\n",
      "04-analytics-engineering/taxi_rides_ny/README.md\n",
      "04-analytics-engineering/taxi_rides_ny/analyses/hack-load-data.sql\n",
      "04-analytics-engineering/taxi_rides_ny/macros/get_payment_type_description.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/core/dim_zones.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/core/dm_monthly_zone_revenue.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/core/fact_trips.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/staging/stg_green_tripdata.sql\n",
      "04-analytics-engineering/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\n",
      "05-batch/README.md\n",
      "05-batch/code/03_test.ipynb\n",
      "05-batch/code/04_pyspark.ipynb\n",
      "05-batch/code/05_taxi_schema.ipynb\n",
      "05-batch/code/06_spark_sql.ipynb\n",
      "05-batch/code/06_spark_sql.py\n",
      "05-batch/code/06_spark_sql_big_query.py\n",
      "05-batch/code/07_groupby_join.ipynb\n",
      "05-batch/code/08_rdds.ipynb\n",
      "05-batch/code/09_spark_gcs.ipynb\n",
      "05-batch/code/cloud.md\n",
      "05-batch/code/homework.ipynb\n",
      "05-batch/setup/hadoop-yarn.md\n",
      "05-batch/setup/linux.md\n",
      "05-batch/setup/macos.md\n",
      "05-batch/setup/pyspark.md\n",
      "05-batch/setup/windows.md\n",
      "06-streaming/README.md\n",
      "06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecord.java\n",
      "06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordCompatible.java\n",
      "06-streaming/java/kafka_examples/build/generated-main-avro-java/schemaregistry/RideRecordNoneCompatible.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/AvroProducer.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonConsumer.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStream.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamJoins.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonKStreamWindow.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducer.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/JsonProducerPickupLocation.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/Secrets.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/Topics.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/customserdes/CustomSerdes.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/data/PickupLocation.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/data/Ride.java\n",
      "06-streaming/java/kafka_examples/src/main/java/org/example/data/VendorInfo.java\n",
      "06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamJoinsTest.java\n",
      "06-streaming/java/kafka_examples/src/test/java/org/example/JsonKStreamTest.java\n",
      "06-streaming/java/kafka_examples/src/test/java/org/example/helper/DataGeneratorHelper.java\n",
      "06-streaming/ksqldb/commands.md\n",
      "06-streaming/pyflink/README.md\n",
      "06-streaming/pyflink/homework.md\n",
      "06-streaming/pyflink/src/job/aggregation_job.py\n",
      "06-streaming/pyflink/src/job/start_job.py\n",
      "06-streaming/pyflink/src/job/taxi_job.py\n",
      "06-streaming/pyflink/src/producers/load_taxi_data.py\n",
      "06-streaming/pyflink/src/producers/producer.py\n",
      "06-streaming/python/README.md\n",
      "06-streaming/python/avro_example/consumer.py\n",
      "06-streaming/python/avro_example/producer.py\n",
      "06-streaming/python/avro_example/ride_record.py\n",
      "06-streaming/python/avro_example/ride_record_key.py\n",
      "06-streaming/python/avro_example/settings.py\n",
      "06-streaming/python/docker/README.md\n",
      "06-streaming/python/json_example/consumer.py\n",
      "06-streaming/python/json_example/producer.py\n",
      "06-streaming/python/json_example/ride.py\n",
      "06-streaming/python/json_example/settings.py\n",
      "06-streaming/python/redpanda_example/README.md\n",
      "06-streaming/python/redpanda_example/consumer.py\n",
      "06-streaming/python/redpanda_example/producer.py\n",
      "06-streaming/python/redpanda_example/ride.py\n",
      "06-streaming/python/redpanda_example/settings.py\n",
      "06-streaming/python/streams-example/faust/branch_price.py\n",
      "06-streaming/python/streams-example/faust/producer_taxi_json.py\n",
      "06-streaming/python/streams-example/faust/stream.py\n",
      "06-streaming/python/streams-example/faust/stream_count_vendor_trips.py\n",
      "06-streaming/python/streams-example/faust/taxi_rides.py\n",
      "06-streaming/python/streams-example/faust/windowing.py\n",
      "06-streaming/python/streams-example/pyspark/README.md\n",
      "06-streaming/python/streams-example/pyspark/consumer.py\n",
      "06-streaming/python/streams-example/pyspark/producer.py\n",
      "06-streaming/python/streams-example/pyspark/settings.py\n",
      "06-streaming/python/streams-example/pyspark/streaming-notebook.ipynb\n",
      "06-streaming/python/streams-example/pyspark/streaming.py\n",
      "06-streaming/python/streams-example/redpanda/README.md\n",
      "06-streaming/python/streams-example/redpanda/consumer.py\n",
      "06-streaming/python/streams-example/redpanda/producer.py\n",
      "06-streaming/python/streams-example/redpanda/settings.py\n",
      "06-streaming/python/streams-example/redpanda/streaming-notebook.ipynb\n",
      "06-streaming/python/streams-example/redpanda/streaming.py\n",
      "README.md\n",
      "after-sign-up.md\n",
      "asking-questions.md\n",
      "awesome-data-engineering.md\n",
      "certificates.md\n",
      "cohorts/2022/README.md\n",
      "cohorts/2022/project.md\n",
      "cohorts/2022/week_1_basics_n_setup/homework.md\n",
      "cohorts/2022/week_2_data_ingestion/README.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/1_setup_official.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/2_setup_nofrills.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/README.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py\n",
      "cohorts/2022/week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py\n",
      "cohorts/2022/week_2_data_ingestion/airflow/dags_local/ingest_script.py\n",
      "cohorts/2022/week_2_data_ingestion/airflow/docs/1_concepts.md\n",
      "cohorts/2022/week_2_data_ingestion/airflow/extras/data_ingestion_gcs_dag_ex2.py\n",
      "cohorts/2022/week_2_data_ingestion/homework/homework.md\n",
      "cohorts/2022/week_2_data_ingestion/homework/solution.py\n",
      "cohorts/2022/week_2_data_ingestion/transfer_service/README.md\n",
      "cohorts/2022/week_3_data_warehouse/airflow/1_setup_official.md\n",
      "cohorts/2022/week_3_data_warehouse/airflow/2_setup_nofrills.md\n",
      "cohorts/2022/week_3_data_warehouse/airflow/README.md\n",
      "cohorts/2022/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py\n",
      "cohorts/2022/week_5_batch_processing/homework.md\n",
      "cohorts/2022/week_6_stream_processing/homework.md\n",
      "cohorts/2023/README.md\n",
      "cohorts/2023/leaderboard.md\n",
      "cohorts/2023/project.md\n",
      "cohorts/2023/week_1_docker_sql/homework.md\n",
      "cohorts/2023/week_1_terraform/homework.md\n",
      "cohorts/2023/week_2_workflow_orchestration/README.md\n",
      "cohorts/2023/week_2_workflow_orchestration/homework.md\n",
      "cohorts/2023/week_3_data_warehouse/homework.md\n",
      "cohorts/2023/week_4_analytics_engineering/homework.md\n",
      "cohorts/2023/week_5_batch_processing/homework.md\n",
      "cohorts/2023/week_6_stream_processing/homework.md\n",
      "cohorts/2023/week_6_stream_processing/producer_confluent.py\n",
      "cohorts/2023/week_6_stream_processing/settings.py\n",
      "cohorts/2023/week_6_stream_processing/streaming_confluent.py\n",
      "cohorts/2023/workshops/piperider.md\n",
      "cohorts/2024/01-docker-terraform/homework.md\n",
      "cohorts/2024/01-docker-terraform/solutions.md\n",
      "cohorts/2024/02-workflow-orchestration/README.md\n",
      "cohorts/2024/02-workflow-orchestration/homework.md\n",
      "cohorts/2024/03-data-warehouse/homework.md\n",
      "cohorts/2024/04-analytics-engineering/homework.md\n",
      "cohorts/2024/05-batch/homework.md\n",
      "cohorts/2024/06-streaming/homework.md\n",
      "cohorts/2024/README.md\n",
      "cohorts/2024/leaderboard.md\n",
      "cohorts/2024/project.md\n",
      "cohorts/2024/workshops/dlt.md\n",
      "cohorts/2024/workshops/dlt_resources/data_ingestion_workshop.md\n",
      "cohorts/2024/workshops/dlt_resources/homework_solution.ipynb\n",
      "cohorts/2024/workshops/dlt_resources/homework_starter.ipynb\n",
      "cohorts/2024/workshops/dlt_resources/workshop.ipynb\n",
      "cohorts/2024/workshops/rising-wave.md\n",
      "cohorts/2025/01-docker-terraform/homework.md\n",
      "cohorts/2025/01-docker-terraform/solution.md\n",
      "cohorts/2025/02-workflow-orchestration/homework.md\n",
      "cohorts/2025/02-workflow-orchestration/solution.md\n",
      "cohorts/2025/03-data-warehouse/DLT_upload_to_GCP.ipynb\n",
      "cohorts/2025/03-data-warehouse/homework.md\n",
      "cohorts/2025/03-data-warehouse/load_yellow_taxi_data.py\n",
      "cohorts/2025/04-analytics-engineering/homework.md\n",
      "cohorts/2025/05-batch/homework.md\n",
      "cohorts/2025/05-batch/homework/solution.ipynb\n",
      "cohorts/2025/06-streaming/homework.md\n",
      "cohorts/2025/06-streaming/homework/homework.ipynb\n",
      "cohorts/2025/README.md\n",
      "cohorts/2025/project.md\n",
      "cohorts/2025/workshops/dlt/README.md\n",
      "cohorts/2025/workshops/dlt/data_ingestion_workshop.md\n",
      "cohorts/2025/workshops/dlt/dlt_homework.md\n",
      "cohorts/2025/workshops/dynamic_load_dlt.py\n",
      "dataset.md\n",
      "learning-in-public.md\n",
      "projects/README.md\n",
      "projects/datasets.md\n"
     ]
    }
   ],
   "source": [
    "for record in de_zoomcamp_data:\n",
    "    print(record['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185a28c-2355-4d0f-94ca-3ba126244f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35242f-16c7-420e-9118-cab5e8567f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f42422-e0e9-4dba-a75e-b1465e7b2cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320a93c-1d82-4a15-aa53-a6961ea89b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
