{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deea5eb6-4af4-415b-8acd-9e02066523e8",
   "metadata": {
    "_sphinx_cell_id": "89577dfe-ecc1-4b36-a665-58185dcaa4cf"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf69474d-d31f-4945-b440-95c68f5c30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_extensions = {'md', 'mdx'}\n",
    "code_extensions = {'py', 'sql', 'java', 'ipynb'}\n",
    "\n",
    "extensions = doc_extensions | code_extensions\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filepath = file_info.filename\n",
    "        filepath_lower = filepath.lower()\n",
    "\n",
    "        if filepath_lower.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        filename = filepath_lower.split('/')[-1]\n",
    "\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        ext = filename.split('.')[-1]\n",
    "\n",
    "        if ext not in extensions:\n",
    "            continue\n",
    "\n",
    "        filepath_edited = filepath.split('/', maxsplit=1)[1]\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                if ext in doc_extensions:\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data['filename'] = filepath_edited\n",
    "                elif ext in code_extensions:\n",
    "                    data = {\n",
    "                        'code': True,\n",
    "                        'content': content,\n",
    "                        'filename': filepath_edited\n",
    "                    }\n",
    "\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef88a6ec-8511-4dd5-86ad-2ad9f967626a",
   "metadata": {
    "_sphinx_cell_id": "125c4d5b-7113-484d-aca0-a83fde964a48"
   },
   "outputs": [],
   "source": [
    "de_zoomcamp_data = read_repo_data('DataTalksClub', 'data-engineering-zoomcamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3031e517-b52b-4779-b463-16e2f6b04599",
   "metadata": {
    "_sphinx_cell_id": "f956d5ae-868c-4ded-a0fa-a3bad88c6b2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_zoomcamp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77a382b-edc7-49c9-95a3-a40b05160159",
   "metadata": {
    "_sphinx_cell_id": "e55121aa-c222-4ac3-9911-37161a43f2ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for record in de_zoomcamp_data:\n",
    "    index[record['filename']] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f185a28c-2355-4d0f-94ca-3ba126244f14",
   "metadata": {
    "_sphinx_cell_id": "d4ce051b-aaae-4725-9eef-869cf50e26fb"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "from nbconvert.preprocessors import ClearOutputPreprocessor\n",
    "\n",
    "exporter = MarkdownExporter()\n",
    "exporter.register_preprocessor(ClearOutputPreprocessor(), enabled=True)\n",
    "\n",
    "def format_notebook_as_md(raw_notebook: str) -> str:\n",
    "    nb_parsed = nbformat.reads(\n",
    "        raw_notebook,\n",
    "        as_version=nbformat.NO_CONVERT,\n",
    "    )\n",
    "    md_body, _ = exporter.from_notebook_node(nb_parsed)\n",
    "    return md_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad8d3523-688c-4724-8b6c-47247388b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_code_fence(text: str) -> str:\n",
    "    text = text.strip()\n",
    "\n",
    "    if not text.startswith(\"```\"):\n",
    "        return text\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    lines = lines[1:]\n",
    "\n",
    "    if lines and lines[-1].strip() == \"```\":\n",
    "        lines = lines[:-1]\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00f42422-e0e9-4dba-a75e-b1465e7b2cd4",
   "metadata": {
    "_sphinx_cell_id": "c933285c-97e4-43f6-9f7d-caba91beaeea"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c320a93c-1d82-4a15-aa53-a6961ea89b68",
   "metadata": {
    "_sphinx_cell_id": "8021b298-87b3-4f8f-8ee7-b675e66bf691"
   },
   "outputs": [],
   "source": [
    "def llm(instructions, content, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages,\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da62b0a0-62b4-4f38-8e85-d2b5ca3e4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_editing_instructions = \"\"\"\n",
    "You're a professional coding editor.\n",
    "\n",
    "You are given a Markdown file that was converted from a Jupyter notebook.  \n",
    "The file already contains code blocks and inline comments.  \n",
    "\n",
    "Your task:\n",
    "\n",
    "- Turn it into clear, well-structured documentation.  \n",
    "- Add section headers (##) where appropriate. Keep sections relatively large (8-10 paragraphs and code blocks)\n",
    "- Add concise, high-level explanations for each code block.  \n",
    "- Summarize what the code is doing without being overly verbose.  \n",
    "- Keep the formatting in Markdown.\n",
    "- Aim for a balance: clear enough to guide someone new, but not overloaded with detail. \n",
    "\n",
    "Output the improved Markdown file with the new documentation.\n",
    "\"\"\".strip()\n",
    "\n",
    "code_doc_instructions = \"\"\"\n",
    "You are given a piece of source code.  \n",
    "\n",
    "Your task:  \n",
    "- Analyze the code and produce a clear, high-level description of what it does.  \n",
    "- If the code defines functions, methods, or classes, describe their purpose and role.  \n",
    "- If it’s just a script without explicit functions/classes, summarize what the script does step by step at a high level.  \n",
    "- Add logical sections or headings (##) if needed. Sections must be relatively large (8-10 paragraphs and code blocks)\n",
    "- Keep explanations concise and clear — avoid unnecessary verbosity.  \n",
    "- Output the result in Markdown, structured like documentation.  \n",
    "- Do not rewrite or modify the code itself, only provide descriptive documentation.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c829f4e5-663b-45ee-becd-caf6b05f8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm(system_prompt, md_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a456b4aa-40f2-428c-973c-4d40f0005213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f0f975c-f603-4a3b-9a32-c261a44d1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0 jupyter notebooks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d24f63b42c4515a7edab82d5836500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipynb_data = []\n",
    "\n",
    "for record in de_zoomcamp_data:\n",
    "    if record.get('code') == True and record['filename'].endswith('.ipynb'):\n",
    "        ipynb_data.append(record)\n",
    "\n",
    "\n",
    "print(f'processing {len(ipynb_data)} jupyter notebooks...')\n",
    "\n",
    "for record in tqdm(ipynb_data):\n",
    "    md_body = format_notebook_as_md(record['content'])\n",
    "    new_content = llm(notebook_editing_instructions, md_body)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d791ab0-a49b-4439-a143-bd3e8c38067f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "941e67d0-9260-48ca-bcdf-8abb6401c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 78 code files...\n"
     ]
    }
   ],
   "source": [
    "code_data = []\n",
    "\n",
    "for record in de_zoomcamp_data:\n",
    "    if record.get('code') != True:\n",
    "        continue\n",
    "\n",
    "    path = record['filename']\n",
    "    ext = path.split('.')[-1]\n",
    "\n",
    "    if ext not in code_extensions:\n",
    "        continue\n",
    "\n",
    "    if ext == 'ipynb':\n",
    "        continue\n",
    "\n",
    "    # print(path)\n",
    "    code_data.append(record)\n",
    "\n",
    "print(f'processing {len(code_data)} code files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d5755e9-1f70-4510-b75c-6b952ad303fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5b443a30d34df386d1fdad219a63ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for record in tqdm(code_data):\n",
    "    code = record['content']\n",
    "\n",
    "    new_content = llm(code_doc_instructions, code)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d50b9af-207b-421c-a567-3aacc0bfacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1e5aeaab-dc9c-4b1d-a00b-ce29bbf9b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da555e70-19ba-4996-9f69-9cfd18a477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'data/de-zoomcamp-processed.json'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    json.dump(de_zoomcamp_data, f_out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e8984a6-78ac-4c05-b065-76f7dc910382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"content\": \"## Terraform Overview\\n\\n[Video](https://www.youtube.com/watch?v=18jIzE41fJ4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=2)\\n\\n### Concepts\\n\\n#### Introduction\\n\\n1. What is [Terraform](https://www.terraform.io)?\\n   * open-source tool by [HashiCorp](https://www.hashicorp.com), used for provisioning infrastructure resources\\n   * supports DevOps best practices for change management\\n   * Managing configuration files in source control to maintain an ideal provisioning state \\n     for testing and production environments\\n2. What is IaC?\\n   * Infrastructure-as-Code\\n   * build, change, and manage your infrastructure in a safe, consistent, and repeatable way \\n     by defining resource configurations that you can version, reuse, and share.\\n3. Some advantages\\n   * Infrastructure lifecycle management\\n   * Version control commits\\n   * Very useful for stack-based deployments, and with cloud providers such as AWS, GCP, Azure, K8S\\u2026\\n   * State-based approach to track resource changes throughout deployments\\n\\n\\n#### Files\\n\\n* `main.tf`\\n* `variables.tf`\\n* Optional: `resources.tf`, `output.tf`\\n* `.tfstate`\\n\\n#### Declarations\\n* `terraform`: configure basic Terraform settings to provision your infrastructure\\n   * `required_version`: minimum Terraform version to apply to your configuration\\n   * `backend`: stores Terraform's \\\"state\\\" snapshots, to map real-world resources to your configuration.\\n      * `local`: stores state file locally as `terraform.tfstate`\\n   * `required_providers`: specifies the providers required by the current module\\n* `provider`:\\n   * adds a set of resource types and/or data sources that Terraform can manage\\n   * The Terraform Registry is the main directory of publicly available providers from most major infrastructure platforms.\\n* `resource`\\n  * blocks to define components of your infrastructure\\n  * Project modules/resources: google_storage_bucket, google_bigquery_dataset, google_bigquery_table\\n* `variable` & `locals`\\n  * runtime arguments and constants\\n\\n\\n#### Execution steps\\n1. `terraform init`: \\n    * Initializes & configures the backend, installs plugins/providers, & checks out an existing configuration from a version control \\n2. `terraform plan`:\\n    * Matches/previews local changes against a remote state, and proposes an Execution Plan.\\n3. `terraform apply`: \\n    * Asks for approval to the proposed plan, and applies changes to cloud\\n4. `terraform destroy`\\n    * Removes your stack from the Cloud\\n\\n\\n### Terraform Workshop to create GCP Infra\\nContinue [here](./terraform): `week_1_basics_n_setup/1_terraform_gcp/terraform`\\n\\n\\n### References\\nhttps://learn.hashicorp.com/collections/terraform/gcp-get-started\",\n",
      "    \"filename\": \"01-docker-terraform/1_terraform_gcp/1_terraform_overview.md\"\n",
      "  },\n",
      "  {\n",
      "    \"content\": \"## GCP Overview\\n\\n[Video](https://www.youtube.com/watch?v=18jIzE41fJ4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=2)\\n\\n\\n### Project infrastructure modules in GCP:\\n* Google Cloud Storage (GCS): Data Lake\\n* BigQuery: Data Warehouse\\n\\n(Concepts explained in Week 2 - Data Ingestion)\\n\\n### Initial Setup\\n\\nFor this course, we'll use a free version (upto EUR 300 credits). \\n\\n1. Create an account with your Google email ID \\n2. Setup your first [project](https://console.cloud.google.com/) if you haven't already\\n    * eg. \\\"DTC DE Course\\\", and note down the \\\"Project ID\\\" (we'll use this later when deploying infra with TF)\\n3. Setup [service account & authentication](https://cloud.google.com/docs/authentication/getting-started) for this project\\n    * Grant `Viewer` role to begin with.\\n    * Download service-account-keys (.json) for auth.\\n4. Download [SDK](https://cloud.google.com/sdk/docs/quickstart) for local setup\\n5. Set environment variable to point to your downloaded GCP keys:\\n   ```shell\\n   export GOOGLE_APPLICATION_CREDENTIALS=\\\"<path/to/your/service-account-authkeys>.json\\\"\\n   \\n   # Refresh token/session, and verify authentication\\n   gcloud auth application-default login\\n   ```\\n   \\n### Setup for Access\\n \\n1. [IAM Roles](https://cloud.google.com/storage/docs/access-control/iam-roles) for Service account:\\n   * Go to the *IAM* section of *IAM & Admin* https://console.cloud.google.com/iam-admin/iam\\n   * Click the *Edit principal* icon for your service account.\\n   * Add these roles in addition to *Viewer* : **Storage Admin** + **Storage Object Admin** + **BigQuery Admin**\\n   \\n2. Enable these APIs for your project:\\n   * https://console.cloud.google.com/apis/library/iam.googleapis.com\\n   * https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com\\n   \\n3. Please ensure `GOOGLE_APPLICATION_CREDENTIALS` env-var is set.\\n   ```shell\\n   export GOOGLE_APPLICATION_CREDENTIALS=\\\"<path/to/your/service-account-authkeys>.json\\\"\\n   ```\\n \\n### Terraform Workshop to create GCP Infra\\nContinue [here](./terraform): `week_1_basics_n_setup/1_terraform_gcp/terraform`\",\n",
      "    \"filename\": \"01-docker-terraform/1_terraform_gcp/2_gcp_overview.md\"\n",
      "  },\n",
      "  {\n"
     ]
    }
   ],
   "source": [
    "!head data/de-zoomcamp-processed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "966c577e-28da-401a-84ff-9c69ddabb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "56d46bee-ed92-4f7c-bac3-3168560d0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zoomcamp_chunks = []\n",
    "\n",
    "for doc in de_zoomcamp_data:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    de_zoomcamp_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "655c97c2-d5cc-4b53-a31a-fccffb8424aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "865"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_zoomcamp_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4cb350c9-ffb1-4d2a-be36-d961e5bd450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 2000,\n",
       " 'chunk': \"d (e.g., '2019', '2020').\\n- `service`: The type of taxi service (e.g., 'green', 'yellow').\\n\\n### Step-by-Step Walkthrough\\n\\n1. **Monthly Loop**: The function iterates over the months from January to December.\\n  \\n2. **File Naming**: For each month, it constructs the name of the CSV file using the service type and year. Months are formatted to ensure two digits (e.g., '01', '02').\\n\\n3. **HTTP Request**: It downloads the CSV file from the constructed URL using the `requests` library. The downloaded content is saved as a local file.\\n\\n4. **Data Reading and Conversion**: The local CSV file is read into a pandas DataFrame with gzip compression. It is then converted and saved as a Parquet file for efficient storage and querying.\\n\\n5. **GCS Upload**: The Parquet file is uploaded to the specified GCS bucket using the `upload_to_gcs` function.\\n\\n6. **Logging**: Throughout the process, the script outputs messages to provide feedback about the local file's creation and successful uploads to GCS.\\n\\n## Dataset Processing Execution\\n\\nAt the bottom of the script, two function calls for processing specific years and services are provided:\\n\\n```python\\nweb_to_gcs('2019', 'green')\\nweb_to_gcs('2020', 'green')\\n```\\n\\nThese lines indicate that the script is set up to process green taxi data for the years 2019 and 2020. Additional lines for yellow taxi data are commented out, which implies they may be intended for future execution.\\n\\n## Conclusion\\n\\nThis script serves as a straightforward utility for downloading NYC taxi trip data, transforming it into a more efficient format, and uploading it to cloud storage. It can be easily extended by modifying the function calls for different years or service types, enhancing its versatility for various data processing needs.\",\n",
       " 'code': False,\n",
       " 'filename': '03-data-warehouse/extras/web_to_gcs.py'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_zoomcamp_chunks[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b39ee7-b0c1-41cf-8954-74fcfd61d17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
