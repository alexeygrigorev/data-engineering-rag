{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0a29d6-ef2a-4cb2-ab42-e14ca029efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zc_agent import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcd2cc6-74d5-44a7-b879-6053af738629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data ingestion...\n",
      "Data indexing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "index = main.initialize_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85cfc61f-0095-4733-a33a-c788a50aef60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "865"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a49bde-f40a-4bd7-8370-10cfb485aa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 2000,\n",
       "  'content': \":\\n    ```\\n    docker system prune\\n    ```\\n    \\n   Also, empty the airflow `logs` directory.\\n    \\n  2. Build the image (only first-time, or when there's any change in the `Dockerfile`):\\n  Takes ~5-10 mins for the first-time\\n    ```shell\\n    docker-compose build\\n    ```\\n    or (for legacy versions)\\n    ```shell\\n    docker build .\\n    ```\\n\\n  3. Kick up the all the services from the container (no need to specially initialize):\\n    ```shell\\n    docker-compose -f docker-compose-nofrills.yml up\\n    ```\\n\\n  4. In another terminal, run `docker ps` to see which containers are up & running (there should be 3, matching with the services in your docker-compose file).\\n\\n  5. Login to Airflow web UI on `localhost:8080` with creds: `admin/admin` (explicit creation of admin user was required)\\n\\n  6. Run your DAG on the Web Console.\\n\\n  7. On finishing your run or to shut down the container/s:\\n    ```shell\\n    docker-compose down\\n    ```\\n    \\n### Setup - Taken from DE Zoomcamp 2.3.4 - Optional: Lightweight Local Setup for Airflow\\n\\nUse the docker-compose_2.3.4.yaml file (and rename it to docker-compose.yaml). Don't forget to replace the variables `GCP_PROJECT_ID` and `GCP_GCS_BUCKET`.\\n\\n### Future Enhancements\\n* Deploy self-hosted Airflow setup on Kubernetes cluster, or use a Managed Airflow (Cloud Composer) service by GCP\\n\\n### References\\nFor more info, check out these official docs:\\n   * https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html\\n   * https://airflow.apache.org/docs/docker-stack/build.html\\n   * https://airflow.apache.org/docs/docker-stack/recipes.html\",\n",
       "  'filename': 'cohorts/2022/week_2_data_ingestion/airflow/README.md'},\n",
       " {'start': 2000,\n",
       "  'content': \"``\\n    \\n   Or, if you need to clear your system of any pre-cached Docker issues:\\n    ```\\n    docker system prune\\n    ```\\n    \\n   Also, empty the airflow `logs` directory.\\n    \\n  2. Build the image (only first-time, or when there's any change in the `Dockerfile`):\\n  Takes ~5-10 mins for the first-time\\n    ```shell\\n    docker-compose build\\n    ```\\n    or (for legacy versions)\\n    ```shell\\n    docker build .\\n    ```\\n\\n  3. Kick up the all the services from the container (no need to specially initialize):\\n    ```shell\\n    docker-compose -f docker-compose-nofrills.yml up\\n    ```\\n\\n  4. In another terminal, run `docker ps` to see which containers are up & running (there should be 3, matching with the services in your docker-compose file).\\n\\n  5. Login to Airflow web UI on `localhost:8080` with creds: `admin/admin` (explicit creation of admin user was required)\\n\\n  6. Run your DAG on the Web Console.\\n\\n  7. On finishing your run or to shut down the container/s:\\n    ```shell\\n    docker-compose down\\n    ```\\n    \\n   \\n\\n### Future Enhancements\\n* Deploy self-hosted Airflow setup on Kubernetes cluster, or use a Managed Airflow (Cloud Composer) service by GCP\\n\\n### References\\nFor more info, check out these official docs:\\n   * https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html\\n   * https://airflow.apache.org/docs/docker-stack/build.html\\n   * https://airflow.apache.org/docs/docker-stack/recipes.html\",\n",
       "  'filename': 'cohorts/2022/week_3_data_warehouse/airflow/README.md'},\n",
       " {'start': 0,\n",
       "  'content': \"### Concepts\\n\\n [Airflow Concepts and Architecture](../week_2_data_ingestion/airflow/docs/1_concepts.md)\\n\\n### Workflow\\n\\n ![](docs/gcs_2_bq_dag_graph_view.png)\\n \\n ![](docs/gcs_2_bq_dag_tree_view.png)\\n \\n### Setup - Official Version\\n (For the section on the Custom/Lightweight setup, scroll down)\\n\\n #### Setup\\n  [Airflow Setup with Docker, through official guidelines](1_setup_official.md)\\n\\n #### Execution\\n \\n  1. Build the image (only first-time, or when there's any change in the `Dockerfile`, takes ~15 mins for the first-time):\\n     ```shell\\n     docker-compose build\\n     ```\\n   \\n     or (for legacy versions)\\n   \\n     ```shell\\n     docker build .\\n     ```\\n\\n 2. Initialize the Airflow scheduler, DB, and other config\\n    ```shell\\n    docker-compose up airflow-init\\n    ```\\n\\n 3. Kick up the all the services from the container:\\n    ```shell\\n    docker-compose up\\n    ```\\n\\n 4. In another terminal, run `docker-compose ps` to see which containers are up & running (there should be 7, matching with the services in your docker-compose file).\\n\\n 5. Login to Airflow web UI on `localhost:8080` with default creds: `airflow/airflow`\\n\\n 6. Run your DAG on the Web Console.\\n\\n 7. On finishing your run or to shut down the container/s:\\n    ```shell\\n    docker-compose down\\n    ```\\n\\n    To stop and delete containers, delete volumes with database data, and download images, run:\\n    ```\\n    docker-compose down --volumes --rmi all\\n    ```\\n\\n    or\\n    ```\\n    docker-compose down --volumes --remove-orphans\\n    ```\\n       \\n### Setup - Custom No-Frills Version (Lightweight)\\nThis is a quick, simple & less memory-intensive setup of Airflow that works on a LocalExecutor.\\n\\n  #### Setup\\n  [Airflow Setup with Docker, customized](2_setup_nofrills.md)\\n\\n  #### Execution\\n  \\n  1. Stop and delete containers, delete volumes with database data, & downloaded images (from the previous setup):\\n    ```\\n    docker-compose down --volumes --rmi all\\n    ```\\n\\n   or\\n    ```\\n    docker-compose down --volumes --remove-orphans\\n    `\",\n",
       "  'filename': 'cohorts/2022/week_3_data_warehouse/airflow/README.md'},\n",
       " {'start': 0,\n",
       "  'content': \"### Concepts\\n\\n [Airflow Concepts and Architecture](docs/1_concepts.md)\\n\\n### Workflow\\n\\n ![](docs/gcs_ingestion_dag.png)\\n \\n### Setup - Official Version\\n (For the section on the Custom/Lightweight setup, scroll down)\\n\\n #### Setup\\n  [Airflow Setup with Docker, through official guidelines](1_setup_official.md)\\n\\n #### Execution\\n \\n  1. Build the image (only first-time, or when there's any change in the `Dockerfile`, takes ~15 mins for the first-time):\\n     ```shell\\n     docker-compose build\\n     ```\\n   \\n     or (for legacy versions)\\n   \\n     ```shell\\n     docker build .\\n     ```\\n\\n 2. Initialize the Airflow scheduler, DB, and other config\\n    ```shell\\n    docker-compose up airflow-init\\n    ```\\n\\n 3. Kick up the all the services from the container:\\n    ```shell\\n    docker-compose up\\n    ```\\n\\n 4. In another terminal, run `docker-compose ps` to see which containers are up & running (there should be 7, matching with the services in your docker-compose file).\\n\\n 5. Login to Airflow web UI on `localhost:8080` with default creds: `airflow/airflow`\\n\\n 6. Run your DAG on the Web Console.\\n\\n 7. On finishing your run or to shut down the container/s:\\n    ```shell\\n    docker-compose down\\n    ```\\n\\n    To stop and delete containers, delete volumes with database data, and download images, run:\\n    ```\\n    docker-compose down --volumes --rmi all\\n    ```\\n\\n    or\\n    ```\\n    docker-compose down --volumes --remove-orphans\\n    ```\\n       \\n### Setup - Custom No-Frills Version (Lightweight)\\nThis is a quick, simple & less memory-intensive setup of Airflow that works on a LocalExecutor.\\n\\n  #### Setup\\n  [Airflow Setup with Docker, customized](2_setup_nofrills.md)\\n\\n  #### Execution\\n  \\n  1. Stop and delete containers, delete volumes with database data, & downloaded images (from the previous setup):\\n    ```\\n    docker-compose down --volumes --rmi all\\n    ```\\n\\n   or\\n    ```\\n    docker-compose down --volumes --remove-orphans\\n    ```\\n    \\n   Or, if you need to clear your system of any pre-cached Docker issues\",\n",
       "  'filename': 'cohorts/2022/week_2_data_ingestion/airflow/README.md'},\n",
       " {'start': 1000,\n",
       "  'content': 'er. \\n    You have to make sure to configure them for the docker-compose:\\n\\n    ```bash\\n    mkdir -p ./dags ./logs ./plugins\\n    echo -e \"AIRFLOW_UID=$(id -u)\" > .env\\n    ```\\n\\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \\n\\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\\n    this content:\\n\\n    ```\\n    AIRFLOW_UID=50000\\n    ```\\n\\n   \\n3. **Import the official docker setup file** from the latest Airflow version:\\n   ```shell\\n   curl -LfO \\'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml\\'\\n   ```\\n   \\n4. It could be overwhelming to see a lot of services in here. \\n   But this is only a quick-start template, and as you proceed you\\'ll figure out which unused services can be removed.\\n   Eg. [Here\\'s](docker-compose-nofrills.yml) a no-frills version of that template.\\n\\n5. **Docker Build**:\\n\\n    When you want to run Airflow locally, you might want to use an extended image, \\n    containing some additional dependencies - for example you might add new python packages, \\n    or upgrade airflow providers to a later version.\\n    \\n    Create a `Dockerfile` pointing to Airflow version you\\'ve just downloaded, \\n    such as `apache/airflow:2.2.3`, as the base image,\\n       \\n    And customize this `Dockerfile` by:\\n    * Adding your custom packages to be installed. The one we\\'ll need the most is `gcloud` to connect with the GCS bucket/Data Lake.\\n    * Also, integrating `requirements.txt` to install libraries via  `pip install`\\n\\n6. **Docker Compose**:\\n\\n    Back in your `docker-compose.yaml`:\\n   * In `x-airflow-common`: \\n     * Remove the `image` tag, to replace it with your `build` from your Dockerfile, as shown\\n     * Mount your `google_credentials` in `volumes` section as read-only\\n     * Set environment variables: `GCP_PROJECT_ID`, `GCP_GCS_BUCKET`, `GOOGLE_APPLICATION_CREDENTIALS` & `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT`, as per your config.\\n   * Change `AIRFLOW__CORE__LOAD_EXAMP',\n",
       "  'filename': 'cohorts/2022/week_2_data_ingestion/airflow/1_setup_official.md'},\n",
       " {'start': 0,\n",
       "  'content': '## Setup (No-frills)\\n\\n### Pre-Reqs\\n\\n1. For the sake of standardization across this workshop\\'s config,\\n    rename your gcp-service-accounts-credentials file to `google_credentials.json` & store it in your `$HOME` directory\\n    ``` bash\\n        cd ~ && mkdir -p ~/.google/credentials/\\n        mv <path/to/your/service-account-authkeys>.json ~/.google/credentials/google_credentials.json\\n    ```\\n\\n2. You may need to upgrade your docker-compose version to v2.x+, and set the memory for your Docker Engine to minimum 4GB\\n(ideally 8GB). If enough memory is not allocated, it might lead to airflow-webserver continuously restarting.\\n\\n3. Python version: 3.7+\\n\\n\\n### Airflow Setup\\n\\n1. Create a new sub-directory called `airflow` in your `project` dir (such as the one we\\'re currently in)\\n   \\n2. **Set the Airflow user**:\\n\\n    On Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. \\n    Otherwise the files created in `dags`, `logs` and `plugins` will be created with root user. \\n    You have to make sure to configure them for the docker-compose:\\n\\n    ```bash\\n    mkdir -p ./dags ./logs ./plugins\\n    echo -e \"AIRFLOW_UID=$(id -u)\" >> .env\\n    ```\\n\\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \\n\\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\\n    this content:\\n\\n    ```\\n    AIRFLOW_UID=50000\\n    ```\\n\\n3. **Docker Build**:\\n\\n    When you want to run Airflow locally, you might want to use an extended image, \\n    containing some additional dependencies - for example you might add new python packages, \\n    or upgrade airflow providers to a later version.\\n    \\n    Create a `Dockerfile` pointing to the latest Airflow version such as `apache/airflow:2.2.3`, for the base image,\\n       \\n    And customize this `Dockerfile` by:\\n    * Adding your custom packages to be installed. The one we\\'ll need the most is `gcloud` to connect with the GCS bucket (Data Lake).\\n    * Also, in',\n",
       "  'filename': 'cohorts/2022/week_2_data_ingestion/airflow/2_setup_nofrills.md'},\n",
       " {'start': 1000,\n",
       "  'content': 'er. \\n    You have to make sure to configure them for the docker-compose:\\n\\n    ```bash\\n    mkdir -p ./dags ./logs ./plugins\\n    echo -e \"AIRFLOW_UID=$(id -u)\" > .env\\n    ```\\n\\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \\n\\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\\n    this content:\\n\\n    ```\\n    AIRFLOW_UID=50000\\n    ```\\n\\n3. **Import the official docker setup file** from the latest Airflow version:\\n   ```shell\\n   curl -LfO \\'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml\\'\\n   ```\\n   \\n4. It could be overwhelming to see a lot of services in here. \\n   But this is only a quick-start template, and as you proceed you\\'ll figure out which unused services can be removed.\\n   Eg. [Here\\'s](docker-compose-nofrills.yml) a no-frills version of that template.\\n\\n\\n5. **Docker Build**:\\n\\n    When you want to run Airflow locally, you might want to use an extended image, \\n    containing some additional dependencies - for example you might add new python packages, \\n    or upgrade airflow providers to a later version.\\n    \\n    Create a `Dockerfile` pointing to Airflow version you\\'ve just downloaded, \\n    such as `apache/airflow:2.2.3`, as the base image,\\n       \\n    And customize this `Dockerfile` by:\\n    * Adding your custom packages to be installed. The one we\\'ll need the most is `gcloud` to connect with the GCS bucket/Data Lake.\\n    * Also, integrating `requirements.txt` to install libraries via  `pip install`\\n\\n\\n6. **Docker Compose**:\\n\\n    Back in your `docker-compose.yaml`:\\n   * In `x-airflow-common`: \\n     * Remove the `image` tag, to replace it with your `build` from your Dockerfile, as shown\\n     * Mount your `google_credentials` in `volumes` section as read-only\\n     * Set environment variables: `GCP_PROJECT_ID`, `GCP_GCS_BUCKET`, `GOOGLE_APPLICATION_CREDENTIALS` & `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT`, as per your config.\\n\\n   * Change `AIRFLOW__CORE__LOAD_EXAMPL',\n",
       "  'filename': 'cohorts/2022/week_3_data_warehouse/airflow/1_setup_official.md'},\n",
       " {'start': 0,\n",
       "  'content': '## Setup (Official)\\n\\n### Pre-Reqs\\n\\n1. For the sake of standardization across this workshop\\'s config,\\n    rename your gcp-service-accounts-credentials file to `google_credentials.json` & store it in your `$HOME` directory\\n    ``` bash\\n        cd ~ && mkdir -p ~/.google/credentials/\\n        mv <path/to/your/service-account-authkeys>.json ~/.google/credentials/google_credentials.json\\n    ```\\n\\n2. You may need to upgrade your docker-compose version to v2.x+, and set the memory for your Docker Engine to minimum 5GB\\n(ideally 8GB). If enough memory is not allocated, it might lead to airflow-webserver continuously restarting.\\n\\n3. Python version: 3.7+\\n\\n\\n### Airflow Setup\\n\\n1. Create a new sub-directory called `airflow` in your `project` dir (such as the one we\\'re currently in)\\n\\n2. **Set the Airflow user**:\\n\\n    On Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. \\n    Otherwise the files created in `dags`, `logs` and `plugins` will be created with root user. \\n    You have to make sure to configure them for the docker-compose:\\n\\n    ```bash\\n    mkdir -p ./dags ./logs ./plugins\\n    echo -e \"AIRFLOW_UID=$(id -u)\" > .env\\n    ```\\n\\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \\n\\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\\n    this content:\\n\\n    ```\\n    AIRFLOW_UID=50000\\n    ```\\n\\n   \\n3. **Import the official docker setup file** from the latest Airflow version:\\n   ```shell\\n   curl -LfO \\'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml\\'\\n   ```\\n   \\n4. It could be overwhelming to see a lot of services in here. \\n   But this is only a quick-start template, and as you proceed you\\'ll figure out which unused services can be removed.\\n   Eg. [Here\\'s](docker-compose-nofrills.yml) a no-frills version of that template.\\n\\n5. **Docker Build**:\\n\\n    When you want to run Airflow locally, you might want to use an extended image, \\n    contain',\n",
       "  'filename': 'cohorts/2022/week_2_data_ingestion/airflow/1_setup_official.md'},\n",
       " {'start': 0,\n",
       "  'content': '## Setup (No-frills)\\n\\n### Pre-Reqs\\n\\n1. For the sake of standardization across this workshop\\'s config,\\n    rename your gcp-service-accounts-credentials file to `google_credentials.json` & store it in your `$HOME` directory\\n    ``` bash\\n        cd ~ && mkdir -p ~/.google/credentials/\\n        mv <path/to/your/service-account-authkeys>.json ~/.google/credentials/google_credentials.json\\n    ```\\n\\n2. You may need to upgrade your docker-compose version to v2.x+, and set the memory for your Docker Engine to minimum 4GB\\n(ideally 8GB). If enough memory is not allocated, it might lead to airflow-webserver continuously restarting.\\n\\n3. Python version: 3.7+\\n\\n\\n### Airflow Setup\\n\\n1. Create a new sub-directory called `airflow` in your `project` dir (such as the one we\\'re currently in)\\n   \\n2. **Set the Airflow user**:\\n\\n    On Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. \\n    Otherwise the files created in `dags`, `logs` and `plugins` will be created with root user. \\n    You have to make sure to configure them for the docker-compose:\\n\\n    ```bash\\n    mkdir -p ./dags ./logs ./plugins\\n    echo -e \"AIRFLOW_UID=$(id -u)\" >> .env\\n    ```\\n\\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \\n\\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\\n    this content:\\n\\n    ```\\n    AIRFLOW_UID=50000\\n    ```\\n\\n3. **Docker Build**:\\n\\n    When you want to run Airflow locally, you might want to use an extended image, \\n    containing some additional dependencies - for example you might add new python packages, \\n    or upgrade airflow providers to a later version.\\n    \\n    Create a `Dockerfile` pointing to the latest Airflow version such as `apache/airflow:2.2.3`, for the base image,\\n       \\n    And customize this `Dockerfile` by:\\n    * Adding your custom packages to be installed. The one we\\'ll need the most is `gcloud` to connect with the GCS bucket (Data Lake).\\n    * Also, in',\n",
       "  'filename': 'cohorts/2022/week_3_data_warehouse/airflow/2_setup_nofrills.md'},\n",
       " {'start': 0,\n",
       "  'content': '## Setup (Official)\\n\\n### Pre-Reqs\\n\\n1. For the sake of standardization across this workshop\\'s config,\\n    rename your gcp-service-accounts-credentials file to `google_credentials.json` & store it in your `$HOME` directory\\n    ``` bash\\n        cd ~ && mkdir -p ~/.google/credentials/\\n        mv <path/to/your/service-account-authkeys>.json ~/.google/credentials/google_credentials.json\\n    ```\\n\\n2. You may need to upgrade your docker-compose version to v2.x+, and set the memory for your Docker Engine to minimum 5GB\\n(ideally 8GB). If enough memory is not allocated, it might lead to airflow-webserver continuously restarting.\\n\\n3. Python version: 3.7+\\n\\n\\n### Airflow Setup\\n\\n1. Create a new sub-directory called `airflow` in your `project` dir (such as the one we\\'re currently in)\\n\\n2. **Set the Airflow user**:\\n\\n    On Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. \\n    Otherwise the files created in `dags`, `logs` and `plugins` will be created with root user. \\n    You have to make sure to configure them for the docker-compose:\\n\\n    ```bash\\n    mkdir -p ./dags ./logs ./plugins\\n    echo -e \"AIRFLOW_UID=$(id -u)\" > .env\\n    ```\\n\\n    On Windows you will probably also need it. If you use MINGW/GitBash, execute the same command. \\n\\n    To get rid of the warning (\"AIRFLOW_UID is not set\"), you can create `.env` file with\\n    this content:\\n\\n    ```\\n    AIRFLOW_UID=50000\\n    ```\\n\\n3. **Import the official docker setup file** from the latest Airflow version:\\n   ```shell\\n   curl -LfO \\'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml\\'\\n   ```\\n   \\n4. It could be overwhelming to see a lot of services in here. \\n   But this is only a quick-start template, and as you proceed you\\'ll figure out which unused services can be removed.\\n   Eg. [Here\\'s](docker-compose-nofrills.yml) a no-frills version of that template.\\n\\n\\n5. **Docker Build**:\\n\\n    When you want to run Airflow locally, you might want to use an extended image, \\n    containing',\n",
       "  'filename': 'cohorts/2022/week_3_data_warehouse/airflow/1_setup_official.md'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search('airflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59874486-950e-4d8f-a48a-999b1dbdfc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
