{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d70cd07-77e3-4eec-b106-6de8ae97ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LOGS_DIRECTORY'] = '../eval_logs'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2866c9-3f74-44da-a0b0-1cadc6a2d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "from logs import LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcac019-a799-411f-9f7a-7db2c8c885da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/alexe/git/aihero/code/eval/../eval_logs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_DIR.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b82a622-16a5-4f7b-b93d-cfe2fc22ecee",
   "metadata": {
    "_sphinx_cell_id": "d9fe0e7f-b2a0-41c1-a2dc-25788670dba7"
   },
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent’s answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG> for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user’s instructions (in <INSRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user’s question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str\n",
    "\n",
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-mini',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f331d4-bd09-4a72-b487-632cf1cd8472",
   "metadata": {
    "_sphinx_cell_id": "774b2272-a1b8-4ef3-bb07-b4791e72bf7c"
   },
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data\n",
    "\n",
    "\n",
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-return':\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc87ceac-a655-4167-bbe2-d27711a4eeed",
   "metadata": {
    "_sphinx_cell_id": "975a9704-f83d-40a4-82e2-bbf6515686d0"
   },
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "    \n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "    \n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c11bb52-af16-4a77-b0bc-debe4183cb52",
   "metadata": {
    "_sphinx_cell_id": "d066b5ee-5c62-4688-b0b9-fc62137d6e35"
   },
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'gh_agent' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record.get('source') != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b26fdb9-e66b-4f95-92cf-c7b3ed805fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5ec47cd-6adb-4cbd-998b-03f3ba3a4ac7",
   "metadata": {
    "_sphinx_cell_id": "2af7fac3-1872-42dd-8a7e-e599e26c3bcc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa5d469506747cab49b06e49397d922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63fbca15-d7b0-41f8-9f36-969d39250789",
   "metadata": {
    "_sphinx_cell_id": "af737e3b-7f91-49d7-b729-2d2a368f84c5"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "455a6b3e-3f72-475c-9fcf-7b9a78dcf3e3",
   "metadata": {
    "_sphinx_cell_id": "c34191c9-acb9-4523-9818-30d99e0d5da7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    100.0\n",
       "instructions_avoid     100.0\n",
       "answer_relevant        100.0\n",
       "answer_clear           100.0\n",
       "answer_citations       100.0\n",
       "completeness            70.0\n",
       "tool_call_search       100.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals = pd.DataFrame(rows)\n",
    "df_evals.mean(numeric_only=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2143361-b558-4da8-bb3d-e9c34562a920",
   "metadata": {
    "_sphinx_cell_id": "007e7c9b-0475-479b-a9ee-6b9556e8913c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fce7c6-fdeb-4e16-980f-5f0a3e6bed9b",
   "metadata": {
    "_sphinx_cell_id": "f9323841-960a-4f81-bf29-abb47e17e9ab"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
