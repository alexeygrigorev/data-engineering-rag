{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f74f81-848a-4c4b-90a3-39f203f65448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LOGS_DIRECTORY'] = '../eval_logs'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d004ec9-7068-4607-b28c-8575fb7b1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "from main import initialize_index, initialize_agent\n",
    "from logs import log_interaction_to_file, LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94ecece-98d9-47b2-8be1-43b1af37e220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/alexe/git/aihero/code/eval/../eval_logs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_DIR.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33835759-91b6-4619-a7a7-bce6a7552a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AI FAQ Assistant for DataTalksClub/faq\n",
      "Initializing data ingestion...\n",
      "Data indexing completed successfully!\n",
      "Initializing search agent...\n",
      "Agent initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "index = initialize_index()\n",
    "agent = initialize_agent(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a5927e-3ac3-4945-9bd7-a3a63181a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=QuestionsList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59416299-77c6-4477-aa53-ca0f89f230d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(index.docs, 10)\n",
    "prompt = [d['content'] for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd401eaf-36a5-4017-89f1-ce52029b2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await question_generator.run(user_prompt=json.dumps(prompt))\n",
    "question_list = result.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8066e930-70c8-45e6-b625-c1336fec2a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c6acc0481046a494ea5889623f43a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I fix the error that says `No matching signature for operator '=' for argument types: 'STRING', 'INT64'` in my SQL code?\n",
      "The error message `No matching signature for operator '=' for argument types: 'STRING', 'INT64'` typically occurs when you attempt to compare or assign a string type with an integer type. Here are some solutions to fix this issue:\n",
      "\n",
      "1. **Ensure Data Type Consistency**: Make sure the fields you are comparing or assigning are of the same data type. For example, if one field is `STRING`, the other should also be converted to `STRING`.\n",
      "\n",
      "   If you are using a `CASE` statement or a comparison in an SQL query, ensure that number literals are enclosed in quotes so they are treated as strings. For instance:\n",
      "   ```sql\n",
      "   CASE payment_type\n",
      "   WHEN '1' THEN 'Credit card'\n",
      "   WHEN '2' THEN 'Cash'\n",
      "   ...\n",
      "   END\n",
      "   ```\n",
      "\n",
      "2. **Casting Fields**: If one field is an integer (like `INT64`) and the other is a string, you can convert the integer to a string using a cast:\n",
      "   ```sql\n",
      "   CAST(your_integer_field AS STRING) = your_string_field\n",
      "   ```\n",
      "   Conversely, if your string needs to be an integer:\n",
      "   ```sql\n",
      "   CAST(your_string_field AS INT64) = your_integer_field\n",
      "   ```\n",
      "\n",
      "3. **Example from Data Engineering Course**: In the context of the Data Engineering course, if you have a `LocationID` in one table as `STRING` and in another as `INT64`, you should unify their types using casting:\n",
      "   ```sql\n",
      "   {{ dbt.safe_cast(\"PULocationID\", api.Column.translate_type(\"integer\")) }} as pickup_locationid\n",
      "   ```\n",
      "\n",
      "By addressing type mismatches through either converting the field types or adjusting the literals in your SQL, you should be able to resolve the error.\n",
      "\n",
      "For more detailed guidance, you can refer to these specific sources:\n",
      "- [dbt: macro errors with get_payment_type_description](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-4/039_89733da275_dbt-macro-errors-with-get_payment_type_description.md)\n",
      "- [Join Error on LocationID: \"Unable to find common supertype for templated argument\"](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-4/077_faddbcb675_join-error-on-locationid-unable-to-find-common-sup.md)\n",
      "\n",
      "What should I do if my `docker-compose.yaml` file refers to an undefined volume after following the video tutorial?\n",
      "If your `docker-compose.yaml` file refers to an undefined volume, you can resolve this issue by ensuring that the volume is defined in your YAML file. \n",
      "\n",
      "According to the documentation, if you encounter an error like:\n",
      "\n",
      "```\n",
      "service \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\n",
      "```\n",
      "\n",
      "You need to add a volume definition to your `docker-compose.yaml` file. You can do this by including the following lines under the `volumes` section:\n",
      "\n",
      "```yaml\n",
      "volumes:\n",
      "  dtc_postgres_volume_local:\n",
      "```\n",
      "\n",
      "Make sure your `docker-compose.yaml` file includes this section to avoid the undefined volume error.\n",
      "\n",
      "For further reference, you can see the documentation [here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/051_5f056e236c_docker-compose-error-undefined-volume-in-windowsws.md).\n",
      "\n",
      "What is the best way to handle missing values in `DOlocationID` and `PUlocationID` columns when using Pandas to inject data into GCS?\n",
      "When handling missing values in the `DOlocationID` and `PUlocationID` columns while using Pandas to inject data into Google Cloud Storage (GCS), it's recommended to ensure correct data types to avoid errors during the loading process to BigQuery.\n",
      "\n",
      "Here is the recommended approach:\n",
      "\n",
      "1. Use the `astype` method to convert the columns to the `Int64` data type. This type is specifically designed to support missing values (represented as `NaN` in Pandas), which allows for both integers and missing values:\n",
      "\n",
      "   ```python\n",
      "   df[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\n",
      "   df[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\n",
      "   ```\n",
      "\n",
      "2. It's essential to perform this data type conversion prior to loading the data into GCS. This ensures that the columns do not end up being cast to `float`, which could cause type mismatches with the schema defined in BigQuery.\n",
      "\n",
      "This approach effectively handles missing values and maintains data integrity across your pipeline.\n",
      "\n",
      "For additional details, see the reference: [GCP BQ: How to handle type error from BigQuery and Parquet data?](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-3/023_66d3c42dd4_gcp-bq-how-to-handle-type-error-from-bigquery-and.md)\n",
      "\n",
      "If I am not registered for the course, can I still submit my homework?\n",
      "Based on the course documentation, it appears that you need to be registered for the course in order to submit your homework. Although late submissions are typically not allowed, if the submission form remains open after the due date, you can still submit your homework. However, this doesn't explicitly confirm if unregistered individuals can submit homework. \n",
      "\n",
      "For specific details related to submissions, you are encouraged to check the course's official website or announcements for any updates. You can find further information in the course homeworks section: [2025 Homework and Project Deadlines](https://courses.datatalks.club/de-zoomcamp-2025/) ([source](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/016_d3f485cd10_homework-what-are-homework-and-project-deadlines.md)). \n",
      "\n",
      "If you're considering submitting and haven’t registered yet, it’s best to contact the course administrators for clarity on this issue.\n",
      "\n",
      "What are the different methods for setting up credentials for `dlt` in my project?\n",
      "You can set up credentials for `dlt` in your project using several methods. Here are the most common ones:\n",
      "\n",
      "### 1. Environment Variables (Easiest)\n",
      "   - This method allows you to set credentials via environment variables, which is a secure way to avoid hardcoding sensitive information in your code. For example, you can configure Google Cloud credentials by setting specific environment variables.\n",
      "\n",
      "### 2. Configuration Files (Recommended for Local Use)\n",
      "   - You can use `.dlt/secrets.toml` for sensitive credentials and `.dlt/config.toml` for non-sensitive configurations. \n",
      "   - An example for Google Cloud in `secrets.toml` might look like this:\n",
      "     ```toml\n",
      "     [google_cloud]\n",
      "     service_account_key = \"YOUR_SERVICE_ACCOUNT_KEY\"\n",
      "     ```\n",
      "   - Make sure to place these files in the `.dlt` folder of your project and never commit `secrets.toml` to version control (add it to `.gitignore`).\n",
      "\n",
      "### Additional Methods\n",
      "   - Credentials can also be stored and loaded from external sources such as vaults, AWS Parameter Store, or through custom configurations.\n",
      "\n",
      "For detailed information, you may refer to the official documentation on setting up credentials: [dlt Credentials Documentation](https://dlthub.com/docs/general-usage/credentials/) [LINK TITLE](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/009_d2add610e3_how-do-i-set-up-credentials-to-run-dlt-in-my-envir.md).\n",
      "\n",
      "What should I do if I encounter the error `ImportError: DLL load failed while importing _sqlite3` in my Anaconda environment?\n",
      "If you encounter the error `ImportError: DLL load failed while importing _sqlite3` in your Anaconda environment, it is likely due to the absence of the `sqlite3.dll` file. To resolve this issue, follow these steps:\n",
      "\n",
      "1. **Locate the DLL File:**\n",
      "   - Find the `sqlite3.dll` file in the directory `C:\\Anaconda3\\Library\\bin`.\n",
      "\n",
      "2. **Copy the DLL File:**\n",
      "   - Copy the `sqlite3.dll` file from `C:\\Anaconda3\\Library\\bin`.\n",
      "\n",
      "3. **Paste the DLL File:**\n",
      "   - Paste the file into the `C:\\Anaconda3\\Dlls` directory.\n",
      "\n",
      "After completing these steps, try to import `_sqlite3` again in your Python environment.\n",
      "\n",
      "For more details, you can refer to the source material from the course: [ImportError: DLL load failed while importing _sqlite3](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/083_646e2067e8_python-modulenotfounderror-no-module-named-pysqlit.md).\n",
      "\n",
      "I'm running out of storage while backfilling on GCP VM, how can I manage my storage better?\n",
      "If you're running out of storage on your GCP VM while backfilling, here are some strategies to manage your storage effectively:\n",
      "\n",
      "1. **Clean Up Your GCP VM Drive**: You can identify which files or directories are using the most space by running the following command:\n",
      "\n",
      "   ```bash\n",
      "   sudo du -sh *\n",
      "   ```\n",
      "\n",
      "   This will give you a summary of disk usage for items in the current directory.\n",
      "\n",
      "2. **Remove Unused Applications**: For instance, if you have installed Anaconda and don't need it anymore, you can delete the Anaconda installer and clean up space. Here's how you can remove it:\n",
      "\n",
      "   ```bash\n",
      "   rm -rf <anacondainstaller_fpath>  # Replace with the actual path to the Anaconda installer.\n",
      "   ```\n",
      "\n",
      "   If you still need Anaconda but want to free up some space, you can run:\n",
      "\n",
      "   ```bash\n",
      "   conda clean --all -y\n",
      "   ```\n",
      "\n",
      "3. **Clean Up Kestra Files**: To manage your Kestra files, you can use a purge flow. For immediate cleanup, you may adjust the `endDate` to reflect the present time, and decide if you'd like to remove entries in a \"FAILED\" state. You can find guidance on this in the Kestra documentation [here](https://kestra.io/docs/administrator-guide/purge).\n",
      "\n",
      "4. **Manage PostgreSQL Database**: Clean up your PostgreSQL database by manually deleting unnecessary tables through pgAdmin or by setting up a workflow in Kestra.\n",
      "\n",
      "By implementing these steps, you should be able to manage your storage more effectively and prevent running out of space further down the line. \n",
      "\n",
      "For further details, you can refer to the source material: [Running out of storage when using Kestra with Postgres on GCP VM](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-2/007_00e8093b90_running-out-of-storage-when-using-kestra-with-post.md).\n",
      "\n",
      "How can I resolve the rename issue with the downloaded Docker Compose file on my Google Cloud VM?\n",
      "To resolve the rename issue with the downloaded Docker Compose file on your Google Cloud VM, you need to rename the file to make it executable easily. Here’s how you can do this:\n",
      "\n",
      "1. If your downloaded Docker Compose file is named `docker-compose-linux-x86_64`, you can rename it by executing the following command in your terminal:\n",
      "\n",
      "   ```bash\n",
      "   mv docker-compose-linux-x86_64 docker-compose\n",
      "   ```\n",
      "\n",
      "2. After renaming, make sure to give it executable permissions using this command:\n",
      "\n",
      "   ```bash\n",
      "   chmod +x docker-compose\n",
      "   ```\n",
      "\n",
      "3. Finally, move it to a directory that is in your PATH for easier access:\n",
      "\n",
      "   ```bash\n",
      "   sudo mv docker-compose /usr/local/bin/\n",
      "   ```\n",
      "\n",
      "By following these steps, you should be able to use the `docker-compose` command directly on your VM.\n",
      "\n",
      "For more details, refer to this information: [Docker: docker-compose still not available after changing .bashrc](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/046_e43abaa421_docker-docker-compose-still-not-available-after-ch.md).\n",
      "\n",
      "What steps should I take to change my Hadoop version to resolve compatibility issues with Windows?\n",
      "To change your Hadoop version in order to resolve compatibility issues with Windows, you should follow these steps:\n",
      "\n",
      "1. **Install Hadoop 3.0.1**:\n",
      "   - Change your Hadoop version to 3.0.1, as it is recommended for better compatibility with Windows.\n",
      "\n",
      "2. **Replace the Existing Files**:\n",
      "   - Go to your local Hadoop `bin` directory (usually found at `C:\\hadoop\\bin`).\n",
      "   - Replace all the files in this directory with the files from the following repository: \n",
      "     [winutils/hadoop-3.0.1/bin at master · cdarlint/winutils](https://github.com/cdarlint/winutils/tree/master/hadoop-3.0.1/bin).\n",
      "\n",
      "3. **Test and Verify**:\n",
      "   - After replacing the files, test your Hadoop installation to see if the compatibility issue has been resolved. \n",
      "\n",
      "If you encounter further issues, you may want to try other versions available in repositories until you find one that works for your setup. \n",
      "\n",
      "For more detailed guidance, you can refer to the source materials: [Hadoop Version Compatibility Issues](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-5/044_3a1fbd50a0_javaioioexception-cannot-run-program-chadoopbinwin.md).\n",
      "\n",
      "Is it possible to take this data engineering course without using cloud services like GCP, and what can I do locally?\n",
      "Yes, it is possible to take the data engineering course without using cloud services like GCP. You can run most of the course materials locally, as everything except for BigQuery can be executed on your local machine. \n",
      "\n",
      "However, please note that Homework 3 specifically requires the use of BigQuery, so that part would need to be addressed through GCP or an alternative setup.\n",
      "\n",
      "For more information, you may refer to the relevant document: [Environment: The GCP and other cloud providers](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/029_77a076baa3_environment-the-gcp-and-other-cloud-providers-are.md).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in tqdm(question_list.questions):\n",
    "    print(question)\n",
    "    result = await agent.run(user_prompt=question)\n",
    "    print(result.output)\n",
    "    log_interaction_to_file(agent, result.new_messages(), source='ai-generated')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac306ff-8456-4c74-8434-f7c3d0af1d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
